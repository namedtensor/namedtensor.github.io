<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="David Chiang University of Notre Dame" />
  <meta name="author" content="Sasha Rush Cornell University" />
  <meta name="author" content="Boaz Barak Harvard University" />
  <title>Named Tensor Notation</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <script src="/usr/share/javascript/mathjax/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="stylesheet" href="https://vanillacss.com/vanilla.css">
      <style>
          body{margin:0 auto;max-width:50rem;}
          @media(max-width:50rem) {
              body {
                  padding: 10px;
              }
          }
      </style>
  
    <meta charset="utf-8" />
    <meta name="generator" content="pandoc" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
    <meta name="author" content="David Chiang and Sasha Rush" />
    <title>Named Tensor Notation</title>
    <style type="text/css">
        code{white-space: pre-wrap;}
        span.smallcaps{font-variant: small-caps;}
        span.underline{text-decoration: underline;}
        div.column{display: inline-block; vertical-align: top; width: 50%;}
    </style>
    <script src="/usr/share/javascript/mathjax/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  
  <div style="display:none">
  \(
    \require{ams}
    \DeclareMathOperator*{\softmax}{softmax}
    \DeclareMathOperator{\ind}{ind}
    \DeclareMathOperator{\rec}{rec}
    \newcommand{\ensuremath}[1]{#1}
    \newcommand{\vdotswithin}[1]{\vdots}
  \)
  </div>
</head>
<body>
<header id="title-block-header">
<h1 class="title"><strong>Named Tensor Notation</strong></h1>
<p class="author">David Chiang<br />
University of Notre Dame</p>
<p class="author">Sasha Rush<br />
Cornell University</p>
<p class="author">Boaz Barak<br />
Harvard University</p>
<p class="date">Version 0.3</p>
</header>
<nav id="TOC">
<ul>
<li><a href="#sec:intro"><span class="toc-section-number">1</span> Introduction</a></li>
<li><a href="#sec:overview"><span class="toc-section-number">2</span> Informal Overview</a></li>
<li><a href="#sec:examples"><span class="toc-section-number">3</span> Examples</a></li>
<li><a href="#latex-macros"><span class="toc-section-number">4</span> LaTeX Macros</a></li>
<li><a href="#sec:definitions"><span class="toc-section-number">5</span> Formal Definitions</a></li>
<li><a href="#differentiation"><span class="toc-section-number">6</span> Differentiation</a></li>
<li><a href="#extensions"><span class="toc-section-number">7</span> Extensions</a></li>
<li><a href="#alternatives"><span class="toc-section-number">8</span> Alternatives</a></li>
<li><a href="#acknowledgements">Acknowledgements</a></li>
<li><a href="#references">References</a></li>
</ul>
</nav>
<h1 id="sec:intro"><span class="header-section-number">1</span> Introduction</h1>
<p>Most papers about neural networks use the notation of vectors and matrices from applied linear algebra. This notation is optimized for talking about vector spaces, but becomes cumbersome when talking about neural networks. Consider the following equation <span class="citation" data-cites="vaswani+:2017">(Vaswani et al. 2017)</span>: <span class="math display">\[\text{Attention}(Q, K, V) = \softmax \left( \frac{QK^\top}{\sqrt{d_k}} \right) V.\]</span> where <span class="math inline">\(Q\)</span>, <span class="math inline">\(K\)</span>, and <span class="math inline">\(V\)</span> (for query, key, and value, respectively) are sequences of feature vectors, packed into matrices. Does the product <span class="math inline">\(QK^\top\)</span> sum over the sequence, or over the features? It sums over columns, but there’s not enough information to know what the columns represent. Is the softmax taken over the query sequence or the key sequence? The usual notation doesn’t even offer a way to answer this question. With multiple attention heads or multiple sentences in a minibatch, the notation becomes more difficult still.</p>
<p>Here, we propose mathematical notation for tensors with <em>named axes</em>. The notation has a formal underpinning, but is hopefully intuitive enough that machine learning researchers can understand it without much effort.</p>
<p>In our notation, the above equation becomes <span class="math display">\[\begin{aligned}
  \text{Attention} \colon \mathbb{R}^{\ensuremath{\mathsf{seq&#39;}} \times \ensuremath{\mathsf{key}}} \times \mathbb{R}^{\ensuremath{\mathsf{seq}} \times\ensuremath{\mathsf{key}}} \times \mathbb{R}^{\ensuremath{\mathsf{seq}} \times\ensuremath{\mathsf{val}}} &amp;\rightarrow \mathbb{R}^{\ensuremath{\mathsf{seq&#39;}} \times \ensuremath{\mathsf{val}}} \\
  \text{Attention}(Q,K,V) = \mathop{\underset{\ensuremath{\mathsf{seq}}}{\mathrm{softmax}}} \left( \frac{Q \mathbin{\underset{\ensuremath{\mathsf{key}}}{\odot}} K}{\sqrt{|\ensuremath{\mathsf{key}}|}} \right) \mathbin{\underset{\ensuremath{\mathsf{seq}}}{\odot}} V.\end{aligned}\]</span> The tensor <span class="math inline">\(K\)</span> has axes for the sequence (<span class="math inline">\(\mathsf{seq}\)</span>) and for the key features (<span class="math inline">\(\mathsf{key}\)</span>), instead of rows or columns, so the reader does not need to remember which is which. The dot product <span class="math inline">\(Q \mathbin{\underset{\ensuremath{\mathsf{key}}}{\odot}} K\)</span> is explicitly over the <span class="math inline">\(\mathsf{key}\)</span> axis. The resulting tensor has a <span class="math inline">\(\mathsf{seq}\)</span> axis for the key sequence and a <span class="math inline">\(\mathsf{seq&#39;}\)</span> axis for the query sequence, and the softmax is explicitly over <span class="math inline">\(\ensuremath{\mathsf{seq}}\)</span>, as is the dot product with <span class="math inline">\(V\)</span>. This formula works as written if we add a <span class="math inline">\(\ensuremath{\mathsf{heads}}\)</span> axis for multiple attention heads, or a <span class="math inline">\(\ensuremath{\mathsf{batch}}\)</span> axis for multiple sequences in a minibatch.</p>
<p>Our notation is inspired by libraries for programming with multidimensional arrays <span class="citation" data-cites="numpy pytorch">(Harris et al. 2020; Paszke et al. 2019)</span> and extensions that use named axes, like xarray <span class="citation" data-cites="xarray">(Hoyer and Hamman 2017)</span>, Nexus <span class="citation" data-cites="chen2017typesafe">(Chen 2017)</span>, tsalib <span class="citation" data-cites="tsalib">(Sinha 2018)</span>, NamedTensor <span class="citation" data-cites="namedtensor">(Rush 2019)</span>, named tensors in PyTorch <span class="citation" data-cites="named-tensors">(Torch Contributors 2019)</span>, and Dex <span class="citation" data-cites="maclaurin+:2019">(Maclaurin et al. 2019)</span>. However, our focus is on mathematical notation rather than code.</p>
<p>The source code for this document can be found at <a href="https://github.com/namedtensor/notation/">https://github.com/namedtensor/notation/</a>. We invite anyone to make comments on this proposal by submitting issues or pull requests on this repository.</p>
<h1 id="sec:overview"><span class="header-section-number">2</span> Informal Overview</h1>
<p>In standard notation, a vector, matrix, or tensor is indexed by an integer or sequence of integers. If <span class="math inline">\(A \in \mathbb{R}^{3\times3}\)</span>, then the order of the two axes matters: <span class="math inline">\(A_{1,3}\)</span> and <span class="math inline">\(A_{3,1}\)</span> are not the same element. It’s up to the reader to remember what each axis of each tensor is for. We think this is a problem and propose a solution.</p>
<h2 id="named-tensors"><span class="header-section-number">2.1</span> Named tensors</h2>
<p>In a <em>named tensor</em>, we give each axis a name. For example, if <span class="math inline">\(A\)</span> represents an image, we can make it a named tensor like so (writing it two equivalent ways to show that the order of axes does not matter): <span class="math display">\[\begin{aligned}
  A &amp;\in \mathbb{R}^{\ensuremath{\ensuremath{\mathsf{height}}[3]} \times \ensuremath{\ensuremath{\mathsf{width}}[3]}} = \mathbb{R}^{\ensuremath{\ensuremath{\mathsf{width}}[3]} \times \ensuremath{\ensuremath{\mathsf{height}}[3]}} \\
  A &amp;= \ensuremath{\mathsf{height}}\begin{array}[b]{@{}c@{}}\ensuremath{\mathsf{width}}\\\begin{bmatrix}
    3 &amp; 1 &amp; 4 \\
    1 &amp; 5 &amp; 9 \\
    2 &amp; 6 &amp; 5
  \end{bmatrix}\end{array} = \ensuremath{\mathsf{width}}\begin{array}[b]{@{}c@{}}\ensuremath{\mathsf{height}}\\\begin{bmatrix}
    3 &amp; 1 &amp; 2 \\
    1 &amp; 5 &amp; 6 \\
    4 &amp; 9 &amp; 5
  \end{bmatrix}\end{array}.\end{aligned}\]</span></p>
<p>We access elements of <span class="math inline">\(A\)</span> using named indices, whose order again does not matter: <span class="math inline">\(A_{\ensuremath{\ensuremath{\mathsf{height}}(1)}, \ensuremath{\ensuremath{\mathsf{width}}(3)}} = A_{\ensuremath{\ensuremath{\mathsf{width}}(3)}, \ensuremath{\ensuremath{\mathsf{height}}(1)}} = 4\)</span>. We also allow partial indexing: <span class="math display">\[\begin{aligned}
A_{\ensuremath{\ensuremath{\mathsf{height}}(1)}} &amp;= \ensuremath{\mathsf{}}\begin{array}[b]{@{}c@{}}\ensuremath{\mathsf{width}}\\\begin{bmatrix}
  3 &amp; 1 &amp; 4
\end{bmatrix}\end{array}
&amp;
A_{\ensuremath{\ensuremath{\mathsf{width}}(3)}} &amp;= \ensuremath{\mathsf{}}\begin{array}[b]{@{}c@{}}\ensuremath{\mathsf{height}}\\\begin{bmatrix}
  4 &amp; 9 &amp; 5
\end{bmatrix}\end{array}.\end{aligned}\]</span></p>
<p>In many contexts, an axis name is used with only one size. If so, we can simply write <span class="math inline">\(\ensuremath{\mathsf{height}}\)</span> for the unique axis with name <span class="math inline">\(\ensuremath{\mathsf{height}}\)</span>, as in <span class="math inline">\(\mathbb{R}^{\ensuremath{\mathsf{height}} \times \ensuremath{\mathsf{width}}}\)</span>. We can leave the size of an axis unspecified at first, and specify its size later (like in a section on experimental details): for example, <span class="math inline">\(|\ensuremath{\mathsf{height}}|=|\ensuremath{\mathsf{width}}|=28\)</span> to specify its exact size or just <span class="math inline">\(|\ensuremath{\mathsf{height}}|=|\ensuremath{\mathsf{width}}|\)</span> to specify that it’s a square image.</p>
<p>What are good choices for axis names? We recommend meaningful <em>words</em> instead of single letters, and we recommend words that describe a <em>whole</em> rather than its parts. For example, if we wanted <span class="math inline">\(A\)</span> to have red, green, and blue channels, we’d name the axis <span class="math inline">\(\mathsf{chans}\)</span>, and if we wanted to represent a minibatch of images, we’d name the axis <span class="math inline">\(\mathsf{batch}\)</span>. Please see §<a href="#sec:examples" data-reference-type="ref" data-reference="sec:examples">3</a> for more examples.</p>
<h2 id="sec:operations"><span class="header-section-number">2.2</span> Named tensor operations</h2>
<p>Operations on named tensors are defined by taking a function on low-order tensors and extending it to higher-order tensors.</p>
<h3 id="elementwise-operations-and-broadcasting"><span class="header-section-number">2.2.1</span> Elementwise operations and broadcasting</h3>
<p>Any function from a scalar to a scalar can be applied elementwise to a named tensor, and any function from two scalars to a scalar can be applied to two named tensors with the same shape. For example: <span class="math display">\[\frac1{1+\exp(-A)} = \ensuremath{\mathsf{height}}\begin{array}[b]{@{}c@{}}\ensuremath{\mathsf{width}}\\\begin{bmatrix}
  \frac 1{1+\exp(-3)} &amp; \frac 1{1+\exp(-1)} &amp; \frac 1{1+\exp(-4)} \\[1ex]
  \frac 1{1+\exp(-1)} &amp; \frac 1{1+\exp(-5)} &amp; \frac 1{1+\exp(-9)} \\[1ex]
  \frac 1{1+\exp(-2)} &amp; \frac 1{1+\exp(-6)} &amp; \frac 1{1+\exp(-5)}
\end{bmatrix}\end{array}.\]</span></p>
<p>But if we apply a binary function/operator to tensors with different shapes, they are <em>broadcast</em> against each other (similarly to NumPy and derivatives). Let <span class="math display">\[\begin{aligned}
  B &amp;\in \mathbb{R}^{\ensuremath{\ensuremath{\mathsf{height}}[3]}} &amp; C &amp;\in \mathbb{R}^{\ensuremath{\ensuremath{\mathsf{width}}[3]}} \\
  B &amp;= \ensuremath{\mathsf{height}}\begin{array}[b]{@{}c@{}}\ensuremath{\mathsf{}}\\\begin{bmatrix}
    2 \\ 7 \\ 1
  \end{bmatrix}\end{array} &amp; 
  C &amp;= \ensuremath{\mathsf{}}\begin{array}[b]{@{}c@{}}\ensuremath{\mathsf{width}}\\\begin{bmatrix}
    1 &amp; 4 &amp; 1
  \end{bmatrix}\end{array}.\end{aligned}\]</span> (We write <span class="math inline">\(B\)</span> as a column just to make the broadcasting easier to visualize.) Then, to evaluate <span class="math inline">\(A+B\)</span>, we effectively replace <span class="math inline">\(B\)</span> with a new tensor <span class="math inline">\(B&#39;\)</span> that contains a copy of <span class="math inline">\(B\)</span> for every index of axis <span class="math inline">\(\ensuremath{\mathsf{width}}\)</span>. Likewise for <span class="math inline">\(A+C\)</span>: <span class="math display">\[\begin{aligned}
A + B &amp;= \ensuremath{\mathsf{height}}\begin{array}[b]{@{}c@{}}\ensuremath{\mathsf{width}}\\\begin{bmatrix}
  3+2 &amp; 1+2 &amp; 4+2 \\
  1+7 &amp; 5+7 &amp; 9+7 \\
  2+1 &amp; 6+1 &amp; 5+1
\end{bmatrix}\end{array} &amp;
A + C &amp;= \ensuremath{\mathsf{height}}\begin{array}[b]{@{}c@{}}\ensuremath{\mathsf{width}}\\\begin{bmatrix}
  3+1 &amp; 1+4 &amp; 4+1 \\
  1+1 &amp; 5+4 &amp; 9+1 \\
  2+1 &amp; 6+4 &amp; 5+1
\end{bmatrix}\end{array}.\end{aligned}\]</span></p>
<h3 id="sec:reductions"><span class="header-section-number">2.2.2</span> Reductions</h3>
<p>The same broadcasting rules apply to functions from vectors to scalars, called <em>reductions</em>. Unlike with functions on scalars, we always have to specify which axis reductions apply to, using a subscript. (This is equivalent to the <code>axis</code> argument in NumPy and <code>dim</code> in PyTorch.)</p>
<p>For example, we can sum over the <span class="math inline">\(\ensuremath{\mathsf{height}}\)</span> axis or the <span class="math inline">\(\ensuremath{\mathsf{width}}\)</span> axis of <span class="math inline">\(A\)</span>: <span class="math display">\[\begin{aligned}
\sum\limits_{\ensuremath{\mathsf{height}}} A &amp;= \sum_i A_{\ensuremath{\ensuremath{\mathsf{height}}(i)}} = \ensuremath{\mathsf{}}\begin{array}[b]{@{}c@{}}\ensuremath{\mathsf{width}}\\\begin{bmatrix}
  3+1+2 &amp; 1+5+6 &amp; 4+9+5
\end{bmatrix}\end{array}
\\
\sum\limits_{\ensuremath{\mathsf{width}}} A &amp;= \sum_j A_{\ensuremath{\ensuremath{\mathsf{width}}(j)}} = \ensuremath{\mathsf{}}\begin{array}[b]{@{}c@{}}\ensuremath{\mathsf{height}}\\\begin{bmatrix}
  3+1+4 &amp; 1+5+9 &amp; 2+6+5
\end{bmatrix}\end{array}.\end{aligned}\]</span> See §<a href="#sec:reductions" data-reference-type="ref" data-reference="sec:reductions">3.1.1</a> for more examples of reductions.</p>
<p>We can also write multiple names to perform the reduction over multiple axes at once. For example, <span class="math display">\[\sum\limits_{\ensuremath{\mathsf{height,width}}} A = \sum_i \sum_j A_{\ensuremath{\ensuremath{\mathsf{height}}(i)},\ensuremath{\ensuremath{\mathsf{width}}(j)}} = 3+1+4+1+5+9+2+6+5.\]</span></p>
<p>The vector dot-product is a function from <em>two</em> vectors to a scalar, which generalizes to named tensors to give the ubiquitous <em>contraction</em> operator. You can think of it as elementwise multiplication, then summation over one axis: <span class="math display">
\[\begin{aligned}
A \mathbin{\underset{\ensuremath{\mathsf{heigth}}}{\odot}} B &amp;= \sum_i A_{\ensuremath{\ensuremath{\mathsf{heigth}}(i)}} B_{\ensuremath{\ensuremath{\mathsf{heigth}}(i)}}= 
\ensuremath{\mathsf{}}\begin{array}[b]{@{}c@{}}\ensuremath{\mathsf{width}}\\\begin{bmatrix}
    3\cdot2 &amp; 1\cdot2   &amp;   4\cdot2  \\
         +     &amp;   +          &amp;     +          \\
    1\cdot7 &amp;  5\cdot7   &amp;  9\cdot7  \\
          +    &amp;   +          &amp;     +          \\
    2\cdot1 &amp;  6\cdot1  &amp;  5\cdot1
\end{bmatrix}\end{array}.\end{aligned}\]</span>

<span class="math display">\[\begin{aligned}
A \mathbin{\underset{\ensuremath{\mathsf{width}}}{\odot}} C &amp;= \sum_j A_{\ensuremath{\ensuremath{\mathsf{width}}(j)}} C_{\ensuremath{\ensuremath{\mathsf{width}}(j)}} = \ensuremath{\mathsf{height}}\begin{array}[b]{@{}c@{}}\ensuremath{\mathsf{}}\\\begin{bmatrix}
  3\cdot 1 + 1\cdot 4 + 4\cdot 1 \\
  1\cdot 1 + 5\cdot 4 + 9\cdot 1 \\
  2\cdot 1 + 6\cdot 4 + 5\cdot 1
\end{bmatrix}\end{array}.\end{aligned}\]</span> See §<a href="#sec:contractions" data-reference-type="ref" data-reference="sec:contractions">3.1.2</a> for more examples of what the contraction operator can do.</p>
<p>Again, we can write multiple names to contract multiple axes at once. An operator <span class="math inline">\(\odot\)</span> with no axis name under it contracts zero axes and is equivalent to elementwise multiplication, so we use <span class="math inline">\(\odot\)</span> for elementwise multiplication as well.</p>
<h3 id="renaming-and-reshaping"><span class="header-section-number">2.2.3</span> Renaming and reshaping</h3>
<p>It’s often useful to rename an axis (analogous to a transpose operation in standard notation): <span class="math display">\[A_{{\ensuremath{\mathsf{height}}\rightarrow\ensuremath{\mathsf{height&#39;}}}} = \ensuremath{\mathsf{height&#39;}}\begin{array}[b]{@{}c@{}}\ensuremath{\mathsf{width}}\\\begin{bmatrix}
  3 &amp; 1 &amp; 4 \\
  1 &amp; 5 &amp; 9 \\
  2 &amp; 6 &amp; 5 \\
\end{bmatrix}\end{array}.\]</span> We can also reshape two or more axes into one axis: <span class="math display">\[A_{{\ensuremath{\mathsf{(height,width)}}\rightarrow\ensuremath{\mathsf{layer}}}} = \ensuremath{\mathsf{}}\begin{array}[b]{@{}c@{}}\ensuremath{\mathsf{layer}}\\\begin{bmatrix}
    3 &amp; 1 &amp; 4 &amp; 1 &amp; 5 &amp; 9 &amp; 2 &amp; 6 &amp; 5
  \end{bmatrix}\end{array}\]</span> The order of elements in the new axis is undefined. If you need a particular order, you can write a more specific definition.</p>
<h1 id="sec:examples"><span class="header-section-number">3</span> Examples</h1>
<p>In this section we give a series of examples illustrating how to use named tensors in various situations, mostly related to machine learning.</p>
<h2 id="more-tensor-operations"><span class="header-section-number">3.1</span> More tensor operations</h2>
<h3 id="sec:reductions"><span class="header-section-number">3.1.1</span> Reductions</h3>
<p><span class="math display">\[\begin{aligned}
  \mathop{\underset{\ensuremath{\mathsf{ax}}}{\mathrm{min}}} A &amp;= \min \{A_{\ensuremath{\ensuremath{\mathsf{ax}}(i)}} \mid 1 \leq i \leq n\} \\
  \mathop{\underset{\ensuremath{\mathsf{ax}}}{\mathrm{max}}} A &amp;= \max \{A_{\ensuremath{\ensuremath{\mathsf{ax}}(i)}} \mid 1 \leq i \leq n\} \\
  \mathop{\underset{\ensuremath{\mathsf{ax}}}{\mathrm{norm}}} A &amp;= \sqrt{\sum\limits_{\ensuremath{\mathsf{ax}}} A^2} \\
  \mathop{\underset{\ensuremath{\mathsf{ax}}}{\mathrm{mean}}} A &amp;= \frac{1}{n} \sum\limits_{\ensuremath{\mathsf{ax}}} A \\
  \mathop{\underset{\ensuremath{\mathsf{ax}}}{\mathrm{var}}} A &amp;= \frac{1}{n} \sum\limits_{\ensuremath{\mathsf{ax}}} (A - \mathop{\underset{\ensuremath{\mathsf{ax}}}{\mathrm{mean}}} A)^2.\end{aligned}\]</span> The <span class="math inline">\(\min\)</span> and <span class="math inline">\(\max\)</span> operators are overloaded, as is the summation operator defined above (§<a href="#sec:reductions" data-reference-type="ref" data-reference="sec:reductions">3.1.1</a>). If the operator is applied to a tensor and has an axis under it, then it’s a reduction performed over the axis. But if it is applied to a set of tensors and has no axis under it, then it’s an elementwise operation performed over the set.</p>
<h3 id="sec:contractions"><span class="header-section-number">3.1.2</span> Contractions</h3>
<p>The contraction operator can be used for many multiplication-like operations.</p>
<p><span class="math display">\[\begin{aligned}
  u, v &amp;\in \mathbb{R}^{\ensuremath{\mathsf{ax1}}} \\
  x, y &amp;\in \mathbb{R}^{\ensuremath{\mathsf{ax2}}} \\
  A &amp;\in \mathbb{R}^{\ensuremath{\mathsf{ax1}} \times \ensuremath{\mathsf{ax2}}} \\
  B &amp;\in \mathbb{R}^{\ensuremath{\mathsf{ax2}} \times \ensuremath{\mathsf{ax3}}}\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}
  u \mathbin{\underset{\ensuremath{\mathsf{ax1}}}{\odot}} v &amp;= \sum_i u_{\ensuremath{\ensuremath{\mathsf{ax1}}(i)}} v_{\ensuremath{\ensuremath{\mathsf{ax1}}(i)}} &amp;&amp; \text{inner product} \\
  u \odot x &amp;= \sum_{i,j} u_{\ensuremath{\ensuremath{\mathsf{ax1}}(i)}} x_{\ensuremath{\ensuremath{\mathsf{ax2}}(j)}} &amp;&amp; \text{outer product} \\
  A \mathbin{\underset{\ensuremath{\mathsf{ax2}}}{\odot}} x &amp;= \sum_j A_{\ensuremath{\ensuremath{\mathsf{ax2}}(j)}} x_{\ensuremath{\ensuremath{\mathsf{ax2}}(j)}} &amp;&amp; \text{matrix-vector multiplication} \\
  u \mathbin{\underset{\ensuremath{\mathsf{ax1}}}{\odot}} A &amp;= \sum_i u_{\ensuremath{\ensuremath{\mathsf{ax1}}(j)}} A_{\ensuremath{\ensuremath{\mathsf{ax1}}(j)}} &amp;&amp; \text{vector-matrix multiplication} \\
  A \mathbin{\underset{\ensuremath{\mathsf{ax2}}}{\odot}} B &amp;= \sum_j A_{\ensuremath{\ensuremath{\mathsf{ax2}}(j)}} \odot B_{\ensuremath{\ensuremath{\mathsf{ax2}}(j)}} &amp;&amp; \text{matrix-matrix multiplication}\end{aligned}\]</span></p>
<h3 id="softmax-and-argmax"><span class="header-section-number">3.1.3</span> Softmax and argmax</h3>
<p>Most activation functions are elementwise operations (sigmoid, tanh, ReLU), so they are straightforward to use in our notation; the softmax, however, is interesting because it’s defined as a function from vectors to vectors: <span class="math display">\[\begin{aligned}
  \mathop{\underset{\ensuremath{\mathsf{ax}}}{\mathrm{softmax}}} A &amp;= \frac{\exp A}{\sum\limits_{\ensuremath{\mathsf{ax}}} \exp A}.\end{aligned}\]</span> As with reductions, we write an axis below the softmax operator, but this axis is retained in the output.</p>
<p>Closely related are argmax and argmin, which we define to compute one-hot vectors with a one at the position containing the maximum or minimum value. <span class="math display">\[\begin{aligned}
  \mathop{\underset{\ensuremath{\mathsf{ax}}}{\mathrm{argmax}}} A &amp;= \lim_{\alpha \rightarrow \infty} \mathop{\underset{\ensuremath{\mathsf{ax}}}{\mathrm{softmax}}} \alpha A \\
  \mathop{\underset{\ensuremath{\mathsf{ax}}}{\mathrm{argmin}}} A &amp;= \lim_{\alpha \rightarrow -\infty} \mathop{\underset{\ensuremath{\mathsf{ax}}}{\mathrm{softmax}}} \alpha A.\end{aligned}\]</span></p>
<h2 id="building-blocks"><span class="header-section-number">3.2</span> Building blocks</h2>
<h3 id="fully-connected-layers"><span class="header-section-number">3.2.1</span> Fully-connected layers</h3>
<p>A feedforward neural network looks like this: <span class="math display">\[\begin{aligned}
  X^0 &amp;\in \mathbb{R}^{\ensuremath{\mathsf{input}}} \\
  X^1 &amp;= \sigma(W^1 \mathbin{\underset{\ensuremath{\mathsf{input}}}{\odot}} X^0 + b^1) &amp; W^1 &amp;\in \mathbb{R}^{\ensuremath{\mathsf{hidden1}} \times \ensuremath{\mathsf{input}}} &amp; b^1 &amp;\in \mathbb{R}^{\ensuremath{\mathsf{hidden1}}} \\
  X^2 &amp;= \sigma(W^2 \mathbin{\underset{\ensuremath{\mathsf{hidden1}}}{\odot}} X^1 + b^2) &amp; W^2 &amp;\in \mathbb{R}^{\ensuremath{\mathsf{hidden2}} \times \ensuremath{\mathsf{hidden1}}} &amp; b^2 &amp;\in \mathbb{R}^{\ensuremath{\mathsf{hidden2}}} \\
  X^3 &amp;= \sigma(W^3 \mathbin{\underset{\ensuremath{\mathsf{hidden2}}}{\odot}} X^2 + b^3) &amp; W^3 &amp;\in \mathbb{R}^{\ensuremath{\mathsf{output}} \times \ensuremath{\mathsf{hidden2}}} &amp; b^3 &amp;\in \mathbb{R}^{\ensuremath{\mathsf{output}}}\end{aligned}\]</span> The layer sizes can be set by writing <span class="math inline">\(|\ensuremath{\mathsf{input}}| = 100\)</span>, etc.</p>
<p>If you don’t like repeating the equations for fully-connected layers, you can put them inside a function: <span class="math display">\[\begin{aligned}
  \text{FullConn}^l(x) &amp;= \sigma\left(W^l \mathbin{\underset{\ensuremath{\mathsf{layer}}}{\odot}} x + b^l\right)_{{\ensuremath{\mathsf{layer&#39;}}\rightarrow\ensuremath{\mathsf{layer}}}}\end{aligned}\]</span> where <span class="math display">\[\begin{aligned}
  W^l &amp;\in \mathbb{R}^{\ensuremath{\ensuremath{\mathsf{layer&#39;}}[n_{l}]} \times \ensuremath{\ensuremath{\mathsf{layer}}[n_{l-1}]}} \\
  b^l &amp;\in \mathbb{R}^{\ensuremath{\ensuremath{\mathsf{layer&#39;}}[n_l]}}.\end{aligned}\]</span> A couple of things are new here. First, <span class="math inline">\(\text{FullConn}^l\)</span> encapsulates both the equation for layer <span class="math inline">\(l\)</span> as well as its parameters (analogous to what TensorFlow and PyTorch call <em>modules</em>). Second, we chose to use the same axis name <span class="math inline">\(\mathsf{layer}\)</span> for all the layers (with different sizes <span class="math inline">\(n_l\)</span>). So <span class="math inline">\(\text{FullConn}^l\)</span> temporarily computes its output over axis <span class="math inline">\(\mathsf{layer&#39;}\)</span>, then renames it back to <span class="math inline">\(\mathsf{layer}\)</span>.</p>
<p>Then the network can be defined like this: <span class="math display">\[\begin{aligned}
  X^0 &amp;\in \mathbb{R}^{\ensuremath{\ensuremath{\mathsf{layer}}[n_0]}} \\
  X^1 &amp;= \text{FullConn}^1(X^0) \\
  X^2 &amp;= \text{FullConn}^2(X^1) \\
  X^3 &amp;= \text{FullConn}^3(X^2).\end{aligned}\]</span></p>
<h3 id="sec:rnn"><span class="header-section-number">3.2.2</span> Recurrent neural networks</h3>
<p>As a second example, let’s define a simple (Elman) RNN. This is similar to the feedforward network, except that the number of timesteps is variable and they all share parameters. <span class="math display">\[\begin{aligned}
x^{t} &amp;\in \mathbb{R}^{\ensuremath{\mathsf{input}}} &amp; t &amp;= 1, \ldots, n \\
W^{\text{h}} &amp;\in \mathbb{R}^{\ensuremath{\mathsf{hidden}} \times \ensuremath{\mathsf{hidden&#39;}}} &amp; |\ensuremath{\mathsf{hidden}}| &amp;= |\ensuremath{\mathsf{hidden&#39;}}| \\
W^{\text{i}} &amp;\in \mathbb{R}^{\ensuremath{\mathsf{input}} \times \ensuremath{\mathsf{hidden&#39;}}} \\
b &amp;\in \mathbb{R}^{\ensuremath{\mathsf{hidden&#39;}}} \\
h^{0} &amp;\in \mathbb{R}^{\ensuremath{\mathsf{hidden}}} \\
h^{t} &amp;= \sigma\left( W^{\text{h}} \mathbin{\underset{\ensuremath{\mathsf{hidden}}}{\odot}} h^{t-1} + W^{\text{i}} \mathbin{\underset{\ensuremath{\mathsf{input}}}{\odot}} x^{t} + b \right)_{{\ensuremath{\mathsf{hidden&#39;}}\rightarrow\ensuremath{\mathsf{hidden}}}} &amp; t &amp;= 1, \ldots, n\end{aligned}\]</span></p>
<h3 id="sec:attention"><span class="header-section-number">3.2.3</span> Attention</h3>
<p>In the introduction (§<a href="#sec:intro" data-reference-type="ref" data-reference="sec:intro">1</a>), we mentioned some difficulties in interpreting the equation for attention as it’s usually written. In our notation, it looks like this: <span class="math display">\[\begin{aligned}
  \text{Attention} \colon \mathbb{R}^{\ensuremath{\mathsf{key}}} \times \mathbb{R}^{\ensuremath{\mathsf{seq}} \times\ensuremath{\mathsf{key}}} \times \mathbb{R}^{\ensuremath{\mathsf{seq}} \times\ensuremath{\mathsf{val}}} &amp;\rightarrow \mathbb{R}^{\ensuremath{\mathsf{val}}} \\
  \text{Attention}(Q,K,V) &amp;= \mathop{\underset{\ensuremath{\mathsf{seq}}}{\mathrm{softmax}}} \left( \frac{Q \mathbin{\underset{\ensuremath{\mathsf{key}}}{\odot}} K}{\sqrt{|\ensuremath{\mathsf{key}}|}} \right) \mathbin{\underset{\ensuremath{\mathsf{seq}}}{\odot}} V.\end{aligned}\]</span></p>
<p>This equation is slightly different from the one in the introduction. The previous definition computed an output sequence over axis <span class="math inline">\(\mathsf{seq&#39;}\)</span>, but this definition computes a single value. If we want a sequence, we can just give <span class="math inline">\(Q\)</span> a <span class="math inline">\(\mathsf{seq&#39;}\)</span> axis (or some other name), and the function will compute an output sequence. Furthermore, if we give <span class="math inline">\(Q\)</span>, <span class="math inline">\(K\)</span>, and <span class="math inline">\(V\)</span> a <span class="math inline">\(\mathsf{heads}\)</span> axis for multiple attention heads, then the function will compute multi-head attention.</p>
<p>Sometimes we need to apply a mask to keep from attending to certain positions. <span class="math display">\[\begin{aligned}
  \text{Attention} \colon \mathbb{R}^{\ensuremath{\mathsf{key}}} \times \mathbb{R}^{\ensuremath{\mathsf{seq}} \times\ensuremath{\mathsf{key}}} \times \mathbb{R}^{\ensuremath{\mathsf{seq}} \times\ensuremath{\mathsf{val}}} \times \mathbb{R}^{\ensuremath{\mathsf{seq}}} &amp;\rightarrow \mathbb{R}^{\ensuremath{\mathsf{val}}} \\
\text{Attention}(Q, K, V, M) &amp;= \mathop{\underset{\ensuremath{\mathsf{seq}}}{\mathrm{softmax}}} \left( \frac{Q \mathbin{\underset{\ensuremath{\mathsf{key}}}{\odot}} K}{\sqrt{|\ensuremath{\mathsf{key}}|}} + M \right) \mathbin{\underset{\ensuremath{\mathsf{seq}}}{\odot}} V.\end{aligned}\]</span></p>
<h3 id="convolution"><span class="header-section-number">3.2.4</span> Convolution</h3>
<p>A 1-dimensional convolution can be written using contractions: <span class="math display">\[\begin{aligned}
\text{Conv1d} \colon \mathbb{R}^{\ensuremath{\mathsf{chans}} \times \ensuremath{\ensuremath{\mathsf{seq}}[n]}} &amp;\rightarrow \mathbb{R}^{\ensuremath{\ensuremath{\mathsf{seq}}[n&#39;]}} \\
\text{Conv1d}(X; W, b) &amp;= [W \mathbin{\underset{\ensuremath{\mathsf{chans,kernel}}}{\odot}} C \mathbin{\underset{\ensuremath{\mathsf{seq}}}{\odot}} X + b]_{{\ensuremath{\mathsf{out}}\rightarrow\ensuremath{\mathsf{seq}}}}\end{aligned}\]</span> where <span class="math display">\[\begin{aligned}
W &amp;\in \mathbb{R}^{\ensuremath{\mathsf{chans}} \times \ensuremath{\mathsf{kernel}}} \\
b &amp;\in \mathbb{R}\\
n&#39; &amp;= n-|\ensuremath{\mathsf{kernel}}|+1 \\
C &amp;\in \mathbb{R}^{\ensuremath{\ensuremath{\mathsf{out}}[n&#39;]} \times \ensuremath{\mathsf{kernel}} \times \ensuremath{\ensuremath{\mathsf{seq}}[n]}} \\
C_{\ensuremath{\ensuremath{\mathsf{out}}(o)},\ensuremath{\ensuremath{\mathsf{kernel}}(k)},\ensuremath{\ensuremath{\mathsf{seq}}(i)}} &amp;= \delta(o+k-1,i).\end{aligned}\]</span> This computes a single output channel, but we can get multiple output channels by giving <span class="math inline">\(W\)</span> and <span class="math inline">\(b\)</span> a <span class="math inline">\(\mathsf{chans&#39;}\)</span> axis (or some other name).</p>
<p>We can use the same <span class="math inline">\(C\)</span> to define a 2-dimensional convolution: <span class="math display">\[\begin{aligned}
  \text{Conv2d} \colon \mathbb{R}^{\ensuremath{\mathsf{chans}} \times \ensuremath{\ensuremath{\mathsf{height}}[h]} \times \ensuremath{\ensuremath{\mathsf{width}}[w]}}
  &amp;\rightarrow \mathbb{R}^{\ensuremath{\ensuremath{\mathsf{height}}[h&#39;]} \times \ensuremath{\ensuremath{\mathsf{width}}[w&#39;]}} \\
  \text{Conv2d}(X; W, b) &amp;= [W \mathbin{\underset{\ensuremath{\mathsf{chans, kh, kw}}}{\odot}} C^2 \mathbin{\underset{\ensuremath{\mathsf{height,width}}}{\odot}} X + b]_{\substack{{\ensuremath{\mathsf{oh}}\rightarrow\ensuremath{\mathsf{height}}}\\{\ensuremath{\mathsf{ow}}\rightarrow\ensuremath{\mathsf{width}}}}}\end{aligned}\]</span> where <span class="math display">\[\begin{aligned}
W &amp;\in \mathbb{R}^{\ensuremath{\mathsf{chans}} \times \ensuremath{\mathsf{kh}} \times \ensuremath{\mathsf{kw}}} \\
b &amp;\in \mathbb{R}\\
h&#39; &amp;= h-|\ensuremath{\mathsf{kh}}|+1 \\
w&#39; &amp;= w-|\ensuremath{\mathsf{kw}}|+1 \\
C^2 &amp;= C_{\substack{{\ensuremath{\mathsf{out}}\rightarrow\ensuremath{\mathsf{oh}}} \\ {\ensuremath{\mathsf{kernel}}\rightarrow\ensuremath{\mathsf{kh}}} \\ {\ensuremath{\mathsf{seq}}\rightarrow\ensuremath{\mathsf{height}}}}} \odot C_{\substack{{\ensuremath{\mathsf{out}}\rightarrow\ensuremath{\mathsf{ow}}} \\ {\ensuremath{\mathsf{kernel}}\rightarrow\ensuremath{\mathsf{kw}}} \\ {\ensuremath{\mathsf{seq}}\rightarrow\ensuremath{\mathsf{width}}}}}.\end{aligned}\]</span></p>
<h3 id="max-pooling"><span class="header-section-number">3.2.5</span> Max pooling</h3>
<p><span class="math display">\[\begin{aligned}
\text{MaxPool1d}_{k} \colon \mathbb{R}^{\ensuremath{\ensuremath{\mathsf{seq}}[n]}} &amp;\rightarrow \mathbb{R}^{\ensuremath{\ensuremath{\mathsf{seq}}[n/k]}} \\
\text{MaxPool1d}_{k}(X) &amp;= \mathop{\underset{\ensuremath{\mathsf{k}}}{\mathrm{max}}} U\end{aligned}\]</span> where <span class="math display">\[\begin{aligned}
U &amp;\in \mathbb{R}^{\ensuremath{\ensuremath{\mathsf{seq}}[n / k]} \times \ensuremath{\ensuremath{\mathsf{k}}[k]}} \\
U_{\ensuremath{\ensuremath{\mathsf{seq}}(i)}, \ensuremath{\ensuremath{\mathsf{k}}(di)}} &amp; = X_{\ensuremath{\ensuremath{\mathsf{seq}}(i \times k + di -1)}}.\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}
\text{MaxPool2d}_{kh,kw} \colon \mathbb{R}^{\ensuremath{\ensuremath{\mathsf{height}}[h]} \times \ensuremath{\ensuremath{\mathsf{width}}[w]}} &amp;\rightarrow \mathbb{R}^{\ensuremath{\ensuremath{\mathsf{height}}[h/kh]} \times \ensuremath{\ensuremath{\mathsf{width}}[w/kw]}} \\
\text{MaxPool2d}_{kh,hw}(X) &amp;= \mathop{\underset{\ensuremath{\mathsf{kh, kw}}}{\mathrm{max}}} U\end{aligned}\]</span> where <span class="math display">\[\begin{aligned}
U &amp;\in \mathbb{R}^{\ensuremath{\ensuremath{\mathsf{height}}[h / kh]} \times \ensuremath{\ensuremath{\mathsf{width}}[w / kw]} \times \ensuremath{\ensuremath{\mathsf{kh}}[kh]} \times \ensuremath{\ensuremath{\mathsf{kw}}[kw]}} \\
U_{\ensuremath{\ensuremath{\mathsf{height}}(i)}, \ensuremath{\ensuremath{\mathsf{width}}(j)}, \ensuremath{\ensuremath{\mathsf{kh}}(di)}, \ensuremath{\ensuremath{\mathsf{kw}}(dj)}} &amp; = X_{\ensuremath{\ensuremath{\mathsf{height}}(i \times kh + di -1)}, \ensuremath{\ensuremath{\mathsf{width}}(j \times kw + dj -1)}}.\end{aligned}\]</span></p>
<h3 id="normalization-layers"><span class="header-section-number">3.2.6</span> Normalization layers</h3>
<p>Batch, instance, and layer normalization are often informally described using the same equation, but they each correspond to very different functions. They differ by which axes are normalized.</p>
<p>We can define a single generic normalization layer: <span class="math display">\[\begin{aligned}
  \mathop{\underset{\ensuremath{\mathsf{ax}}}{\mathrm{XNorm}}} \colon \mathbb{R}^{\ensuremath{\mathsf{ax}}} &amp;\rightarrow \mathbb{R}^{\ensuremath{\mathsf{ax}}} \\
  \mathop{\underset{\ensuremath{\mathsf{ax}}}{\mathrm{XNorm}}}(X; \gamma, \beta, \epsilon) &amp;= \frac{X - \mathop{\underset{\ensuremath{\mathsf{ax}}}{\mathrm{mean}}}(X)}{\sqrt{\mathop{\underset{\ensuremath{\mathsf{ax}}}{\mathrm{var}}}(X)} + \epsilon} \odot \gamma + \beta\end{aligned}\]</span> where <span class="math display">\[\begin{aligned}
  \gamma, \beta &amp;\in \mathbb{R}^{\ensuremath{\mathsf{ax}}} \\
  \epsilon &amp;&gt; 0.\end{aligned}\]</span></p>
<p>Now, suppose that the input has three axes: <span class="math display">\[\begin{aligned}
X &amp;\in \mathbb{R}^{{\ensuremath{\mathsf{batch}} \times \ensuremath{\mathsf{chans}} \times \ensuremath{\mathsf{layer}}}}\end{aligned}\]</span> Then the three kinds of normalization layers can be written as: <span class="math display">\[\begin{aligned}
Y &amp;= \mathop{\underset{\ensuremath{\mathsf{batch}}}{\mathrm{XNorm}}}(X; \gamma, \beta) &amp;&amp; \text{batch normalization} \\
Y &amp;= \mathop{\underset{\ensuremath{\mathsf{layer}}}{\mathrm{XNorm}}}(X; \gamma, \beta) &amp;&amp; \text{instance normalization} \\
Y &amp;= \mathop{\underset{\ensuremath{\mathsf{layer,chans}}}{\mathrm{XNorm}}}(X; \gamma, \beta) &amp;&amp; \text{layer normalization}\end{aligned}\]</span></p>
<h2 id="sec:transformer"><span class="header-section-number">3.3</span> Transformer</h2>
<p>We define a Transformer used autoregressively as a language model. The input is a sequence of one-hot vectors, from which we compute word embeddings and positional encodings: <span class="math display">\[\begin{aligned}
  I &amp;\in \{0, 1\}^{\ensuremath{\mathsf{seq}} \times \ensuremath{\mathsf{vocab}}} &amp; \sum\limits_{\ensuremath{\mathsf{vocab}}} I &amp;= 1 \\
  W &amp;= (E \mathbin{\underset{\ensuremath{\mathsf{vocab}}}{\odot}} I)\sqrt{|\ensuremath{\mathsf{layer}}|} &amp; E &amp;\in \mathbb{R}^{\ensuremath{\mathsf{vocab}} \times \ensuremath{\mathsf{layer}}} \\
  P &amp;\in \mathbb{R}^{\ensuremath{\mathsf{seq}} \times \ensuremath{\mathsf{layer}}} \\
  P_{\ensuremath{\ensuremath{\mathsf{seq}}(p)}, \ensuremath{\ensuremath{\mathsf{layer}}(i)}} &amp;= \begin{cases}
    \sin((p-1) / 10000^{(i-1) / |\ensuremath{\mathsf{layer}}|}) &amp; \text{$i$ odd} \\ 
    \cos((p-1) / 10000^{(i-2) / |\ensuremath{\mathsf{layer}}|}) &amp; \text{$i$ even.}
  \end{cases}\end{aligned}\]</span></p>
<p>Then we use <span class="math inline">\(L\)</span> layers of self-attention and feed-forward neural networks: <span class="math display">\[\begin{aligned}
X^0 &amp;= W+P \\
T^1 &amp;= \text{LayerNorm}^1(\text{SelfAtt}^1(X^0)) + X^0\\
X^1 &amp;= \text{LayerNorm}^{1&#39;}(\text{FFN}^1(T^1)) + T^1\\
&amp;\vdotswithin{=} \\
T^{L} &amp;= \text{LayerNorm}^L(\text{SelfAtt}^L(X^{L-1})) + X^{L-1}\\
X^{L} &amp;= \text{LayerNorm}^{L&#39;}(\text{FFN}^L(T^L)) + T^L\\
O &amp;= \mathop{\underset{\ensuremath{\mathsf{vocab}}}{\mathrm{softmax}}}(E \mathbin{\underset{\ensuremath{\mathsf{layer}}}{\odot}} X^L)\end{aligned}\]</span> where <span class="math inline">\(\text{LayerNorm}\)</span>, <span class="math inline">\(\text{SelfAtt}\)</span> and <span class="math inline">\(\text{FFN}\)</span> are defined below.</p>
<p>Layer normalization (<span class="math inline">\(l = 1, 1&#39;, \ldots, L, L&#39;\)</span>): <span class="math display">\[\begin{aligned}
  \text{LayerNorm}^l \colon \mathbb{R}^{\ensuremath{\mathsf{layer}}} &amp;\rightarrow \mathbb{R}^{\ensuremath{\mathsf{layer}}} \\
  \text{LayerNorm}^l(X) &amp;= \mathop{\underset{\ensuremath{\mathsf{layer}}}{\mathrm{XNorm}}}(X; \beta^l, \gamma^l).\end{aligned}\]</span></p>
<p>We defined attention in §<a href="#sec:attention" data-reference-type="ref" data-reference="sec:attention">3.2.3</a>; the Transformer uses multi-head self-attention, in which queries, keys, and values are all computed from the same sequence. <span class="math display">\[\begin{aligned}
  \text{SelfAtt}^l \colon \mathbb{R}^{\ensuremath{\mathsf{seq}} \times \ensuremath{\mathsf{layer}}} &amp;\rightarrow \mathbb{R}^{\ensuremath{\mathsf{seq}} \times \ensuremath{\mathsf{layer}}} \\
  \text{SelfAtt}^l(X) &amp;= Y\end{aligned}\]</span> where <span class="math display">\[\begin{aligned}
  |\ensuremath{\mathsf{seq}}| &amp;= |\ensuremath{\mathsf{seq&#39;}}| \\
  |\ensuremath{\mathsf{key}}| = |\ensuremath{\mathsf{val}}| &amp;= |\ensuremath{\mathsf{layer}}|/|\ensuremath{\mathsf{heads}}| \\
  Q &amp;= W^{l,Q} \mathbin{\underset{\ensuremath{\mathsf{layer}}}{\odot}} X_{{\ensuremath{\mathsf{seq}}\rightarrow\ensuremath{\mathsf{seq&#39;}}}} &amp; W^{l,Q} &amp;\in \mathbb{R}^{\ensuremath{\mathsf{heads}} \times \ensuremath{\mathsf{layer}} \times \ensuremath{\mathsf{key}}} \\
  K &amp;= W^{l,K} \mathbin{\underset{\ensuremath{\mathsf{layer}}}{\odot}} X &amp; W^{l,K} &amp;\in \mathbb{R}^{\ensuremath{\mathsf{heads}} \times \ensuremath{\mathsf{layer}} \times \ensuremath{\mathsf{key}}} \\
  V &amp;= W^{l,V} \mathbin{\underset{\ensuremath{\mathsf{layer}}}{\odot}} X &amp; W^{l,V} &amp;\in \mathbb{R}^{\ensuremath{\mathsf{heads}} \times \ensuremath{\mathsf{layer}} \times \ensuremath{\mathsf{val}}} \\
  M &amp; \in \mathbb{R}^{\ensuremath{\mathsf{seq}} \times \ensuremath{\mathsf{seq&#39;}}} \\
  M_{\ensuremath{\ensuremath{\mathsf{seq}}(i)}, \ensuremath{\ensuremath{\mathsf{seq&#39;}}(j)}} &amp;= \begin{cases}
    0 &amp; i \leq j\\
    -\infty &amp; \text{otherwise}
  \end{cases} \\
  Y &amp;= W^{l,O} \mathbin{\underset{\ensuremath{\mathsf{heads,val}}}{\odot}} \text{Attention}(Q, K, V, M)_{{\ensuremath{\mathsf{seq&#39;}}\rightarrow\ensuremath{\mathsf{seq}}}} &amp; W^{l,O} &amp;\in \mathbb{R}^{\ensuremath{\mathsf{heads}} \times \ensuremath{\mathsf{val}} \times \ensuremath{\mathsf{layer}}}\end{aligned}\]</span></p>
<p>Feedforward neural networks: <span class="math display">\[\begin{aligned}
  \text{FFN}^l \colon \mathbb{R}^{\ensuremath{\mathsf{layer}}} &amp;\rightarrow \mathbb{R}^{\ensuremath{\mathsf{layer}}} \\
  \text{FFN}^l(X) &amp;= X^2\end{aligned}\]</span> where <span class="math display">\[\begin{aligned}
  X^1 &amp;= \text{relu}(W^{l,1} \mathbin{\underset{\ensuremath{\mathsf{layer}}}{\odot}} X + b^{l,1}) &amp; W^{l,1} &amp;\in \mathbb{R}^{\ensuremath{\mathsf{hidden}} \times \ensuremath{\mathsf{layer}}} &amp; b^{l,1} &amp;\in \mathbb{R}^{\ensuremath{\mathsf{hidden}}} \\
  X^2 &amp;= \text{relu}(W^{l,2} \mathbin{\underset{\ensuremath{\mathsf{hidden}}}{\odot}} X^1 + b^{l,2}) &amp; W^{l,2} &amp;\in \mathbb{R}^{\ensuremath{\mathsf{layer}} \times \ensuremath{\mathsf{hidden}}} &amp; b^{l,2} &amp;\in \mathbb{R}^{\ensuremath{\mathsf{hidden}}}.\end{aligned}\]</span></p>
<h2 id="lenet"><span class="header-section-number">3.4</span> LeNet</h2>
<p><span class="math display">\[\begin{aligned}
X^0 &amp;\in \mathbb{R}^{\ensuremath{\mathsf{batch}} \times \ensuremath{\ensuremath{\mathsf{chans}}[c_0]} \times \ensuremath{\mathsf{height}} \times \ensuremath{\mathsf{width}}} \\
T^1 &amp;= \text{relu}(\text{Conv}^1(X^0)) \\
X^1 &amp;= \text{MaxPool}^1(T^1) \\
T^2 &amp;= \text{relu}(\text{Conv}^2(X^1)) \\
X^2 &amp;= \text{MaxPool}^2(T^2)_{{\ensuremath{\mathsf{(height,width,chans)}}\rightarrow\ensuremath{\mathsf{layer}}}} \\
X^3 &amp;= \text{relu}(W^3 \mathbin{\underset{\ensuremath{\mathsf{layer}}}{\odot}} X^2 + b^3) &amp; W^3 &amp;\in \mathbb{R}^{\ensuremath{\mathsf{hidden}} \times \ensuremath{\mathsf{layer}}} &amp; b^3 &amp;\in \mathbb{R}^{\ensuremath{\mathsf{hidden}}} \\
O &amp;= \mathop{\underset{\ensuremath{\mathsf{classes}}}{\mathrm{softmax}}} (W^4 \mathbin{\underset{\ensuremath{\mathsf{hidden}}}{\odot}} X^3 + b^4) &amp; W^4 &amp;\in \mathbb{R}^{\ensuremath{\mathsf{classes}} \times \ensuremath{\mathsf{hidden}}} &amp; b^4 &amp;\in \mathbb{R}^{\ensuremath{\mathsf{classes}}}\end{aligned}\]</span> As an alternative to the flattening operation in the equation for <span class="math inline">\(X^2\)</span>, we could have written <span class="math display">\[\begin{aligned}
X^2 &amp;= \text{MaxPool}^2(T^2) \\
X^3 &amp;= \text{relu}(W^3 \mathbin{\underset{\ensuremath{\mathsf{height,width,chans}}}{\odot}} X^2 + b^3) &amp; W^3 &amp;\in \mathbb{R}^{\ensuremath{\mathsf{hidden}} \times \ensuremath{\mathsf{height}} \times \ensuremath{\mathsf{width}} \times \ensuremath{\mathsf{chans}}}.\end{aligned}\]</span></p>
<p>The convolution and pooling operations are defined as follows: <span class="math display">\[\begin{aligned}
\text{Conv}^l(X) &amp;= \text{Conv2d}(X; W^l, b^l)_{{\ensuremath{\mathsf{chans&#39;}}\rightarrow\ensuremath{\mathsf{chans}}}}\end{aligned}\]</span> where <span class="math display">\[\begin{aligned}
W^l &amp; \in \mathbb{R}^{\ensuremath{\ensuremath{\mathsf{chans&#39;}}[c_{l}]} \times \ensuremath{\ensuremath{\mathsf{chans}}[c_{l-1}]} \times \ensuremath{\ensuremath{\mathsf{kh}}[kh_l]} \times \ensuremath{\ensuremath{\mathsf{kw}}[kw_l]}} \\
b^l &amp;\in \mathbb{R}^{\ensuremath{\ensuremath{\mathsf{chans&#39;}}[c_{l}]}}\end{aligned}\]</span> and <span class="math display">\[\begin{aligned}
\text{MaxPool}^l(X) &amp;= \text{MaxPol2d}_{ph^l,ph^l}(X).\end{aligned}\]</span></p>
<h2 id="other-examples"><span class="header-section-number">3.5</span> Other examples</h2>
<h3 id="discrete-random-variables"><span class="header-section-number">3.5.1</span> Discrete random variables</h3>
<p>Named axes are very helpful for working with discrete random variables, because each random variable can be represented by an axis with the same name. For instance, if <span class="math inline">\(\ensuremath{\mathsf{A}}\)</span> and <span class="math inline">\(\ensuremath{\mathsf{B}}\)</span> are random variables, we can treat <span class="math inline">\(p(\ensuremath{\mathsf{B}} \mid \ensuremath{\mathsf{A}})\)</span> and <span class="math inline">\(p(\ensuremath{\mathsf{A}})\)</span> as tensors: <span class="math display">\[\begin{aligned}
p(\ensuremath{\mathsf{B}} \mid \ensuremath{\mathsf{A}}) &amp;\in [0, 1]^{\ensuremath{\mathsf{A}} \times \ensuremath{\mathsf{B}}} &amp; \sum\limits_{\ensuremath{\mathsf{B}}} p(\ensuremath{\mathsf{B}}\mid \ensuremath{\mathsf{A}}) &amp;= 1 \\
p(\ensuremath{\mathsf{A}}) &amp;\in [0, 1]^{\ensuremath{\mathsf{A}}} &amp; \sum\limits_{\ensuremath{\mathsf{A}}} p(\ensuremath{\mathsf{A}}) &amp;= 1\end{aligned}\]</span> Then many common operations on probability distributions can be expressed in terms of tensor operations: <span class="math display">\[\begin{aligned}
p(\ensuremath{\mathsf{A}}, \ensuremath{\mathsf{B}}) &amp;= p(\ensuremath{\mathsf{B}} \mid \ensuremath{\mathsf{A}}) \odot p(\ensuremath{\mathsf{A}}) &amp;&amp; \text{chain rule}\\
p(\ensuremath{\mathsf{B}}) &amp;= \sum\limits_{\ensuremath{\mathsf{A}}} p(\ensuremath{\mathsf{A}}, \ensuremath{\mathsf{B}}) = p(\ensuremath{\mathsf{B}} \mid \ensuremath{\mathsf{A}}) \mathbin{\underset{\ensuremath{\mathsf{A}}}{\odot}} p(\ensuremath{\mathsf{A}}) &amp;&amp; \text{marginalization} \\
p(\ensuremath{\mathsf{A}} \mid \ensuremath{\mathsf{B}}) &amp;= \frac{p(\ensuremath{\mathsf{A}}, \ensuremath{\mathsf{B}})}{p(\ensuremath{\mathsf{B}})} = \frac{p(\ensuremath{\mathsf{B}} \mid \ensuremath{\mathsf{A}}) \odot p(\ensuremath{\mathsf{A}})}{p(\ensuremath{\mathsf{B}} \mid \ensuremath{\mathsf{A}}) \mathbin{\underset{\ensuremath{\mathsf{A}}}{\odot}} p(\ensuremath{\mathsf{A}})}. &amp;&amp; \text{Bayes&#39; rule}\end{aligned}\]</span></p>
<h3 id="continuous-bag-of-words"><span class="header-section-number">3.5.2</span> Continuous bag of words</h3>
<p>A continuous bag-of-words model classifies by summing up the embeddings of a sequence of words <span class="math inline">\(X\)</span> and then projecting them to the space of classes.</p>
<p><span class="math display">\[\begin{aligned}
\text{CBOW} \colon \{0, 1\}^{\ensuremath{\mathsf{seq}} \times \ensuremath{\mathsf{vocab}}} &amp;\rightarrow \mathbb{R}^{\ensuremath{\mathsf{seq}} \times \ensuremath{\mathsf{classes}}} \\
\text{CBOW}(X; E, W) &amp;= \mathop{\underset{\ensuremath{\mathsf{class}}}{\mathrm{softmax}}} (W \mathbin{\underset{\ensuremath{\mathsf{hidden}}}{\odot}} E \mathbin{\underset{\ensuremath{\mathsf{vocab}}}{\odot}} X)\end{aligned}\]</span> where <span class="math display">\[\begin{aligned}
\sum\limits_{\ensuremath{\mathsf{vocab}}} X &amp;= 1 \\
E &amp;\in \mathbb{R}^{\ensuremath{\mathsf{vocab}} \times \ensuremath{\mathsf{hidden}}} \\
W &amp;\in \mathbb{R}^{\ensuremath{\mathsf{classes}} \times \ensuremath{\mathsf{hidden}}}.\end{aligned}\]</span> Here, the two contractions can be done in either order, so we leave the parentheses off.</p>
<h3 id="sudoku-ilp"><span class="header-section-number">3.5.3</span> Sudoku ILP</h3>
<p>Sudoku puzzles can be represented as binary tiled tensors. Given a grid we can check that it is valid by converting it to a grid of grids. Constraints then ensure that there is one digit per row, per column and per sub-box.</p>
<p><span class="math display">\[\begin{aligned}
\text{check} \colon \{0, 1\}^{\ensuremath{\ensuremath{\mathsf{height}}[9]} \times \ensuremath{\ensuremath{\mathsf{width}}[9]} \times \ensuremath{\ensuremath{\mathsf{assign}}[9]}} &amp;\rightarrow \{0, 1\} \\
\text{check}(X) &amp;=
\mathbb{I}\left[\begin{aligned}
\sum\limits_{\ensuremath{\mathsf{assign}}} X = 1 &amp;\land \sum\limits_{\ensuremath{\mathsf{height, width}}} Y = 1 \land {} \\
\sum\limits_{\ensuremath{\mathsf{height}}} X = 1 &amp;\land \sum\limits_{\ensuremath{\mathsf{width}}} X = 1
\end{aligned}\right]\end{aligned}\]</span> where <span class="math display">\[\begin{aligned}
Y &amp;\in \{0, 1\}^{\ensuremath{\ensuremath{\mathsf{height&#39;}}[3]} \times \ensuremath{\ensuremath{\mathsf{width&#39;}}[3]} \times \ensuremath{\ensuremath{\mathsf{height}}[3]} \times \ensuremath{\ensuremath{\mathsf{width}}[3]} \times \ensuremath{\ensuremath{\mathsf{assign}}[9]}}  \\
Y_{\ensuremath{\ensuremath{\mathsf{height&#39;}}(h&#39;)}, \ensuremath{\ensuremath{\mathsf{height}}(h)}, \ensuremath{\ensuremath{\mathsf{width&#39;}}(w&#39;)}, \ensuremath{\ensuremath{\mathsf{width}}(w)}} &amp;= X_{\ensuremath{\ensuremath{\mathsf{height}}(3h&#39; + h-1)}, \ensuremath{\ensuremath{\mathsf{width}}(3 w&#39; + w-1)}}.\end{aligned}\]</span></p>
<h3 id="k-means-clustering"><span class="header-section-number">3.5.4</span> <span class="math inline">\(K\)</span>-means clustering</h3>
<p>The following equations define one step of <span class="math inline">\(k\)</span>-means clustering. Given a set of points <span class="math inline">\(X\)</span> and an initial set of cluster centers <span class="math inline">\(C\)</span>, <span class="math display">\[\begin{aligned}
  X &amp;\in \mathbb{R}^{\ensuremath{\mathsf{batch}} \times \ensuremath{\mathsf{space}}} \\
C &amp;\in \mathbb{R}^{\ensuremath{\mathsf{clusters}} \times \ensuremath{\mathsf{space}}}\end{aligned}\]</span> we repeat the following update: Compute cluster assignments <span class="math display">\[\begin{aligned}
Q &amp;= \mathop{\underset{\ensuremath{\mathsf{clusters}}}{\mathrm{argmin}}} \mathop{\underset{\ensuremath{\mathsf{space}}}{\mathrm{norm}}}(C-X)\end{aligned}\]</span> then recompute the cluster centers: <span class="math display">\[C \leftarrow \sum\limits_{\ensuremath{\mathsf{batch}}} \frac{Q \odot X}{Q}.\]</span></p>
<h3 id="beam-search"><span class="header-section-number">3.5.5</span> Beam search</h3>
<p>Beam search is a commonly used approach for approximate discrete search. Here <span class="math inline">\(H\)</span> is the score of each element in the beam, <span class="math inline">\(S\)</span> is the state of each element in the beam, and <span class="math inline">\(f\)</span> is an update function that returns the score of each state transition. <span class="math display">\[\begin{aligned}
H &amp;\in \mathbb{R}^{\ensuremath{\mathsf{beam}}} \\
S &amp;\in \{0, 1\}^{\ensuremath{\mathsf{beam}} \times \ensuremath{\mathsf{state}}} &amp; \sum\limits_{\ensuremath{\mathsf{state}}} S &amp;= 1 \\
f &amp;\colon \{0, 1\}^{\ensuremath{\mathsf{state}}} \rightarrow \mathbb{R}^{\ensuremath{\mathsf{state}}} \\\end{aligned}\]</span> Then we repeat the following update: <span class="math display">\[\begin{aligned}
H&#39; &amp;= \mathop{\underset{\ensuremath{\mathsf{beam}}}{\mathrm{max}}} (H \odot f(S)) \\
H &amp;\leftarrow \mathop{\underset{\ensuremath{\mathsf{state,beam}}}{\mathrm{maxk}}} H&#39; \\
S &amp;\leftarrow \mathop{\underset{\ensuremath{\mathsf{state,beam}}}{\mathrm{argmaxk}}} H&#39;\end{aligned}\]</span> where <span class="math display">\[\begin{aligned}
\mathop{\underset{\ensuremath{\mathsf{ax,k}}}{\mathrm{maxk}}} \colon \mathbb{R}^{\ensuremath{\mathsf{ax}}} &amp;\rightarrow \mathbb{R}^{\ensuremath{\mathsf{k}}} \\
\mathop{\underset{\ensuremath{\mathsf{ax,k}}}{\mathrm{argmaxk}}} \colon \mathbb{R}^{\ensuremath{\mathsf{ax}}} &amp;\rightarrow \{0,1\}^{\ensuremath{\mathsf{ax}},\ensuremath{\mathsf{k}}}\end{aligned}\]</span> are defined such that <span class="math inline">\([\mathop{\underset{\ensuremath{\mathsf{ax,k}}}{\mathrm{maxk}}} A]_{\ensuremath{\ensuremath{\mathsf{k}}(i)}}\)</span> is the <span class="math inline">\(i\)</span>-th largest value along axis <span class="math inline">\(\ensuremath{\mathsf{ax}}\)</span> and <span class="math inline">\(A \mathbin{\underset{\ensuremath{\mathsf{ax}}}{\odot}} (\mathop{\underset{\ensuremath{\mathsf{ax,k}}}{\mathrm{argmaxk}}}{A}) = \mathop{\underset{\ensuremath{\mathsf{ax,k}}}{\mathrm{max}}} A\)</span>.</p>
<p>We can add a <span class="math inline">\(\mathsf{batch}\)</span> axis to <span class="math inline">\(H\)</span> and <span class="math inline">\(S\)</span> and the above equations will work unchanged.</p>
<h3 id="multivariate-normal-distribution"><span class="header-section-number">3.5.6</span> Multivariate normal distribution</h3>
<p>To define a multivariate normal distribution, we need some matrix operations. These have two axis names written under them, for rows and columns, respectively. Determinant and inverse have the following signatures: <span class="math display">\[\begin{aligned}
\mathop{\underset{\ensuremath{\mathsf{ax1,ax2}}}{\mathrm{det}}} \colon F^{\ensuremath{\ensuremath{\mathsf{ax1}}[n]} \times \ensuremath{\ensuremath{\mathsf{ax2}}[n]}} &amp;\rightarrow F \\
\mathop{\underset{\ensuremath{\mathsf{ax1,ax2}}}{\mathrm{inv}}} \colon F^{\ensuremath{\ensuremath{\mathsf{ax1}}[n]} \times \ensuremath{\ensuremath{\mathsf{ax2}}[n]}} &amp;\rightarrow F^{\ensuremath{\ensuremath{\mathsf{ax1}}[n]} \times \ensuremath{\ensuremath{\mathsf{ax2}}[n]}}.\end{aligned}\]</span> (We write <span class="math inline">\(\text{inv}\)</span> instead of <span class="math inline">\(\cdot^{-1}\)</span> because there’s no way to write axis names under the latter.)</p>
<p>In our notation, the application of a bilinear form is more verbose than the standard notation (<span class="math inline">\((X-\mu)^\top \Sigma^{-1} (X-\mu)\)</span>), but also makes it look more like a function of two arguments (and would generalize to three or more arguments).</p>
<p><span class="math display">\[\begin{aligned}
\mathcal{N} \colon \mathbb{R}^{\ensuremath{\mathsf{d}}} &amp;\rightarrow \mathbb{R}\\
\mathcal{N}(X; \mu, \Sigma) &amp;= \frac{\exp\left(-\frac{1}{2} \left(\mathop{\underset{\ensuremath{\mathsf{d1, d2}}}{\mathrm{inv}}} \Sigma\right) \mathbin{\underset{\ensuremath{\mathsf{d1,d2}}}{\odot}} \left([X - \mu]_{{\ensuremath{\mathsf{d}}\rightarrow\ensuremath{\mathsf{d1}}}} \odot [X - \mu]_{{\ensuremath{\mathsf{d}}\rightarrow\ensuremath{\mathsf{d2}}}} \right) \right)}{\sqrt{(2 \pi)^{|\ensuremath{\mathsf{d}}|} \mathop{\underset{\ensuremath{\mathsf{d1, d2}}}{\mathrm{det}}} \Sigma}}\end{aligned}\]</span> where <span class="math display">\[\begin{aligned}
|\ensuremath{\mathsf{d}}| &amp;= |\ensuremath{\mathsf{d1}}| = |\ensuremath{\mathsf{d2}}| \\
\mu &amp;\in \mathbb{R}^{\ensuremath{\mathsf{d}}} \\
\Sigma &amp; \in \mathbb{R}^{\ensuremath{\mathsf{d1}} \times \ensuremath{\mathsf{d2}}}.\end{aligned}\]</span></p>
<h1 id="latex-macros"><span class="header-section-number">4</span> LaTeX Macros</h1>
<p>Many of the LaTeX macros used in this document are available in the style file <a href="https://namedtensor.github.io/namedtensor.sty">https://namedtensor.github.io/namedtensor.sty</a>. To use it, put</p>
<blockquote>
<pre><code>\usepackage{namedtensor}</code></pre>
</blockquote>
<p>in the preamble of your LaTeX source file (after <code>\documentclass{article}</code> but before <code>\begin{document}</code>).</p>
<p>The style file contains a small number of macros:</p>
<ul>
<li><p>Basics</p>
<ul>
<li><p>Use <code>\name{foo}</code> to write an axis name: <span class="math inline">\(\ensuremath{\mathsf{foo}}\)</span>.</p></li>
<li><p>Use <code>\mathbb{R}^{\nset{foo}{2}}</code> to write a set of tensors: <span class="math inline">\(\mathbb{R}^{\ensuremath{\ensuremath{\mathsf{foo}}[2]}}\)</span>.</p></li>
<li><p>Use <code>A_{\nidx{foo}{1}}</code> to index a tensor: <span class="math inline">\(A_{\ensuremath{\ensuremath{\mathsf{foo}}(1)}}\)</span>.</p></li>
<li><p>Use <code>A_{\nmov{foo}{bar}}</code> for renaming: <span class="math inline">\(A_{{\ensuremath{\mathsf{foo}}\rightarrow\ensuremath{\mathsf{bar}}}}\)</span>.</p></li>
</ul></li>
<li><p>Binary operators</p>
<ul>
<li><p>Use <code>A \ndot{foo} B</code> for contraction: <span class="math inline">\(A \mathbin{\underset{\ensuremath{\mathsf{foo}}}{\odot}} B\)</span>.</p></li>
<li><p>Use <code>A \ncat{foo} B</code> for concatenation: <span class="math inline">\(A \mathbin{\underset{\ensuremath{\mathsf{foo}}}{\oplus}} B\)</span>.</p></li>
<li><p>In general, you can use <code>\nbin</code> to make a new binary operator with a name under it: <code>A \nbin{foo}{\star} B</code> gives you <span class="math inline">\(A \mathbin{\underset{\ensuremath{\mathsf{foo}}}{\star}} B\)</span>.</p></li>
</ul></li>
<li><p>Functions</p>
<ul>
<li><p>Use <code>\nsum{foo} A</code> for summation: <span class="math inline">\(\sum\limits_{\ensuremath{\mathsf{foo}}} A\)</span>.</p></li>
<li><p>In general, you can use <code>\nfun</code> to make a function with a name under it: <code>\nfun{foo}{qux} A</code> gives you <span class="math inline">\(\mathop{\underset{\ensuremath{\mathsf{foo}}}{\mathrm{qux}}} A\)</span>.</p></li>
</ul></li>
</ul>
<h1 id="sec:definitions"><span class="header-section-number">5</span> Formal Definitions</h1>
<h2 id="records-and-shapes"><span class="header-section-number">5.1</span> Records and shapes</h2>
<p>A <em>named index</em> is a pair, written <span class="math inline">\(\ensuremath{\ensuremath{\mathsf{ax}}(i)}\)</span>, where <span class="math inline">\(\ensuremath{\mathsf{ax}}\)</span> is a <em>name</em> and <span class="math inline">\(i\)</span> is usually a natural number. We write both names and variables ranging over names using sans-serif font.</p>
<p>A <em>record</em> is a set of named indices <span class="math inline">\(\{\ensuremath{\ensuremath{\mathsf{ax_\text{$1$}}}(i_1)}, \ldots, \ensuremath{\ensuremath{\mathsf{ax_\text{$r$}}}(i_r)}\}\)</span>, where <span class="math inline">\(\ensuremath{\mathsf{ax_\text{$1$}}}, \ldots \ensuremath{\mathsf{ax_\text{$r$}}}\)</span> are pairwise distinct names.</p>
<p>An <em>axis</em> is a pair, written <span class="math inline">\(\ensuremath{\ensuremath{\mathsf{ax}}[I]}\)</span>, where <span class="math inline">\(\ensuremath{\mathsf{ax}}\)</span> is a name and <span class="math inline">\(I\)</span> is a set of <em>indices</em>. We deal with axes of the form <span class="math inline">\(\ensuremath{\ensuremath{\mathsf{ax}}[[n]]}\)</span> (that is, <span class="math inline">\(\ensuremath{\ensuremath{\mathsf{ax}}[\{1, \ldots, n\}]}\)</span>) so frequently that we abbreviate this as <span class="math inline">\(\ensuremath{\ensuremath{\mathsf{ax}}[n]}\)</span>.</p>
<p>In many contexts, there is only one axis with name <span class="math inline">\(\ensuremath{\mathsf{ax}}\)</span>, and so we refer to the axis simply as <span class="math inline">\(\ensuremath{\mathsf{ax}}\)</span>. The context always makes it clear whether <span class="math inline">\(\ensuremath{\mathsf{ax}}\)</span> is a name or an axis. If <span class="math inline">\(\ensuremath{\mathsf{ax}}\)</span> is an axis, we write <span class="math inline">\(\ind(\ensuremath{\mathsf{ax}})\)</span> for its index set, and we write <span class="math inline">\(|\ensuremath{\mathsf{ax}}|\)</span> as shorthand for <span class="math inline">\(|\ind(\ensuremath{\mathsf{ax}})|\)</span>.</p>
<p>A <em>shape</em> is a set of axes, written <span class="math inline">\(\ensuremath{\ensuremath{\mathsf{ax_\text{$1$}}}[I_1]} \times \cdots \times \ensuremath{\ensuremath{\mathsf{ax_\text{$r$}}}[I_r]}\)</span>, where <span class="math inline">\(\ensuremath{\mathsf{ax_\text{$1$}}}, \ldots \ensuremath{\mathsf{ax_\text{$r$}}}\)</span> are pairwise distinct names. We write <span class="math inline">\(\emptyset\)</span> for the empty shape. A shape defines a set of records: <span class="math display">\[\rec (\ensuremath{\ensuremath{\mathsf{ax_\text{$1$}}}[I_1]} \times \cdots \times \ensuremath{\ensuremath{\mathsf{ax_\text{$r$}}}[I_r]}) = \left\{\{\ensuremath{\ensuremath{\mathsf{ax_\text{$1$}}}(i_1)}, \ldots, \ensuremath{\ensuremath{\mathsf{ax_\text{$r$}}}(i_r)}\} \mid i_1 \in I_1, \ldots, i_r \in I_r\right\}.\]</span></p>
<p>We say two shapes <span class="math inline">\(\mathcal{S}\)</span> and <span class="math inline">\(\mathcal{T}\)</span> are <em>compatible</em> if whenever <span class="math inline">\(\ensuremath{\ensuremath{\mathsf{ax}}[I]} \in \mathcal{S}\)</span> and <span class="math inline">\(\ensuremath{\ensuremath{\mathsf{ax}}[J]} \in \mathcal{T}\)</span>, then <span class="math inline">\(I = J\)</span>. We say that <span class="math inline">\(\mathcal{S}\)</span> and <span class="math inline">\(\mathcal{T}\)</span> are <em>orthogonal</em> if there is no <span class="math inline">\(\ensuremath{\mathsf{ax}}\)</span> such that <span class="math inline">\(\ensuremath{\ensuremath{\mathsf{ax}}[I]} \in \mathcal{S}\)</span> and <span class="math inline">\(\ensuremath{\ensuremath{\mathsf{ax}}[J]} \in \mathcal{T}\)</span> for any <span class="math inline">\(I\)</span>, <span class="math inline">\(J\)</span>.</p>
<p>If <span class="math inline">\(t \in \rec \mathcal{T}\)</span> and <span class="math inline">\(\mathcal{S} \subseteq \mathcal{T}\)</span>, then we write <span class="math inline">\(\mathopen{}\left.t\right|_{\mathcal{S}}\)</span> for the unique record in <span class="math inline">\(\rec \mathcal{S}\)</span> such that <span class="math inline">\(\mathopen{}\left.t\right|_{\mathcal{S}} \subseteq t\)</span>.</p>
<h2 id="named-tensors-1"><span class="header-section-number">5.2</span> Named tensors</h2>
<p>Let <span class="math inline">\(F\)</span> be a field and let <span class="math inline">\(\mathcal{S}\)</span> be a shape. Then a <em>named tensor over <span class="math inline">\(F\)</span> with shape <span class="math inline">\(\mathcal{S}\)</span></em> is a mapping from <span class="math inline">\(\mathcal{S}\)</span> to <span class="math inline">\(F\)</span>. We write the set of all named tensors with shape <span class="math inline">\(\mathcal{S}\)</span> as <span class="math inline">\(F^{\mathcal{S}}\)</span>.</p>
<p>We don’t make any distinction between a scalar (an element of <span class="math inline">\(F\)</span>) and a named tensor with empty shape (an element of <span class="math inline">\(F^\emptyset\)</span>).</p>
<p>If <span class="math inline">\(A \in F^{\mathcal{S}}\)</span>, then we access an element of <span class="math inline">\(A\)</span> by applying it to a record <span class="math inline">\(s \in \rec \mathcal{S}\)</span>; but we write this using the usual subscript notation: <span class="math inline">\(A_s\)</span> rather than <span class="math inline">\(A(s)\)</span>. To avoid clutter, in place of <span class="math inline">\(A_{\{\ensuremath{\ensuremath{\mathsf{ax_\text{$1$}}}(x_1)}, \ldots, \ensuremath{\ensuremath{\mathsf{ax_\text{$r$}}}(x_r)}\}}\)</span>, we usually write <span class="math inline">\(A_{\ensuremath{\ensuremath{\mathsf{ax_\text{$1$}}}(x_1)}, \ldots, \ensuremath{\ensuremath{\mathsf{ax_\text{$r$}}}(x_r)}}\)</span>. When a named tensor is an expression like <span class="math inline">\((A+B)\)</span>, we surround it with square brackets like this: <span class="math inline">\([A+B]_{\ensuremath{\ensuremath{\mathsf{ax_\text{$1$}}}(x_1)}, \ldots, \ensuremath{\ensuremath{\mathsf{ax_\text{$r$}}}(x_r)}}\)</span>.</p>
<p>We also allow partial indexing. If <span class="math inline">\(A\)</span> is a tensor with shape <span class="math inline">\(\mathcal{T}\)</span> and <span class="math inline">\(s \in \rec \mathcal{S}\)</span> where <span class="math inline">\(\mathcal{S} \subseteq \mathcal{T}\)</span>, then we define <span class="math inline">\(A_s\)</span> to be the named tensor with shape <span class="math inline">\(\mathcal{T} \setminus \mathcal{S}\)</span> such that, for any <span class="math inline">\(t \in \rec (\mathcal{T} \setminus \mathcal{S})\)</span>, <span class="math display">\[\begin{aligned}
\left[A_s\right]_t &amp;= A_{s \cup t}.\end{aligned}\]</span> (For the edge case <span class="math inline">\(\mathcal{T} = \emptyset\)</span>, our definitions for indexing and partial indexing coincide: one gives a scalar and the other gives a tensor with empty shape, but we don’t distinguish between the two.)</p>
<h2 id="sec:tensorfunctions"><span class="header-section-number">5.3</span> Named tensor operations</h2>
<p>In §<a href="#sec:overview" data-reference-type="ref" data-reference="sec:overview">2</a>, we described several classes of functions that can be extended to named tensors. Here, we define how to do this for general functions.</p>
<p>Let <span class="math inline">\(f \colon F^{\mathcal{S}} \rightarrow G^{\mathcal{T}}\)</span> be a function from tensors to tensors. For any shape <span class="math inline">\(\mathcal{S&#39;}\)</span> orthogonal to both <span class="math inline">\(\mathcal{S}\)</span> and <span class="math inline">\(\mathcal{T}\)</span>, we can extend <span class="math inline">\(f\)</span> to: <span class="math display">\[\begin{aligned}
f \colon F^{\mathcal{S} \cup \mathcal{S&#39;}} &amp;\rightarrow G^{\mathcal{T} \cup \mathcal{S&#39;}} \\
[f(A)]_r &amp;= f(A_r) \qquad \text{for all $r \in \rec\mathcal{S&#39;}$.}\end{aligned}\]</span></p>
<p>If <span class="math inline">\(f\)</span> is a multary function, we can extend its arguments to larger shapes, and we don’t have to extend all the arguments with the same names. We consider just the case of two arguments; three or more arguments are analogous. Let <span class="math inline">\(f \colon F^{\mathcal{S}} \times G^{\mathcal{T}} \rightarrow H^{\mathcal{U}}\)</span> be a binary function from tensors to tensors. For any shapes <span class="math inline">\(\mathcal{S&#39;}\)</span> and <span class="math inline">\(\mathcal{T&#39;}\)</span> that are compatible with each other and orthogonal to <span class="math inline">\(\mathcal{S}\)</span> and <span class="math inline">\(\mathcal{T}\)</span>, respectively, and <span class="math inline">\(\mathcal{S&#39;} \cup \mathcal{T&#39;}\)</span> is orthogonal to <span class="math inline">\(\mathcal{U}\)</span>, we can extend <span class="math inline">\(f\)</span> to: <span class="math display">\[\begin{aligned}
f \colon F^{\mathcal{S} \cup \mathcal{S&#39;}} \times G^{\mathcal{T} \cup \mathcal{T&#39;}} &amp;\rightarrow H^{\mathcal{U} \cup \mathcal{S&#39;} \cup \mathcal{T&#39;}} \\
  [f(A,B)]_r &amp;= f\left(A_{\mathopen{}\left.r\right|_{\mathcal{S&#39;}}},B_{\mathopen{}\left.r\right|_{\mathcal{T&#39;}}}\right) \qquad \text{for all $r \in \rec (\mathcal{S&#39;} \cup \mathcal{T&#39;})$.}\end{aligned}\]</span></p>
<p>All of the tensor operations described in §<a href="#sec:operations" data-reference-type="ref" data-reference="sec:operations">2.2</a> can be defined in this way. For example, the contraction operator can be defined as: <span class="math display">\[\begin{aligned}
  \mathbin{\underset{\ensuremath{\mathsf{ax}}}{\odot}} \colon F^{\ensuremath{\ensuremath{\mathsf{ax}}[n]}} \times F^{\ensuremath{\ensuremath{\mathsf{ax}}[n]}} &amp;\rightarrow F \\
  A \mathbin{\underset{\ensuremath{\mathsf{ax}}}{\odot}} B &amp;= \sum_{i=1}^n A_{\ensuremath{\ensuremath{\mathsf{ax}}(i)}} B_{\ensuremath{\ensuremath{\mathsf{ax}}(i)}}.\end{aligned}\]</span></p>
<h1 id="differentiation"><span class="header-section-number">6</span> Differentiation</h1>
<p>If <span class="math inline">\(f\)</span> is a function from order-<span class="math inline">\(m\)</span> tensors to order-<span class="math inline">\(n\)</span> tensors, the partial derivatives of <span class="math inline">\(f\)</span> (evaluated on a tensor <span class="math inline">\(X\)</span>) form an order-<span class="math inline">\((m+n)\)</span> tensor: <span class="math inline">\(m\)</span> “input” axes for the directions in which <span class="math inline">\(X\)</span> could change and <span class="math inline">\(n\)</span> “output” axes for the change in <span class="math inline">\(f(X)\)</span>.</p>
<p>For example, the derivative of a function from vectors to vectors is a matrix (the Jacobian). But using matrix notation, there are conflicting conventions about whether the first axis is the input axis (“denominator layout”) or the output axis (“numerator layout”). The derivative of a function from vectors to matrices or matrices to vectors cannot be represented as a matrix at all, so one must resort to flattening the matrices into vectors.</p>
<p>With tensors, taking derivatives of higher-order tensors with respect to higher-order tensors is not difficult <span class="citation" data-cites="laue+:2018">(Laue, Mitterreiter, and Giesen 2018)</span>. With named tensors, we get the additional advantage of using names to distinguish input and output axes.</p>
<h2 id="definition"><span class="header-section-number">6.1</span> Definition</h2>
<p>Let <span class="math inline">\(f \colon \mathbb{R}^\mathcal{S} \rightarrow \mathbb{R}^\mathcal{T}\)</span>. The derivative of <span class="math inline">\(f\)</span> (evaluated at <span class="math inline">\(X\)</span>) has an input axis for each axis in <span class="math inline">\(\mathcal{S}\)</span> and an output axis for each axis in <span class="math inline">\(\mathcal{T}\)</span>, and they have to have distinct names. So if <span class="math inline">\(\mathcal{S} = \ensuremath{\mathsf{ax_1}} \times \cdots \times \ensuremath{\mathsf{ax_\text{$r$}}}\)</span>, then for each axis name <span class="math inline">\(\ensuremath{\mathsf{ax_\text{$i$}}}\)</span>, let <span class="math inline">\(\ensuremath{\mathsf{ax_\text{$i$}^*}}\)</span> be a new axis name, not in <span class="math inline">\(\mathcal{T}\)</span>, and let <span class="math inline">\(\mathcal{S}^* = \ensuremath{\mathsf{ax_1^*}} \times \cdots \times \ensuremath{\mathsf{ax_\text{$r$}^*}}\)</span>. If <span class="math inline">\(s = \{\ensuremath{\ensuremath{\mathsf{ax_1}}(i_1)}, \ldots, \ensuremath{\ensuremath{\mathsf{ax_\text{$r$}}}(i_r)}\}\)</span>, let <span class="math inline">\(s^* = \{\ensuremath{\ensuremath{\mathsf{ax_1^*}}(i_1)}, \ldots, \ensuremath{\ensuremath{\mathsf{ax_\text{$r$}^*}}(i_r)}\}\)</span>.</p>
<p>Then the derivative of <span class="math inline">\(f\)</span> at <span class="math inline">\(X\)</span> is the tensor with shape <span class="math inline">\(\mathcal{S}^* \times \mathcal{T}\)</span> such that for all <span class="math inline">\(s \in \rec\mathcal{S}\)</span> and <span class="math inline">\(t \in \rec\mathcal{T}\)</span>, <span class="math display">\[\left[\frac{\partial}{\partial X}f(X) \right]_{s^*,t} = \frac{\partial}{\partial X_s} [f(X)]_t.\]</span></p>
<p>We’ll often make use of the following generalization of the identity matrix: <span class="math display">\[\begin{aligned}
  I_\mathcal{S} &amp;\in \mathbb{R}^{\mathcal{S}^* \times \mathcal{S}} \\
  [I_\mathcal{S}]_{s^*, s} &amp;= \begin{cases}
    1 &amp; \text{if $s^* = s$} \\
    0 &amp; \text{otherwise.}
  \end{cases}\end{aligned}\]</span></p>
<h2 id="rules"><span class="header-section-number">6.2</span> Rules</h2>
<p>Now we give some rules for computing derivatives. Unless otherwise indicated, <span class="math inline">\(X\)</span> has shape <span class="math inline">\(\mathcal{S}\)</span>, and <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> are dependent on <span class="math inline">\(x\)</span> and have shapes <span class="math inline">\(\mathcal{U}\)</span> and <span class="math inline">\(\mathcal{V}\)</span>, respectively. <span class="math display">\[\begin{aligned}
  \frac{\partial}{\partial X}X &amp;= I_\mathcal{S} \\
  \frac{\partial}{\partial X}U &amp;= 0 &amp;&amp; \text{$U$ does not depend on $X$} \\
  \frac{\partial}{\partial X}f(U) &amp;= f&#39;(U) \odot \frac{\partial}{\partial X}U &amp;&amp; f \colon \mathbb{R}\rightarrow \mathbb{R}\\
  \frac{\partial}{\partial X}(U + V) &amp;= \frac{\partial}{\partial X}U + \frac{\partial}{\partial X}V \\
  \frac{\partial}{\partial X}\sum\limits_{\ensuremath{\mathsf{ax}}} U &amp;= \sum\limits_{\ensuremath{\mathsf{ax}}} \frac{\partial}{\partial X}U \\
  \frac{\partial}{\partial X}(U \odot V) &amp;= \frac{\partial}{\partial X}U \odot V + U \odot \frac{\partial}{\partial X}V \\
  \frac{\partial}{\partial X}(U \mathbin{\underset{\ensuremath{\mathsf{ax}}}{\odot}} V) &amp;= \frac{\partial}{\partial X}U \mathbin{\underset{\ensuremath{\mathsf{ax}}}{\odot}} V + U \mathbin{\underset{\ensuremath{\mathsf{ax}}}{\odot}} \frac{\partial}{\partial X}V \\
  \frac{\partial}{\partial X}\frac{U}{V} &amp;= \frac{\frac{\partial}{\partial X}U \odot V - U \odot \frac{\partial}{\partial X}V}{V^2} \\
  \frac{\partial}{\partial X}U_r &amp;= \left[\frac{\partial}{\partial X}U\right]_r &amp;&amp; r \in \rec \mathcal{R}, \mathcal{R} \subseteq \mathcal{U} \\
  \frac{\partial}{\partial X}U_{{\ensuremath{\mathsf{ax1}}\rightarrow\ensuremath{\mathsf{ax2}}}} &amp;= \left[\frac{\partial}{\partial X}U\right]_{{\ensuremath{\mathsf{ax1}}\rightarrow\ensuremath{\mathsf{ax2}}}}\end{aligned}\]</span></p>
<p>The chain rule above is for elementwise operations. The general chain rule looks like this for functions of one and two variables; three or more variables are analogous. <span class="math display">\[\begin{aligned}
  \frac{\partial}{\partial X}f(U) &amp;= \frac{\partial}{\partial U} f(U) \mathbin{\underset{\ensuremath{\mathsf{\mathcal{U}^* \mid \mathcal{U}}}}{\odot}} \frac{\partial}{\partial X}U \\
  \frac{\partial}{\partial X}f(U, V) &amp;= \frac{\partial}{\partial U} f(U, V) \mathbin{\underset{\ensuremath{\mathsf{\mathcal{U}^* \mid \mathcal{U}}}}{\odot}} \frac{\partial}{\partial X}U + \frac{\partial}{\partial V} f(U, V) \mathbin{\underset{\ensuremath{\mathsf{\mathcal{V}^* \mid \mathcal{V}}}}{\odot}} \frac{\partial}{\partial X}V\end{aligned}\]</span> where <span class="math inline">\(\mathbin{\underset{\ensuremath{\mathsf{ax1|ax2}}}{\odot}}\)</span> contracts <span class="math inline">\(\ensuremath{\mathsf{ax1}}\)</span> in the left operand with <span class="math inline">\(\ensuremath{\mathsf{ax2}}\)</span> in the right operand: <span class="math inline">\(A \mathbin{\underset{\ensuremath{\mathsf{ax1|ax2}}}{\odot}} B = \sum_i A_{\ensuremath{\ensuremath{\mathsf{ax1}}(i)}} \odot B_{\ensuremath{\ensuremath{\mathsf{ax2}}(i)}}\)</span>.</p>
<h2 id="examples"><span class="header-section-number">6.3</span> Examples</h2>
<p>Here’s an example using these rules to derive the Jacobian for softmax: <span class="math display">\[\begin{aligned}
  \frac{\partial}{\partial X}(\mathop{\underset{\ensuremath{\mathsf{ax}}}{\mathrm{softmax}}} X) &amp;= \frac{\partial}{\partial X}\frac{\exp X}{\sum\limits_{\ensuremath{\mathsf{ax}}} \exp X} \\
    &amp;= \frac{\exp X \odot \frac{\partial}{\partial X}X \odot \sum\limits_{\ensuremath{\mathsf{ax}}} \exp X - \exp X \odot \sum\limits_{\ensuremath{\mathsf{ax}}} (\exp X \odot \frac{\partial}{\partial X}X)}{(\sum\limits_{\ensuremath{\mathsf{ax}}} \exp X)^2} \\
    &amp;= \mathop{\underset{\ensuremath{\mathsf{ax}}}{\mathrm{softmax}}} X \odot \left(\frac{\partial}{\partial X}X - \mathop{\underset{\ensuremath{\mathsf{ax}}}{\mathrm{softmax}}} X \mathbin{\underset{\ensuremath{\mathsf{ax}}}{\odot}} \frac{\partial}{\partial X}X\right) \\
    &amp;= \mathop{\underset{\ensuremath{\mathsf{ax}}}{\mathrm{softmax}}} X \odot \left(I_\mathcal{S} - \mathop{\underset{\ensuremath{\mathsf{ax}}}{\mathrm{softmax}}} X \mathbin{\underset{\ensuremath{\mathsf{ax}}}{\odot}} I_\mathcal{S}\right).\end{aligned}\]</span> To derive the backpropagation rule: <span class="math display">\[\begin{aligned}
  \frac{\partial}{\partial X}f(\mathop{\underset{\ensuremath{\mathsf{ax}}}{\mathrm{softmax}}} X) &amp;= f&#39;(\mathop{\underset{\ensuremath{\mathsf{ax}}}{\mathrm{softmax}}} X) \mathbin{\underset{\ensuremath{\mathsf{\mathcal{S}^*|\mathcal{S}}}}{\odot}} \frac{\partial}{\partial X}\mathop{\underset{\ensuremath{\mathsf{ax}}}{\mathrm{softmax}}} X \\
    &amp;= f&#39;(\mathop{\underset{\ensuremath{\mathsf{ax}}}{\mathrm{softmax}}} X) \mathbin{\underset{\ensuremath{\mathsf{\mathcal{S}^*|\mathcal{S}}}}{\odot}} \mathop{\underset{\ensuremath{\mathsf{ax}}}{\mathrm{softmax}}} X \odot \left(I_\mathcal{S} - \mathop{\underset{\ensuremath{\mathsf{ax}}}{\mathrm{softmax}}} X \mathbin{\underset{\ensuremath{\mathsf{ax}}}{\odot}} I_\mathcal{S}\right).\end{aligned}\]</span></p>
<p>As another example, here are Jacobian and backpropagation rule for Conv1d: <span class="math display">\[\begin{aligned}
  \frac{\partial}{\partial X} \text{Conv1d}(X) &amp;= [W \mathbin{\underset{\ensuremath{\mathsf{chans,kernel}}}{\odot}} C \mathbin{\underset{\ensuremath{\mathsf{seq}}}{\odot}} I_\mathcal{S}]_{{\ensuremath{\mathsf{out}}\rightarrow\ensuremath{\mathsf{seq}}}} \\
  &amp;= [W_{{\ensuremath{\mathsf{chans}}\rightarrow\ensuremath{\mathsf{chans^*}}}} \mathbin{\underset{\ensuremath{\mathsf{kernel}}}{\odot}} C_{\ensuremath{\mathsf{seq}}\rightarrow\ensuremath{\mathsf{seq^*}}}]_{\ensuremath{\mathsf{out}}\rightarrow\ensuremath{\mathsf{seq}}} \\
  \frac{\partial}{\partial X} f(\text{Conv1d}(X)) &amp;= f&#39;(\text{Conv1d}(X)) \mathbin{\underset{\ensuremath{\mathsf{seq^*|seq}}}{\odot}} [W_{{\ensuremath{\mathsf{chans}}\rightarrow\ensuremath{\mathsf{chans^*}}}} \mathbin{\underset{\ensuremath{\mathsf{kernel}}}{\odot}} C_{\ensuremath{\mathsf{seq}}\rightarrow\ensuremath{\mathsf{seq^*}}}]_{\ensuremath{\mathsf{out}}\rightarrow\ensuremath{\mathsf{seq}}} \\
  &amp;= f&#39;(\text{Conv1d}(X)) \mathbin{\underset{\ensuremath{\mathsf{seq^*|out}}}{\odot}} (W_{\ensuremath{\mathsf{chans}}\rightarrow\ensuremath{\mathsf{chans^*}}} \mathbin{\underset{\ensuremath{\mathsf{kernel}}}{\odot}} C_{\ensuremath{\mathsf{seq}}\rightarrow\ensuremath{\mathsf{seq^*}}}) \\
  &amp;= W_{\ensuremath{\mathsf{chans}}\rightarrow\ensuremath{\mathsf{chans^*}}} \mathbin{\underset{\ensuremath{\mathsf{kernel}}}{\odot}} C_{{\ensuremath{\mathsf{seq}}\rightarrow\ensuremath{\mathsf{seq^*}}}} \mathbin{\underset{\ensuremath{\mathsf{out|seq^*}}}{\odot}} f&#39;(\text{Conv1d}(X)).\end{aligned}\]</span></p>
<h2 id="broadcasting"><span class="header-section-number">6.4</span> Broadcasting</h2>
<p>If <span class="math inline">\(f \colon \mathbb{R}^\mathcal{S} \rightarrow \mathbb{R}^\mathcal{T}\)</span>, then recall that <span class="math inline">\(f\)</span> can be extended to <span class="math inline">\(\mathbb{R}^{\mathcal{S} \cup \mathcal{S^+}}\)</span> where <span class="math inline">\(\mathcal{S}\)</span> and <span class="math inline">\(\mathcal{S^+}\)</span> are orthogonal.</p>
<p>It’s more convenient here to notate the derivative of <span class="math inline">\(f\)</span> as <span class="math inline">\(Df\)</span>. If <span class="math inline">\(f\)</span> has two arguments, its partial derivatives are <span class="math inline">\(D_1 f\)</span> and <span class="math inline">\(D_2 f\)</span>.</p>
<p>Although <span class="math inline">\(Df\)</span> extends to <span class="math inline">\(\mathbb{R}^{\mathcal{S} \cup \mathcal{S^+}}\)</span> using the usual broadcasting rules, the extension of the derivative is unfortunately not the derivative of the extension. To avoid confusion, write <span class="math inline">\(f^+\)</span> for the extension: <span class="math display">\[\begin{aligned}
  f^+ \colon \mathbb{R}^{\mathcal{S} \cup \mathcal{S^+}} &amp;\rightarrow \mathbb{R}^{\mathcal{T} \cup \mathcal{S^+}} \\
  f^+(X)_r &amp;= f(X_r).\end{aligned}\]</span> Then the derivative of <span class="math inline">\(f^+\)</span> is: <span class="math display">\[\begin{aligned}
  Df^+ \colon \mathbb{R}^{\mathcal{S}^* \cup \mathcal{{S^+}}^* \cup \mathcal{T} \cup \mathcal{S^+}} &amp;\rightarrow \mathbb{R}^{\mathcal{T} \cup \mathcal{S^+}} \\
  Df^+(X) &amp;= Df(X) \odot I_{\mathcal{S}^+}.\end{aligned}\]</span></p>
<p>Similarly, if <span class="math inline">\(f \colon \mathbb{R}^\mathcal{S} \times \mathbb{R}^\mathcal{T} \rightarrow \mathbb{R}^\mathcal{U}\)</span>, we can extend <span class="math inline">\(f\)</span> to <span class="math inline">\(f^+ \colon \mathbb{R}^\mathcal{S \cup S^+} \times \mathbb{R}^\mathcal{T \cup T^+} \rightarrow \mathbb{R}^\mathcal{U \cup S^+ \cup T^+}\)</span>. Then <span class="math display">\[\begin{aligned}
  D_1 f^+(X, Y) &amp;= D_1 f(X, Y) \odot I_{\mathcal{S}^+} \\
  D_2 f^+(X, Y) &amp;= D_2 f(X, Y) \odot I_{\mathcal{T}^+}.\end{aligned}\]</span></p>
<h1 id="extensions"><span class="header-section-number">7</span> Extensions</h1>
<h2 id="index-types"><span class="header-section-number">7.1</span> Index types</h2>
<p>We have defined an axis as a pair <span class="math inline">\(\ensuremath{\ensuremath{\mathsf{ax}}[I]}\)</span>, where <span class="math inline">\(\ensuremath{\mathsf{ax}}\)</span> is a name and <span class="math inline">\(I\)</span> is a set, usually <span class="math inline">\([n]\)</span> for some <span class="math inline">\(n\)</span>. In this section, we consider some other possibilities for <span class="math inline">\(I\)</span>.</p>
<h3 id="non-integral-types"><span class="header-section-number">7.1.1</span> Non-integral types</h3>
<p>The sets <span class="math inline">\(I\)</span> don’t have to contain integers. For example, if <span class="math inline">\(V\)</span> is the vocabulary of a natural language (<span class="math inline">\(V = \{ \ensuremath{\mathsf{cat}}, \ensuremath{\mathsf{dog}}, \ldots \}\)</span>), we could define a matrix of word embeddings: <span class="math display">\[\begin{aligned}
  E &amp;\in \mathbb{R}^{\ensuremath{\ensuremath{\mathsf{vocab}}[V]} \times \ensuremath{\ensuremath{\mathsf{emb}}[d]}}.\end{aligned}\]</span></p>
<h3 id="integers-with-units"><span class="header-section-number">7.1.2</span> Integers with units</h3>
<p>If <span class="math inline">\(\ensuremath{\mathsf{u}}\)</span> is a symbol and <span class="math inline">\(n &gt; 0\)</span>, define <span class="math inline">\([n]\ensuremath{\mathsf{u}} = \{1\ensuremath{\mathsf{u}}, 2\ensuremath{\mathsf{u}}, \ldots, n\ensuremath{\mathsf{u}}\}\)</span>. You could think of <span class="math inline">\(\ensuremath{\mathsf{u}}\)</span> as analogous to a physical unit, like kilograms. The elements of <span class="math inline">\([n]\ensuremath{\mathsf{u}}\)</span> can be added and subtracted like integers (<span class="math inline">\(a\ensuremath{\mathsf{u}} + b\ensuremath{\mathsf{u}} = (a+b)\ensuremath{\mathsf{u}}\)</span>) or multiplied by unitless integers (<span class="math inline">\(c \cdot a\ensuremath{\mathsf{u}} = (c \cdot a) \ensuremath{\mathsf{u}}\)</span>), but numbers with different units are different (<span class="math inline">\(a \ensuremath{\mathsf{u}} \neq a \ensuremath{\mathsf{v}}\)</span>).</p>
<p>Then the set <span class="math inline">\([n]\ensuremath{\mathsf{u}}\)</span> could be used as an index set, which would prevent the axis from being aligned with another axis that uses different units. For example, if we want to define a tensor representing an image, we might write <span class="math display">\[A \in \mathbb{R}^{\ensuremath{\ensuremath{\mathsf{height}}[[h]\ensuremath{\mathsf{pixels}}]} \times \ensuremath{\ensuremath{\mathsf{width}}[[w]\ensuremath{\mathsf{pixels}}]}}.\]</span> If we have another tensor representing a go board, we might write <span class="math display">\[B \in \mathbb{R}^{\ensuremath{\ensuremath{\mathsf{height}}[[n]\ensuremath{\mathsf{points}}]} \times \ensuremath{\ensuremath{\mathsf{width}}[[n]\ensuremath{\mathsf{points}}]}},\]</span> and even if it happens that <span class="math inline">\(h = w = n\)</span>, it would be incorrect to write <span class="math inline">\(A+B\)</span> because the units do not match.</p>
<h3 id="tuples-of-integers"><span class="header-section-number">7.1.3</span> Tuples of integers</h3>
<p>An index set could also be <span class="math inline">\([m] \times [n]\)</span>, which would be a way of sneaking ordered indices into named tensors, useful for matrix operations. For example, instead of defining an <span class="math inline">\(\text{inv}\)</span> operator that takes two subscripts, we could write <span class="math display">\[\begin{aligned}
  A &amp;\in \mathbb{R}^{\ensuremath{\ensuremath{\mathsf{ax}}[{m\times n}]}} = \mathbb{R}^{\ensuremath{\ensuremath{\mathsf{ax}}[{[m]\times [n]}]}} \\
  B &amp;= \mathop{\underset{\ensuremath{\mathsf{ax}}}{\mathrm{inv}}} A.\end{aligned}\]</span> We could also define an operator <span class="math inline">\(\circ\)</span> for matrix-matrix and matrix-vector multiplication: <span class="math display">\[\begin{aligned}
  c &amp;\in \mathbb{R}^{\ensuremath{\ensuremath{\mathsf{ax}}[n]}} \\
  D &amp;= A \mathbin{\underset{\ensuremath{\mathsf{ax}}}{\circ}} B \mathbin{\underset{\ensuremath{\mathsf{ax}}}{\circ}} c.\end{aligned}\]</span></p>
<h2 id="indexing-with-a-tensor-of-indices"><span class="header-section-number">7.2</span> Indexing with a tensor of indices</h2>
<p>Contributors: Tongfei Chen and Chu-Cheng Lin</p>
<p>NumPy defines two kinds of <em>advanced</em> (also known as <em>fancy</em>) indexing: by integer arrays and by Boolean arrays. Here, we generalize indexing by integer arrays to named tensors. That is, if <span class="math inline">\(A\)</span> is a named tensor with <span class="math inline">\(D\)</span> indices and <span class="math inline">\(\iota^1, \ldots, \iota^D\)</span> are named tensors, called “indexers,” what is <span class="math inline">\(A_{\iota^1, \ldots, \iota^D}\)</span>?</p>
<p>Advanced indexing could be derived by taking a function <span class="math display">\[\begin{aligned}
  \mathop{\underset{\ensuremath{\mathsf{ax}}}{\mathrm{index}}} &amp;\colon F^{\ensuremath{\ensuremath{\mathsf{ax}}[I]}} \times I \rightarrow F \\
  \mathop{\underset{\ensuremath{\mathsf{ax}}}{\mathrm{index}}}(A, i) &amp;= A_{\ensuremath{\ensuremath{\mathsf{ax}}(i)}}\end{aligned}\]</span> and extending it to higher-order tensors in its second argument according to the rules in §<a href="#sec:tensorfunctions" data-reference-type="ref" data-reference="sec:tensorfunctions">5.3</a>. But because that’s somewhat abstract, we give a more concrete definition below.</p>
<p>We first consider the case where all the indexers have the same shape <span class="math inline">\(\mathcal{S}\)</span>: <span class="math display">\[\begin{aligned}
  A &amp;\in F^{\ensuremath{\ensuremath{\mathsf{ax_\text{$1$}}}[I_1]} \times \cdots \times \ensuremath{\ensuremath{\mathsf{ax_\text{$D$}}}[I_D]}} \\
  \iota^d &amp;\in I_d^{\mathcal{S}} &amp; d &amp;= 1, \ldots, D.\end{aligned}\]</span> Then <span class="math inline">\(A_{\iota^1, \ldots, \iota^D}\)</span> is the named tensor with shape <span class="math inline">\(\mathcal{S}\)</span> such that for any <span class="math inline">\(s \in \rec{\mathcal{S}}\)</span>, <span class="math display">\[\begin{aligned}
  [A_{\iota^1, \ldots, \iota^D}]_s &amp;= A_{\iota^1_s, \ldots, \iota^D_s}.\end{aligned}\]</span> More generally, suppose the indexers have different but compatible shapes: <span class="math display">\[\begin{aligned}
  A &amp;\in F^{\ensuremath{\ensuremath{\mathsf{ax_\text{$1$}}}[I_1]} \times \cdots \times \ensuremath{\ensuremath{\mathsf{ax_\text{$D$}}}[I_D]}} \\
  \iota^d &amp;\in I_d^{\mathcal{S}_d} &amp; d &amp;= 1, \ldots, D,\end{aligned}\]</span> where the <span class="math inline">\(\mathcal{S}_d\)</span> are pairwise compatible. Then <span class="math inline">\(A_{\iota^1, \ldots, \iota^D}\)</span> is the named tensor with shape <span class="math inline">\(\mathcal{S} = \bigcup_d \mathcal{S}_d\)</span> such that for any <span class="math inline">\(s \in \rec{\mathcal{S}}\)</span>, <span class="math display">\[\begin{aligned}
  [A_{\iota^1, \ldots, \iota^D}]_s &amp;= A_{\iota^1_{\mathopen{}\left.s\right|_{\mathcal{S}_1}}, \ldots, \iota^D_{\mathopen{}\left.s\right|_{\mathcal{S}_D}}}.\end{aligned}\]</span></p>
<p>Let’s consider a concrete example in natural language processing. Consider a batch of sentences encoded as a sequence of word vectors, that is, a tensor <span class="math inline">\(X \in \mathbb{R}^{\ensuremath{\ensuremath{\mathsf{batch}}[B]} \times \ensuremath{\ensuremath{\mathsf{sent}}[N]} \times \ensuremath{\ensuremath{\mathsf{emb}}[E]}}\)</span>. For each sentence, we would like to take out the encodings of a particular span for each sentence <span class="math inline">\(b \in [B]\)</span> in the batch, resulting in a tensor <span class="math inline">\(Y \in \mathbb{R}^{\ensuremath{\ensuremath{\mathsf{batch}}[B]} \times \ensuremath{\ensuremath{\mathsf{span}}[M]} \times \ensuremath{\ensuremath{\mathsf{emb}}[E]}}\)</span>.</p>
<p>We create a indexer for the <span class="math inline">\(\ensuremath{\mathsf{sent}}\)</span> axis: <span class="math inline">\(\iota \in [N]^{\ensuremath{\ensuremath{\mathsf{batch}}[B]} \times \ensuremath{\ensuremath{\mathsf{span}}[M]}}\)</span> that selects the desired tokens. Also define the function <span class="math display">\[\begin{aligned}
  \mathop{\underset{\ensuremath{\mathsf{ax}}}{\mathrm{arange}}}(I) &amp;\in I^{\ensuremath{\ensuremath{\mathsf{ax}}[I]}} \\
  \left[\mathop{\underset{\ensuremath{\mathsf{ax}}}{\mathrm{arange}}}(I)\right]_{\ensuremath{\ensuremath{\mathsf{ax}}(i)}} &amp;= i\end{aligned}\]</span> which generalizes the NumPy function of the same name.</p>
<p>Then we can write <span class="math display">\[Y = X_{\ensuremath{\ensuremath{\mathsf{batch}}(\iota)}, \ensuremath{\ensuremath{\mathsf{sent}}(\mathop{\underset{\ensuremath{\mathsf{sent}}}{\mathrm{arange}}}(n))}, \ensuremath{\ensuremath{\mathsf{emb}}(\mathop{\underset{\ensuremath{\mathsf{emb}}}{\mathrm{arange}}}(E))}}.\]</span></p>
<h1 id="alternatives"><span class="header-section-number">8</span> Alternatives</h1>
<p>A very frequently asked question is why we haven’t used index notation as used in physics, and the Einstein summation convention in particular. In this notation, axes are ordered, and every equation is written in terms of tensor components. If an index appears on both sides of an equation, then the equation must hold for each value of the index, and if an index appears twice on one side and not on the other, there is an implicit summation over that index. <span class="math display">\[\begin{aligned}
  \text{Attention} \colon \mathbb{R}^{n&#39; \times d_k} \times \mathbb{R}^{n \times d_k} \times \mathbb{R}^{n \times d_v} &amp;\rightarrow \mathbb{R}^{n&#39; \times d_v} \\
  \left[\text{Attention}(Q, K, V)\right]_{i&#39;k} &amp;= \softmax_i \left( \frac{Q_{i&#39;j} K_{ij}}{\sqrt{d_k}} \right) V_{ik}.\end{aligned}\]</span> Because <span class="math inline">\(i&#39;\)</span> and <span class="math inline">\(k\)</span> appear on both sides, the equation must hold over all values of these indices. But because <span class="math inline">\(j\)</span> and <span class="math inline">\(k\)</span> occur twice on only the right-hand side, they are both summed over. We’d have to define exactly what the <span class="math inline">\(i\)</span> under softmax means (<span class="math inline">\(i\)</span> is bound inside the softmax and free outside it), and since softmax doesn’t distribute over addition, we’d need to clarify that the summation over <span class="math inline">\(j\)</span> occurs inside the softmax.</p>
<p>Other than that, this is concise and unambiguous. But it doesn’t really solve the main problem we set out to solve, which is that ordered axes force the author and reader to remember the purpose of each axis. The indices do act as symbolic names for axes (indeed, in <em>abstract</em> index notation, they really are symbols, not variables), but they are temporary names; they could be totally different in the next equation. It would be up to the author to choose to use consistent names, and to do so correctly.</p>
<p>A second issue is that because it depends on repetition of indices to work, index notation can be a little bit more verbose than our notation, particularly for reductions and contractions: <span class="math display">\[\begin{aligned}
  C &amp;= \max_i A_i &amp; C &amp;=\mathop{\underset{\ensuremath{\mathsf{ax}}}{\mathrm{max}}} A \\
  C &amp;= A_i B_i &amp; C &amp;= A \mathbin{\underset{\ensuremath{\mathsf{ax}}}{\odot}} B.\end{aligned}\]</span></p>
<p>Finally, index notation requires us to write out all indices explicitly. So if we wanted to extend attention to multiple heads and minibatches, we would write: <span class="math display">\[\begin{gathered}
  \text{Attention} \colon \mathbb{R}^{B \times H \times n&#39; \times d_k} \times \mathbb{R}^{B \times H \times n \times d_k} \times \mathbb{R}^{B \times H \times n \times d_v} \rightarrow \mathbb{R}^{B \times H \times n&#39; \times d_v} \\
  \left[\text{Attention}(Q, K, V)\right]_{bhi&#39;k} = \softmax_i \left( \frac{Q_{bhi&#39;j} K_{bhij}}{\sqrt{d_k}} \right) V_{bhik}.\end{gathered}\]</span> We could adopt a convention that extends a function on tensors to tensors that have extra axes to the <em>left</em>, but such conventions tend to lead to messy reordering and squeezing/unsqueezing of axes. Named axes make this unnecessary.</p>
<h1 id="acknowledgements" class="unnumbered">Acknowledgements</h1>
<p>Thanks to Ekin Akyürek, Colin McDonald, Adam Poliak, Matt Post, Chung-chieh Shan, Nishant Sinha, and Yee Whye Teh for their input to this document (or the ideas in it).</p>
<h1 id="references" class="unnumbered">References</h1>
<div id="refs" class="references">
<div id="ref-chen2017typesafe">
<p>Chen, Tongfei. 2017. “Typesafe Abstractions for Tensor Operations.” In <em>Proceedings of the 8th Acm Sigplan International Symposium on Scala</em>, 45–50. SCALA 2017. <a href="https://doi.org/10.1145/3136000.3136001">https://doi.org/10.1145/3136000.3136001</a>.</p>
</div>
<div id="ref-numpy">
<p>Harris, Charles R., K. Jarrod Millman, Stéfan J. van der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, et al. 2020. “Array Programming with NumPy.” <em>Nature</em> 585 (7825): 357–62. <a href="https://doi.org/10.1038/s41586-020-2649-2">https://doi.org/10.1038/s41586-020-2649-2</a>.</p>
</div>
<div id="ref-xarray">
<p>Hoyer, Stephan, and Joe Hamman. 2017. “xarray: N-D Labeled Arrays and Datasets in Python.” <em>Journal of Open Research Software</em> 5 (1): 10. <a href="https://doi.org/http://doi.org/10.5334/jors.148">https://doi.org/http://doi.org/10.5334/jors.148</a>.</p>
</div>
<div id="ref-laue+:2018">
<p>Laue, Soeren, Matthias Mitterreiter, and Joachim Giesen. 2018. “Computing Higher Order Derivatives of Matrix and Tensor Expressions.” In <em>Advances in Neural Information Processing Systems</em>, edited by S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, 31:2750–9. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper/2018/file/0a1bf96b7165e962e90cb14648c9462d-Paper.pdf">https://proceedings.neurips.cc/paper/2018/file/0a1bf96b7165e962e90cb14648c9462d-Paper.pdf</a>.</p>
</div>
<div id="ref-maclaurin+:2019">
<p>Maclaurin, Dougal, Alexey Radul, Matthew J. Johnson, and Dimitrios Vytiniotis. 2019. “Dex: Array Programming with Typed Indices.” In <em>NeurIPS Workshop on Program Transformations for Ml</em>. <a href="https://openreview.net/forum?id=rJxd7vsWPS">https://openreview.net/forum?id=rJxd7vsWPS</a>.</p>
</div>
<div id="ref-pytorch">
<p>Paszke, Adam, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, et al. 2019. “PyTorch: An Imperative Style, High-Performance Deep Learning Library.” In <em>Advances in Neural Information Processing Systems 32</em>, edited by H. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alché-Buc, E. Fox, and R. Garnett, 8024–35. Curran Associates, Inc. <a href="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf">http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf</a>.</p>
</div>
<div id="ref-namedtensor">
<p>Rush, Alexander. 2019. “Named Tensors.” <a href="https://github.com/harvardnlp/NamedTensor">https://github.com/harvardnlp/NamedTensor</a>.</p>
</div>
<div id="ref-tsalib">
<p>Sinha, Nishant. 2018. “Tensor Shape (Annotation) Library.” <a href="https://github.com/ofnote/tsalib">https://github.com/ofnote/tsalib</a>.</p>
</div>
<div id="ref-named-tensors">
<p>Torch Contributors. 2019. “Named Tensors.” <a href="https://pytorch.org/docs/stable/named_tensor.html">https://pytorch.org/docs/stable/named_tensor.html</a>.</p>
</div>
<div id="ref-vaswani+:2017">
<p>Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” In <em>Advances in Neural Information Processing Systems</em>, edited by I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, 30:5998–6008. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</a>.</p>
</div>
</div>
</body>
</html>
