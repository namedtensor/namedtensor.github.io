<!doctype html>
<html >
<head>
    
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <!--[if lt IE 9]>
                <script src="http://css3-mediaqueries-js.googlecode.com/svn/trunk/css3-mediaqueries.js"></script>
        <![endif]-->
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta http-equiv="Content-Style-Type" content="text/css" />

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: { equationNumbers: { autoNumber: "all" } }
  });
</script>
<script type="text/x-mathjax-config;executed=true">
  MathJax.Hub.Config({ TeX: { equationNumbers: {autoNumber: "AMS"} } });
</script>

<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <link rel="stylesheet" href="https://vanillacss.com/vanilla.css">

  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <meta name="author" content="David Chiang and Sasha Rush" />
  <title>Named Tensor Notation</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <script src="/usr/share/javascript/mathjax/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

    
    <link href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.1/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-WskhaSGFgHYWDcbwN70/dfYBj47jz9qbsMId/iRN3ewGhXQFZCSftd1LZCfmhktB" crossorigin="anonymous">
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.1/js/bootstrap.min.js" integrity="sha384-smHYKdLADwkXOn1EmN1qk/HfnUcbVRZyYmZ4qpPea6sjB/pTJ0euyQp0Mk8ck+5T" crossorigin="anonymous"></script>
    <!-- <link rel="stylesheet" type="text/css" href="template.css" /> -->
    <link rel="stylesheet" type="text/css" href="https://cdn.rawgit.com/diversen/pandoc-bootstrap-adaptive-template/959c3622/template.css" />

    <link href="https://vjs.zencdn.net/5.4.4/video-js.css" rel="stylesheet" />

    <script src="https://code.jquery.com/jquery-2.2.1.min.js"></script>
    <!-- <script type='text/javascript' src='menu/js/jquery.cookie.js'></script> -->
    <!-- <script type='text/javascript' src='menu/js/jquery.hoverIntent.minified.js'></script> -->
    <!-- <script type='text/javascript' src='menu/js/jquery.dcjqaccordion.2.7.min.js'></script> -->
    <!-- <link href="menu/css/skins/blue.css" rel="stylesheet" type="text/css" /> -->
    <!-- <link href="menu/css/skins/graphite.css" rel="stylesheet" type="text/css" /> -->
    <!-- <link href="menu/css/skins/grey.css" rel="stylesheet" type="text/css" /> -->
    <!-- <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->
    <!-- <script src="script.js"></script> -->
    <!-- <script src="jquery.sticky-kit.js "></script> -->
    <script type='text/javascript' src='https://cdn.rawgit.com/diversen/pandoc-bootstrap-adaptive-template/959c3622/menu/js/jquery.cookie.js'></script>
    <script type='text/javascript' src='https://cdn.rawgit.com/diversen/pandoc-bootstrap-adaptive-template/959c3622/menu/js/jquery.hoverIntent.minified.js'></script>
    <script type='text/javascript' src='https://cdn.rawgit.com/diversen/pandoc-bootstrap-adaptive-template/959c3622/menu/js/jquery.dcjqaccordion.2.7.min.js'></script>

    <link href="https://cdn.rawgit.com/diversen/pandoc-bootstrap-adaptive-template/959c3622/menu/css/skins/blue.css" rel="stylesheet" type="text/css" />
    <link href="https://cdn.rawgit.com/diversen/pandoc-bootstrap-adaptive-template/959c3622/menu/css/skins/graphite.css" rel="stylesheet" type="text/css" />
    <link href="https://cdn.rawgit.com/diversen/pandoc-bootstrap-adaptive-template/959c3622/menu/css/skins/grey.css" rel="stylesheet" type="text/css" />
    <link href="https://cdn.rawgit.com/ryangrose/easy-pandoc-templates/948e28e5/css/elegant_bootstrap.css" rel="stylesheet" type="text/css" />
  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  
    <script src="https://cdn.rawgit.com/diversen/pandoc-bootstrap-adaptive-template/959c3622/script.js"></script>
  
    <script src="https://cdn.rawgit.com/diversen/pandoc-bootstrap-adaptive-template/959c3622/jquery.sticky-kit.js"></script>
    <meta name="generator" content="pandoc" />
  <meta name="author" content="David Chiang University of Notre Dame" />
  <meta name="author" content="Alexander M. Rush Cornell University" />
  <meta name="author" content="Boaz Barak Harvard University" />
  <title>Named Tensor Notation</title> 
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">body { font-family: Domine, Georgia, Palatino, 'Palatino Linotype', Times, 'Times New Roman', serif} h1, h2, h3, h4, h5, h6 { font-family: Montserrat, sans }</style>
  <script src="/usr/share/javascript/mathjax/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
</head>
<body>

<div style="display:none">
\(
  \require{ams}
  \DeclareMathOperator*{\softmax}{softmax}
  \DeclareMathOperator{\ind}{ind}
  \DeclareMathOperator{\rec}{rec}
  \newcommand{\ensuremath}[1]{#1}
  \newcommand{\vdotswithin}[1]{\vdots}
\)
</div>
    
    <div class="navbar navbar-static-top navbar-expand-sm px-0 pt-0">
    <div class="navbar-inner container-fluid">
      <div class="container">
        <span class="doc-title"><strong>Named Tensor Notation</strong> </span>
        
        <ul class="nav pull-right doc-info">
                    <li><p class="navbar-text">David Chiang<br />
University of Notre Dame</p></li>
                    <li><p class="navbar-text">Alexander M. Rush<br />
Cornell University</p></li>
                    <li><p class="navbar-text">Boaz Barak<br />
Harvard University</p></li>
                              <li><p class="navbar-text">Version 1.0</p></li>
                  </ul>
      </div>
    </div>
  </div>
  <div class="container">
      <div class="row">
            <div id="TOC" class="col-sm-7 col-lg-3">
        <div class="well toc">
          <ul><li> <a href="https://github.com/namedtensor/notation">Source + Discussions</a>
              </li></ul>
        <ul>
        <li><a href="#sec:intro"><span class="toc-section-number">1</span> Introduction</a></li>
        <li><a href="#sec:overview"><span class="toc-section-number">2</span> Informal Overview</a></li>
        <li><a href="#sec:examples"><span class="toc-section-number">3</span> Examples</a></li>
        <li><a href="#latex-macros"><span class="toc-section-number">4</span> LaTeX Macros</a></li>
        <li><a href="#sec:definitions"><span class="toc-section-number">5</span> Formal Definitions</a></li>
        <li><a href="#differentiation"><span class="toc-section-number">6</span> Differentiation</a></li>
        <li><a href="#alternatives"><span class="toc-section-number">7</span> Alternatives</a></li>
        <li><a href="#acknowledgements">Acknowledgements</a></li>
        <li><a href="#references">References</a></li>
        </ul>

        </div>
      </div>
            <div class="col-9">
            <h1 id="sec:intro"><span class="header-section-number">1</span> Introduction</h1>
<p>Most papers about neural networks use the notation of vectors and matrices from applied linear algebra. This notation is optimized for talking about vector spaces, but becomes cumbersome when talking about neural networks. Consider the following equation <span class="citation" data-cites="vaswani+:2017">(Vaswani et al. 2017)</span>: <span class="math display">\[\text{Attention}(Q, K, V) = \softmax \left( \frac{QK^\top}{\sqrt{d_k}} \right) V.\]</span> where <span class="math inline">\(Q\)</span>, <span class="math inline">\(K\)</span>, and <span class="math inline">\(V\)</span> (for query, key, and value, respectively) are sequences of feature vectors, packed into matrices. Does the product <span class="math inline">\(QK^\top\)</span> sum over the sequence, or over the features? It sums over columns, but there’s not enough information to know what the columns represent. Is the softmax taken over the query sequence or the key sequence? The usual notation doesn’t even offer a way to answer this question. With multiple attention heads or multiple sentences in a minibatch, the notation becomes more difficult still.</p>
<p>Here, we propose mathematical notation for tensors with <em>named axes</em>. The notation has a formal underpinning, but is hopefully intuitive enough that machine learning researchers can understand it without much effort.</p>
<p>In our notation, the above equation becomes <span class="math display">\[\begin{aligned}
  \text{Attention} \colon \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}seq}}&#39; \times \ensuremath{\mathsf{\vphantom{fg}key}}} \times \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}seq}}\times \ensuremath{\mathsf{\vphantom{fg}key}}} \times \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}seq}}\times\ensuremath{\mathsf{\vphantom{fg}val}}} &amp;\rightarrow \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}seq}}&#39; \times \ensuremath{\mathsf{\vphantom{fg}val}}} \\
  \text{Attention}(Q,K,V) = \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}seq}}}}{\vphantom{fg}\mathrm{softmax}}} \left( \frac{Q \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}key}}}}{\vphantom{fg}\odot}} K}{\sqrt{|\ensuremath{\mathsf{\vphantom{fg}key}}|}} \right) \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}seq}}}}{\vphantom{fg}\odot}} V.\end{aligned}\]</span> The tensor <span class="math inline">\(K\)</span> has axes for the sequence (<span class="math inline">\(\mathsf{\vphantom{fg}seq}\)</span>) and for the key features (<span class="math inline">\(\mathsf{\vphantom{fg}key}\)</span>), instead of rows or columns, so the reader does not need to remember which is which. The dot product <span class="math inline">\(Q \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}key}}}}{\vphantom{fg}\odot}} K\)</span> is explicitly over the <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}key}}\)</span> axis. The resulting tensor has a <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}seq}}\)</span> axis for the key sequence and a <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}seq}}&#39;\)</span> axis for the query sequence, and the softmax is explicitly over <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}seq}}\)</span>, as is the dot product with <span class="math inline">\(V\)</span>. This formula works as written if we add a <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}heads}}\)</span> axis for multiple attention heads, or a <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}batch}}\)</span> axis for multiple sequences in a minibatch.</p>
<p>Our notation is inspired by libraries for programming with multidimensional arrays <span class="citation" data-cites="numpy pytorch">(Harris et al. 2020; Paszke et al. 2019)</span> and extensions that use named axes, like xarray <span class="citation" data-cites="xarray">(Hoyer and Hamman 2017)</span>, Nexus <span class="citation" data-cites="chen2017typesafe">(Chen 2017)</span>, tsalib <span class="citation" data-cites="tsalib">(Sinha 2018)</span>, NamedTensor <span class="citation" data-cites="namedtensor">(Rush 2019)</span>, named tensors in PyTorch <span class="citation" data-cites="named-tensors">(Torch Contributors 2019)</span>, and Dex <span class="citation" data-cites="maclaurin+:2019">(Maclaurin et al. 2019)</span>. However, our focus is on mathematical notation rather than code.</p>
<p>The source code for this document can be found at <a href="https://github.com/namedtensor/notation/">https://github.com/namedtensor/notation/</a>. We invite anyone to make comments on this proposal by submitting issues or pull requests on this repository.</p>
<h1 id="sec:overview"><span class="header-section-number">2</span> Informal Overview</h1>
<p>In standard notation, a vector, matrix, or tensor is indexed by an integer or sequence of integers. If <span class="math inline">\(A \in \mathbb{R}^{3\times3}\)</span>, then the order of the two axes matters: <span class="math inline">\(A_{1,3}\)</span> and <span class="math inline">\(A_{3,1}\)</span> are not the same element. It’s up to the reader to remember what each axis of each tensor is for. We think this is a problem and propose a solution.</p>
<h2 id="named-tensors"><span class="header-section-number">2.1</span> Named tensors</h2>
<p>In a <em>named tensor</em>, we give each axis a name. For example, if <span class="math inline">\(A\)</span> represents an image, we can make it a named tensor like so (writing it two equivalent ways to show that the order of axes does not matter): <span class="math display">\[\begin{aligned}
  A &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}height}}[3] \times \ensuremath{\mathsf{\vphantom{fg}width}}[3]} = \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}width}}[3] \times \ensuremath{\mathsf{\vphantom{fg}height}}[3]} \\
  A &amp;= \ensuremath{\mathsf{\vphantom{fg}height}}\begin{array}[b]{@{}c@{}}\ensuremath{\mathsf{\vphantom{fg}width}}\\\begin{bmatrix}
    3 &amp; 1 &amp; 4 \\
    1 &amp; 5 &amp; 9 \\
    2 &amp; 6 &amp; 5
  \end{bmatrix}\end{array} = \ensuremath{\mathsf{\vphantom{fg}width}}\begin{array}[b]{@{}c@{}}\ensuremath{\mathsf{\vphantom{fg}height}}\\\begin{bmatrix}
    3 &amp; 1 &amp; 2 \\
    1 &amp; 5 &amp; 6 \\
    4 &amp; 9 &amp; 5
  \end{bmatrix}\end{array}.\end{aligned}\]</span></p>
<p>We access elements of <span class="math inline">\(A\)</span> using named indices, whose order again does not matter: <span class="math inline">\(A_{\ensuremath{\mathsf{\vphantom{fg}height}}(1), \ensuremath{\mathsf{\vphantom{fg}width}}(3)} = A_{\ensuremath{\mathsf{\vphantom{fg}width}}(3), \ensuremath{\mathsf{\vphantom{fg}height}}(1)} = 4\)</span>. We also allow partial indexing: <span class="math display">\[\begin{aligned}
A_{\ensuremath{\mathsf{\vphantom{fg}height}}(1)} &amp;= \begin{array}[b]{@{}c@{}}\ensuremath{\mathsf{\vphantom{fg}width}}\\\begin{bmatrix}
  3 &amp; 1 &amp; 4
\end{bmatrix}\end{array}
&amp;
A_{\ensuremath{\mathsf{\vphantom{fg}width}}(3)} &amp;= \begin{array}[b]{@{}c@{}}\ensuremath{\mathsf{\vphantom{fg}height}}\\\begin{bmatrix}
  4 &amp; 9 &amp; 5
\end{bmatrix}\end{array}.\end{aligned}\]</span></p>
<p>In many contexts, an axis name is used with only one size. If so, we can simply write <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}height}}\)</span> for the unique axis with name <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}height}}\)</span>, as in <span class="math inline">\(\mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}height}}\times \ensuremath{\mathsf{\vphantom{fg}width}}}\)</span>. We can leave the size of an axis unspecified at first, and specify its size later (like in a section on experimental details): for example, <span class="math inline">\(|\ensuremath{\mathsf{\vphantom{fg}height}}|=|\ensuremath{\mathsf{\vphantom{fg}width}}|=28\)</span> to specify its exact size or just <span class="math inline">\(|\ensuremath{\mathsf{\vphantom{fg}height}}|=|\ensuremath{\mathsf{\vphantom{fg}width}}|\)</span> to specify that it’s a square image.</p>
<p>What are good choices for axis names? We recommend meaningful <em>words</em> instead of single letters, and we recommend words that describe a <em>whole</em> rather than its parts. For example, if we wanted <span class="math inline">\(A\)</span> to have red, green, and blue channels, we’d name the axis <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}chans}}\)</span>, and if we wanted to represent a minibatch of images, we’d name the axis <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}batch}}\)</span>. Please see §<a href="#sec:examples" data-reference-type="ref" data-reference="sec:examples">3</a> for more examples.</p>
<h2 id="sec:operations"><span class="header-section-number">2.2</span> Named tensor operations</h2>
<p>Operations on named tensors are defined by taking a function on low-order tensors and extending it to higher-order tensors.</p>
<h3 id="elementwise-operations-and-broadcasting"><span class="header-section-number">2.2.1</span> Elementwise operations and broadcasting</h3>
<p>Any function from a scalar to a scalar can be applied elementwise to a named tensor, and any function from two scalars to a scalar can be applied to two named tensors with the same shape. For example: <span class="math display">\[\frac1{1+\exp(-A)} = \ensuremath{\mathsf{\vphantom{fg}height}}\begin{array}[b]{@{}c@{}}\ensuremath{\mathsf{\vphantom{fg}width}}\\\begin{bmatrix}
  \frac 1{1+\exp(-3)} &amp; \frac 1{1+\exp(-1)} &amp; \frac 1{1+\exp(-4)} \\[1ex]
  \frac 1{1+\exp(-1)} &amp; \frac 1{1+\exp(-5)} &amp; \frac 1{1+\exp(-9)} \\[1ex]
  \frac 1{1+\exp(-2)} &amp; \frac 1{1+\exp(-6)} &amp; \frac 1{1+\exp(-5)}
\end{bmatrix}\end{array}.\]</span></p>
<p>But if we apply a binary function/operator to tensors with different shapes, they are <em>broadcast</em> against each other (similarly to NumPy and derivatives). Let <span class="math display">\[\begin{aligned}
  x &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}height}}[3]} &amp; y &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}width}}[3]} \\
  x &amp;= \ensuremath{\mathsf{\vphantom{fg}height}}\begin{array}[b]{@{}c@{}}\\\begin{bmatrix}
    2 \\ 7 \\ 1
  \end{bmatrix}\end{array} &amp; 
  y &amp;= \begin{array}[b]{@{}c@{}}\ensuremath{\mathsf{\vphantom{fg}width}}\\\begin{bmatrix}
    1 &amp; 4 &amp; 1
  \end{bmatrix}\end{array}.\end{aligned}\]</span> (We write <span class="math inline">\(x\)</span> as a column just to make the broadcasting easier to visualize.) Then, to evaluate <span class="math inline">\(A+x\)</span>, we effectively replace <span class="math inline">\(x\)</span> with a new tensor <span class="math inline">\(x&#39;\)</span> that contains a copy of <span class="math inline">\(x\)</span> for every index of axis <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}width}}\)</span>. Likewise for <span class="math inline">\(A+y\)</span>: <span class="math display">\[\begin{aligned}
A + x &amp;= \ensuremath{\mathsf{\vphantom{fg}height}}\begin{array}[b]{@{}c@{}}\ensuremath{\mathsf{\vphantom{fg}width}}\\\begin{bmatrix}
  3+2 &amp; 1+2 &amp; 4+2 \\
  1+7 &amp; 5+7 &amp; 9+7 \\
  2+1 &amp; 6+1 &amp; 5+1
\end{bmatrix}\end{array} &amp;
A + y &amp;= \ensuremath{\mathsf{\vphantom{fg}height}}\begin{array}[b]{@{}c@{}}\ensuremath{\mathsf{\vphantom{fg}width}}\\\begin{bmatrix}
  3+1 &amp; 1+4 &amp; 4+1 \\
  1+1 &amp; 5+4 &amp; 9+1 \\
  2+1 &amp; 6+4 &amp; 5+1
\end{bmatrix}\end{array}.\end{aligned}\]</span></p>
<h3 id="sec:reductions"><span class="header-section-number">2.2.2</span> Reductions</h3>
<p>The same broadcasting rules apply to functions from vectors to scalars, called <em>reductions</em>. We always specify which axis a reduction applies to using a subscript (equivalent to the <code>axis</code> argument in NumPy and <code>dim</code> in PyTorch).</p>
<p>See §<a href="#sec:commonops" data-reference-type="ref" data-reference="sec:commonops">5.4</a> for some common reductions. Here we take summation as an example. <span class="math display">\[\begin{aligned}
\sum\limits_{\substack{\ensuremath{\mathsf{\vphantom{fg}height}}}} A &amp;= \sum_i A_{\ensuremath{\mathsf{\vphantom{fg}height}}(i)} = \begin{array}[b]{@{}c@{}}\ensuremath{\mathsf{\vphantom{fg}width}}\\\begin{bmatrix}
  3+1+2 &amp; 1+5+6 &amp; 4+9+5
\end{bmatrix}\end{array}
\\
\sum\limits_{\substack{\ensuremath{\mathsf{\vphantom{fg}width}}}} A &amp;= \sum_j A_{\ensuremath{\mathsf{\vphantom{fg}width}}(j)} = \begin{array}[b]{@{}c@{}}\ensuremath{\mathsf{\vphantom{fg}height}}\\\begin{bmatrix}
  3+1+4 &amp; 1+5+9 &amp; 2+6+5
\end{bmatrix}\end{array}.\end{aligned}\]</span></p>
<p>We can also write multiple names to sum over multiple axes: <span class="math display">\[\sum\limits_{\substack{\ensuremath{\mathsf{\vphantom{fg}height}}\\ \ensuremath{\mathsf{\vphantom{fg}width}}}} A = \sum_i \sum_j A_{\ensuremath{\mathsf{\vphantom{fg}height}}(i),\ensuremath{\mathsf{\vphantom{fg}width}}(j)} = 3+1+4+1+5+9+2+6+5.\]</span> But a summation with an index variable (like <span class="math inline">\(i\)</span> or <span class="math inline">\(j\)</span> above) is a standard summation over values of that variable, and a summation with no subscript is a standard summation over a set.</p>
<p>The vector dot-product is a function from <em>two</em> vectors to a scalar, which generalizes to named tensors to give the ubiquitous <em>contraction</em> operator. You can think of it as elementwise multiplication, then summation over one axis: <span class="math display">\[\begin{aligned}
%A \ndot{\height} x &amp;= \sum_i A_{\height(i)} x_{\height(i)} = \nmatrix{}{\width}{
%  3\cdot2 + 1\cdot7 + 2\cdot1 &amp; 1\cdot2 + 5\cdot7 + 6\cdot1 &amp; 4\cdot2 + 9\cdot7 + 5\cdot 1
%} \\
A \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}width}}}}{\vphantom{fg}\odot}} y &amp;= \sum_j A_{\ensuremath{\mathsf{\vphantom{fg}width}}(j)} \, y_{\ensuremath{\mathsf{\vphantom{fg}width}}(j)} = \ensuremath{\mathsf{\vphantom{fg}height}}\begin{array}[b]{@{}c@{}}\\\begin{bmatrix}
  3\cdot 1 + 1\cdot 4 + 4\cdot 1 \\
  1\cdot 1 + 5\cdot 4 + 9\cdot 1 \\
  2\cdot 1 + 6\cdot 4 + 5\cdot 1
\end{bmatrix}\end{array}.\end{aligned}\]</span></p>
<p>Again, we can write multiple names to contract multiple axes at once. A <span class="math inline">\(\odot\)</span> with no axis name under it contracts zero axes and is equivalent to elementwise multiplication, so we use <span class="math inline">\(\odot\)</span> for elementwise multiplication as well.</p>
<p>The contraction operator can be used for many multiplication-like operations: <span class="math display">\[\begin{aligned}
  x \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}height}}}}{\vphantom{fg}\odot}} x &amp;= \sum_i x_{\ensuremath{\mathsf{\vphantom{fg}height}}(i)} \, x_{\ensuremath{\mathsf{\vphantom{fg}height}}(i)} &amp;&amp; \text{inner product} \\
  [x \odot y]_{\ensuremath{\mathsf{\vphantom{fg}height}}(i), \ensuremath{\mathsf{\vphantom{fg}width}}(j)} &amp;= x_{\ensuremath{\mathsf{\vphantom{fg}height}}(i)} \, y_{\ensuremath{\mathsf{\vphantom{fg}width}}(j)} &amp;&amp; \text{outer product} \\
  A \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}width}}}}{\vphantom{fg}\odot}} y &amp;= \sum_i A_{\ensuremath{\mathsf{\vphantom{fg}width}}(i)} \, y_{\ensuremath{\mathsf{\vphantom{fg}width}}(i)} &amp;&amp; \text{matrix-vector product} \\
  x \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}height}}}}{\vphantom{fg}\odot}} A &amp;= \sum_i x_{\ensuremath{\mathsf{\vphantom{fg}height}}(i)} \, A_{\ensuremath{\mathsf{\vphantom{fg}height}}(i)} &amp;&amp; \text{vector-matrix product} \\
  A \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}width}}}}{\vphantom{fg}\odot}} B &amp;= \sum_i A_{\ensuremath{\mathsf{\vphantom{fg}width}}(i)} \odot B_{\ensuremath{\mathsf{\vphantom{fg}width}}(i)} &amp;&amp; \text{matrix-matrix product}~(B \in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}width}}\times \ensuremath{\mathsf{\vphantom{fg}width}}&#39;})\end{aligned}\]</span></p>
<h3 id="renaming-and-reshaping"><span class="header-section-number">2.2.3</span> Renaming and reshaping</h3>
<p>It’s often useful to rename an axis (analogous to a transpose operation in standard notation): <span class="math display">\[A_{\ensuremath{\mathsf{\vphantom{fg}height}}\rightarrow\ensuremath{\mathsf{\vphantom{fg}height}}&#39;} = \ensuremath{\mathsf{\vphantom{fg}height}}&#39;\begin{array}[b]{@{}c@{}}\ensuremath{\mathsf{\vphantom{fg}width}}\\\begin{bmatrix}
  3 &amp; 1 &amp; 4 \\
  1 &amp; 5 &amp; 9 \\
  2 &amp; 6 &amp; 5 \\
\end{bmatrix}\end{array}.\]</span> We can also reshape two or more axes into one axis: <span class="math display">\[A_{(\ensuremath{\mathsf{\vphantom{fg}height}},\ensuremath{\mathsf{\vphantom{fg}width}})\rightarrow\ensuremath{\mathsf{\vphantom{fg}layer}}} = \begin{array}[b]{@{}c@{}}\ensuremath{\mathsf{\vphantom{fg}layer}}\\\begin{bmatrix}
    3 &amp; 1 &amp; 4 &amp; 1 &amp; 5 &amp; 9 &amp; 2 &amp; 6 &amp; 5
  \end{bmatrix}\end{array}\]</span> The order of elements in the new axis is undefined. If you need a particular order, you can write a more specific definition.</p>
<h1 id="sec:examples"><span class="header-section-number">3</span> Examples</h1>
<p>In this section we give a series of examples illustrating how to use named tensors in various situations, mostly related to machine learning. Many of these examples use functions that the reader can probably guess the meaning of; if not, please see §<a href="#sec:commonops" data-reference-type="ref" data-reference="sec:commonops">5.4</a> for definitions.</p>
<h2 id="building-blocks"><span class="header-section-number">3.1</span> Building blocks</h2>
<h3 id="feedforward-neural-networks"><span class="header-section-number">3.1.1</span> Feedforward neural networks</h3>
<p>A feedforward neural network looks like this: <span class="math display">\[\begin{aligned}
  X^0 &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}input}}} \\
  X^1 &amp;= \sigma(W^1 \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}input}}}}{\vphantom{fg}\odot}} X^0 + b^1) &amp; W^1 &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}hidden}}_1 \times \ensuremath{\mathsf{\vphantom{fg}input}}} &amp; b^1 &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}hidden}}_1} \\
  X^2 &amp;= \sigma(W^2 \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}hidden}}_1}}{\vphantom{fg}\odot}} X^1 + b^2) &amp; W^2 &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}hidden}}_2 \times \ensuremath{\mathsf{\vphantom{fg}hidden}}_1} &amp; b^2 &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}hidden}}_2} \\
  X^3 &amp;= \sigma(W^3 \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}hidden}}_2}}{\vphantom{fg}\odot}} X^2 + b^3) &amp; W^3 &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}out}}\times \ensuremath{\mathsf{\vphantom{fg}hidden}}_2} &amp; b^3 &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}out}}}\end{aligned}\]</span> The layer sizes can be set by writing <span class="math inline">\(|\ensuremath{\mathsf{\vphantom{fg}input}}| = 100\)</span>, etc.</p>
<p>If you don’t like repeating the equations for fully-connected layers, you can put them inside a function: <span class="math display">\[\begin{aligned}
  \text{FullConn}^l(x) &amp;= \sigma\left(W^l \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}layer}}}}{\vphantom{fg}\odot}} x + b^l\right)_{\ensuremath{\mathsf{\vphantom{fg}layer}}&#39;\rightarrow\ensuremath{\mathsf{\vphantom{fg}layer}}}\end{aligned}\]</span> where <span class="math display">\[\begin{aligned}
  W^l &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}layer}}&#39;[n_l] \times \ensuremath{\mathsf{\vphantom{fg}layer}}[n_{l-1}]} \\
  b^l &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}layer}}&#39;[n_l]}.\end{aligned}\]</span> A couple of things are new here. First, <span class="math inline">\(\text{FullConn}^l\)</span> encapsulates both the equation for layer <span class="math inline">\(l\)</span> as well as its parameters (analogous to what TensorFlow and PyTorch call <em>modules</em>). Second, we chose to use the same axis name <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}layer}}\)</span> for all the layers (with different sizes <span class="math inline">\(n_l\)</span>). So <span class="math inline">\(\text{FullConn}^l\)</span> temporarily computes its output over axis <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}layer}}&#39;\)</span>, then renames it back to <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}layer}}\)</span>.</p>
<p>Then the network can be defined like this: <span class="math display">\[\begin{aligned}
  X^0 &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}layer}}[n_0]} \\
  X^1 &amp;= \text{FullConn}^1(X^0) \\
  X^2 &amp;= \text{FullConn}^2(X^1) \\
  X^3 &amp;= \text{FullConn}^3(X^2).\end{aligned}\]</span></p>
<h3 id="sec:rnn"><span class="header-section-number">3.1.2</span> Recurrent neural networks</h3>
<p>As a second example, let’s define a simple (Elman) RNN. This is similar to the feedforward network, except that the number of timesteps is variable and they all share parameters. <span class="math display">\[\begin{aligned}
x^{t} &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}input}}} &amp; t &amp;= 1, \ldots, n \\
W^{\text{h}} &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}hidden}}\times \ensuremath{\mathsf{\vphantom{fg}hidden}}&#39;} &amp; |\ensuremath{\mathsf{\vphantom{fg}hidden}}| &amp;= |\ensuremath{\mathsf{\vphantom{fg}hidden}}&#39;| \\
W^{\text{i}} &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}input}}\times \ensuremath{\mathsf{\vphantom{fg}hidden}}&#39;} \\
b &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}hidden}}&#39;} \\
h^{0} &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}hidden}}} \\
h^{t} &amp;= \sigma\left( W^{\text{h}} \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}hidden}}}}{\vphantom{fg}\odot}} h^{t-1} + W^{\text{i}} \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}input}}}}{\vphantom{fg}\odot}} x^{t} + b \right)_{\ensuremath{\mathsf{\vphantom{fg}hidden}}&#39;\rightarrow\ensuremath{\mathsf{\vphantom{fg}hidden}}} &amp; t &amp;= 1, \ldots, n\end{aligned}\]</span></p>
<h3 id="sec:attention"><span class="header-section-number">3.1.3</span> Attention</h3>
<p>In the introduction (§<a href="#sec:intro" data-reference-type="ref" data-reference="sec:intro">1</a>), we mentioned some difficulties in interpreting the equation for attention as it’s usually written. In our notation, it looks like this: <span class="math display">\[\begin{aligned}
  \text{Attention} \colon \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}key}}} \times \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}seq}}\times\ensuremath{\mathsf{\vphantom{fg}key}}} \times \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}seq}}\times\ensuremath{\mathsf{\vphantom{fg}val}}} &amp;\rightarrow \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}val}}} \\
  \text{Attention}(Q,K,V) &amp;= \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}seq}}}}{\vphantom{fg}\mathrm{softmax}}} \left( \frac{Q \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}key}}}}{\vphantom{fg}\odot}} K}{\sqrt{|\ensuremath{\mathsf{\vphantom{fg}key}}|}} \right) \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}seq}}}}{\vphantom{fg}\odot}} V.\end{aligned}\]</span></p>
<p>This equation is slightly different from the one in the introduction. The previous definition computed an output sequence over axis <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}seq}}&#39;\)</span>, but this definition computes a single value. If we want a sequence, we can just give <span class="math inline">\(Q\)</span> a <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}seq}}&#39;\)</span> axis (or some other name), and the function will compute an output sequence. Furthermore, if we give <span class="math inline">\(Q\)</span>, <span class="math inline">\(K\)</span>, and <span class="math inline">\(V\)</span> a <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}heads}}\)</span> axis for multiple attention heads, then the function will compute multi-head attention.</p>
<p>Sometimes we need to apply a mask to keep from attending to certain positions. <span class="math display">\[\begin{aligned}
  \text{Attention} \colon \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}key}}} \times \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}seq}}\times\ensuremath{\mathsf{\vphantom{fg}key}}} \times \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}seq}}\times\ensuremath{\mathsf{\vphantom{fg}val}}} \times \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}seq}}} &amp;\rightarrow \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}val}}} \\
\text{Attention}(Q, K, V, M) &amp;= \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}seq}}}}{\vphantom{fg}\mathrm{softmax}}} \left( \frac{Q \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}key}}}}{\vphantom{fg}\odot}} K}{\sqrt{|\ensuremath{\mathsf{\vphantom{fg}key}}|}} + M \right) \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}seq}}}}{\vphantom{fg}\odot}} V.\end{aligned}\]</span></p>
<h3 id="convolution"><span class="header-section-number">3.1.4</span> Convolution</h3>
<p>Convolutions can be easily written by unrolling a tensor and then applying a standard dot product. First, we define the unrolling operation: <span class="math display">\[\begin{aligned}
  \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}seq}}\\ \ensuremath{\mathsf{\vphantom{fg}kernel}}}}{\vphantom{fg}\mathrm{unroll}}} \colon \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}seq}}[n]} &amp;\rightarrow \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}seq}}[n-|\ensuremath{\mathsf{\vphantom{fg}kernel}}|+1], \ensuremath{\mathsf{\vphantom{fg}kernel}}} \\
  \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}seq}}\\ \ensuremath{\mathsf{\vphantom{fg}kernel}}}}{\vphantom{fg}\mathrm{unroll}}} X &amp;= Y,\ \text{where} \\
  Y_{\ensuremath{\mathsf{\vphantom{fg}seq}}(i), \ensuremath{\mathsf{\vphantom{fg}kernel}}(j)} &amp;= X_{\ensuremath{\mathsf{\vphantom{fg}seq}}(i+j - 1)}.\end{aligned}\]</span></p>
<p>Then we can define convolutions as: <span class="math display">\[\begin{aligned}
\text{Conv1d} \colon \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}chans}}\times \ensuremath{\mathsf{\vphantom{fg}seq}}[n]} &amp;\rightarrow \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}seq}}[n&#39;]} \\
\text{Conv1d}(X; W, b) &amp;= W \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}chans}}\\ \ensuremath{\mathsf{\vphantom{fg}kernel}}}}{\vphantom{fg}\odot}} \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}seq}}\\ \ensuremath{\mathsf{\vphantom{fg}kernel}}}}{\vphantom{fg}\mathrm{unroll}}} X + b\end{aligned}\]</span> where <span class="math display">\[\begin{aligned}
W &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}chans}}\times \ensuremath{\mathsf{\vphantom{fg}kernel}}} \\
b &amp;\in \mathbb{R}\\\end{aligned}\]</span> This computes a single output channel, but we can get multiple output channels by giving <span class="math inline">\(W\)</span> and <span class="math inline">\(b\)</span> a <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}chans}}&#39;\)</span> axis (or some other name).</p>
<p>The same unrolling operation can be used to define higher-dimensional convolutions: <span class="math display">\[\begin{aligned}
  \text{Conv2d} \colon \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}chans}}\times \ensuremath{\mathsf{\vphantom{fg}height}}[h] \times \ensuremath{\mathsf{\vphantom{fg}width}}[w]}
  &amp;\rightarrow \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}height}}[h&#39;] \times \ensuremath{\mathsf{\vphantom{fg}width}}[w&#39;]} \\
  \text{Conv2d}(X; W, b) &amp;= W \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}chans}}\\ \ensuremath{\mathsf{\vphantom{fg}kh}}, \ensuremath{\mathsf{\vphantom{fg}kw}}}}{\vphantom{fg}\odot}} \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}height}}\\ \ensuremath{\mathsf{\vphantom{fg}kh}}}}{\vphantom{fg}\mathrm{unroll}}} \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}width}}\\\ensuremath{\mathsf{\vphantom{fg}kw}}}}{\vphantom{fg}\mathrm{unroll}}} X + b\end{aligned}\]</span> where <span class="math display">\[\begin{aligned}
W &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}chans}}\times \ensuremath{\mathsf{\vphantom{fg}kh}}\times \ensuremath{\mathsf{\vphantom{fg}kw}}} \\
b &amp;\in \mathbb{R}.\end{aligned}\]</span></p>
<h3 id="max-pooling"><span class="header-section-number">3.1.5</span> Max pooling</h3>
<p>We first define an operation to reshape an axis: <span class="math display">\[\begin{aligned}
  \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}seq}},\ensuremath{\mathsf{\vphantom{fg}kernel}}}}{\vphantom{fg}\mathrm{pool}}} \colon \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}seq}}[n]} &amp;\rightarrow \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}seq}}[n/|\ensuremath{\mathsf{\vphantom{fg}kernel}}|],\ensuremath{\mathsf{\vphantom{fg}kernel}}} \\
  \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}seq}},\ensuremath{\mathsf{\vphantom{fg}kernel}}}}{\vphantom{fg}\mathrm{pool}}} X &amp;= Y,\ \text{where} \\
  Y_{\ensuremath{\mathsf{\vphantom{fg}seq}}(i), \ensuremath{\mathsf{\vphantom{fg}kernel}}(j)} &amp;= X_{\ensuremath{\mathsf{\vphantom{fg}seq}}((i-1) \cdot |\ensuremath{\mathsf{\vphantom{fg}kernel}}| + j)}.\end{aligned}\]</span></p>
<p>Then we can define: <span class="math display">\[\begin{aligned}
\text{MaxPool1d}_{k} \colon \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}seq}}[n]} &amp;\rightarrow \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}seq}}[n/k]} \\
\text{MaxPool1d}_{k}(X) &amp;= \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}kernel}}}}{\vphantom{fg}\mathrm{max}}} \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}seq}},\ensuremath{\mathsf{\vphantom{fg}kernel}}}}{\vphantom{fg}\mathrm{pool}}} X \\
|\ensuremath{\mathsf{\vphantom{fg}kernel}}| &amp;= k \\
\text{MaxPool2d}_{kh,kw} \colon \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}height}}[h] \times \ensuremath{\mathsf{\vphantom{fg}width}}[w]} &amp;\rightarrow \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}height}}[h/kh] \times \ensuremath{\mathsf{\vphantom{fg}width}}[w/kw]} \\
\text{MaxPool2d}_{kh,kw}(X) &amp;= \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}kh}},\ensuremath{\mathsf{\vphantom{fg}kw}}}}{\vphantom{fg}\mathrm{max}}} \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}height}},\ensuremath{\mathsf{\vphantom{fg}kh}}}}{\vphantom{fg}\mathrm{pool}}} \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}width}},\ensuremath{\mathsf{\vphantom{fg}kw}}}}{\vphantom{fg}\mathrm{pool}}} X \\
|\ensuremath{\mathsf{\vphantom{fg}kh}}| &amp;= kh \\
|\ensuremath{\mathsf{\vphantom{fg}kw}}| &amp;= kw.\end{aligned}\]</span> Other pooling functions could be defined similarly.</p>
<h3 id="normalization-layers"><span class="header-section-number">3.1.6</span> Normalization layers</h3>
<p>Batch, instance, and layer normalization are often informally described using the same equation, but they each correspond to very different functions. They differ both by which axes are <em>standardized</em> as well as their parameters.</p>
<p>We can define a single generic standardization function as: <span class="math display">\[\begin{aligned}
  \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}}{\vphantom{fg}\mathrm{standardize}}} \colon \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}ax}}} &amp;\rightarrow \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}ax}}} \\
  \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}}{\vphantom{fg}\mathrm{standardize}}}(X) &amp;= \frac{X - \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}}{\vphantom{fg}\mathrm{mean}}}(X)}{\sqrt{\mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}}{\vphantom{fg}\mathrm{var}}}(X) + \epsilon}}\end{aligned}\]</span> where <span class="math inline">\(\epsilon &gt; 0\)</span> is a small constant for numerical stability.</p>
<p>Then, we can define the three kinds of normalization layers, all with type <span class="math inline">\(\mathbb{R}^{{\ensuremath{\mathsf{\vphantom{fg}batch}}\times \ensuremath{\mathsf{\vphantom{fg}chans}}\times \ensuremath{\mathsf{\vphantom{fg}layer}}}} \rightarrow \mathbb{R}^{{\ensuremath{\mathsf{\vphantom{fg}batch}}\times \ensuremath{\mathsf{\vphantom{fg}chans}}\times \ensuremath{\mathsf{\vphantom{fg}layer}}}}\)</span>: <span class="math display">\[\begin{aligned}
\text{BatchNorm}(X; \gamma, \beta) &amp;= \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}batch}},\ensuremath{\mathsf{\vphantom{fg}layer}}}}{\vphantom{fg}\mathrm{standardize}}}(X) \mathbin{\underset{\substack{}}{\vphantom{fg}\odot}} \gamma + \beta &amp; \gamma, \beta &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}chans}}} \\
\text{InstanceNorm}(X; \gamma, \beta) &amp;= \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}layer}}}}{\vphantom{fg}\mathrm{standardize}}}(X) \mathbin{\underset{\substack{}}{\vphantom{fg}\odot}} \gamma + \beta &amp; \gamma, \beta &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}chans}}} \\
\text{LayerNorm}(X; \gamma, \beta) &amp;= \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}layer}},\ensuremath{\mathsf{\vphantom{fg}chans}}}}{\vphantom{fg}\mathrm{standardize}}}(X) \mathbin{\underset{\substack{}}{\vphantom{fg}\odot}} \gamma + \beta &amp; \gamma, \beta &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}chans}},\ensuremath{\mathsf{\vphantom{fg}layer}}}\end{aligned}\]</span></p>
<p>Note that, while superficially similar, these functions differ in their standardized axes and their parameter shape.</p>
<p>Other deep learning methods have been proposed which consider different shapes of standardization. For instance, group norm is a popular extension that first pools channels into <span class="math inline">\(k\)</span>-size groups before standardizing.</p>
<p><span class="math display">\[\begin{aligned}
\text{GroupNorm}_k(X; \gamma, \beta) &amp;= \left[ \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}kernel}},\ensuremath{\mathsf{\vphantom{fg}layer}}}}{\vphantom{fg}\mathrm{standardize}}} \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}chans}}, \ensuremath{\mathsf{\vphantom{fg}kernel}}}}{\vphantom{fg}\mathrm{pool}}} X \right]_{(\ensuremath{\mathsf{\vphantom{fg}chans}},\ensuremath{\mathsf{\vphantom{fg}kernel}})\rightarrow \ensuremath{\mathsf{\vphantom{fg}chans}}} \mathbin{\underset{\substack{}}{\vphantom{fg}\odot}} \gamma + \beta \\ \end{aligned}\]</span> where <span class="math display">\[\begin{aligned}
|\ensuremath{\mathsf{\vphantom{fg}kernel}}| &amp;= k\\
\gamma, \beta &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}chans}}}.\end{aligned}\]</span></p>
<h2 id="sec:transformer"><span class="header-section-number">3.2</span> Transformer</h2>
<p>We define a Transformer used autoregressively as a language model. The input is a sequence of one-hot vectors, from which we compute word embeddings and positional encodings: <span class="math display">\[\begin{aligned}
  I &amp;\in \{0, 1\}^{\ensuremath{\mathsf{\vphantom{fg}seq}}\times \ensuremath{\mathsf{\vphantom{fg}vocab}}} &amp; \sum\limits_{\substack{\ensuremath{\mathsf{\vphantom{fg}vocab}}}} I &amp;= 1 \\
  W &amp;= (E \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}vocab}}}}{\vphantom{fg}\odot}} I)\sqrt{|\ensuremath{\mathsf{\vphantom{fg}layer}}|} &amp; E &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}vocab}}\times \ensuremath{\mathsf{\vphantom{fg}layer}}} \\
  P &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}seq}}\times \ensuremath{\mathsf{\vphantom{fg}layer}}} \\
  P_{\ensuremath{\mathsf{\vphantom{fg}seq}}(p), \ensuremath{\mathsf{\vphantom{fg}layer}}(i)} &amp;= \begin{cases}
    \sin((p-1) / 10000^{(i-1) / |\ensuremath{\mathsf{\vphantom{fg}layer}}|}) &amp; \text{$i$ odd} \\ 
    \cos((p-1) / 10000^{(i-2) / |\ensuremath{\mathsf{\vphantom{fg}layer}}|}) &amp; \text{$i$ even.}
  \end{cases}\end{aligned}\]</span></p>
<p>Then we use <span class="math inline">\(L\)</span> layers of self-attention and feed-forward neural networks: <span class="math display">\[\begin{aligned}
X^0 &amp;= W+P \\
T^1 &amp;= \text{LayerNorm}^1(\text{SelfAtt}^1(X^0)) + X^0\\
X^1 &amp;= \text{LayerNorm}^{1&#39;}(\text{FFN}^1(T^1)) + T^1\\
&amp;\vdotswithin{=} \\
T^{L} &amp;= \text{LayerNorm}^L(\text{SelfAtt}^L(X^{L-1})) + X^{L-1}\\
X^{L} &amp;= \text{LayerNorm}^{L&#39;}(\text{FFN}^L(T^L)) + T^L\\
O &amp;= \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}vocab}}}}{\vphantom{fg}\mathrm{softmax}}}(E \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}layer}}}}{\vphantom{fg}\odot}} X^L)\end{aligned}\]</span> where <span class="math inline">\(\text{LayerNorm}\)</span>, <span class="math inline">\(\text{SelfAtt}\)</span> and <span class="math inline">\(\text{FFN}\)</span> are defined below.</p>
<p>Layer normalization (<span class="math inline">\(l = 1, 1&#39;, \ldots, L, L&#39;\)</span>): <span class="math display">\[\begin{aligned}
  \text{LayerNorm}^l \colon \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}layer}}} &amp;\rightarrow \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}layer}}} \\
  \text{LayerNorm}^l(X) &amp;= \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}layer}}}}{\vphantom{fg}\mathrm{XNorm}}}(X; \beta^l, \gamma^l).\end{aligned}\]</span></p>
<p>We defined attention in §<a href="#sec:attention" data-reference-type="ref" data-reference="sec:attention">3.1.3</a>; the Transformer uses multi-head self-attention, in which queries, keys, and values are all computed from the same sequence. <span class="math display">\[\begin{aligned}
  \text{SelfAtt}^l \colon \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}seq}}\times \ensuremath{\mathsf{\vphantom{fg}layer}}} &amp;\rightarrow \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}seq}}\times \ensuremath{\mathsf{\vphantom{fg}layer}}} \\
  \text{SelfAtt}^l(X) &amp;= Y\end{aligned}\]</span> where <span class="math display">\[\begin{aligned}
  |\ensuremath{\mathsf{\vphantom{fg}seq}}| &amp;= |\ensuremath{\mathsf{\vphantom{fg}seq}}&#39;| \\
  |\ensuremath{\mathsf{\vphantom{fg}key}}| = |\ensuremath{\mathsf{\vphantom{fg}val}}| &amp;= |\ensuremath{\mathsf{\vphantom{fg}layer}}|/|\ensuremath{\mathsf{\vphantom{fg}heads}}| \\
  Q &amp;= W^{l,Q} \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}layer}}}}{\vphantom{fg}\odot}} X_{\ensuremath{\mathsf{\vphantom{fg}seq}}\rightarrow\ensuremath{\mathsf{\vphantom{fg}seq}}&#39;} &amp; W^{l,Q} &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}heads}}\times \ensuremath{\mathsf{\vphantom{fg}layer}}\times \ensuremath{\mathsf{\vphantom{fg}key}}} \\
  K &amp;= W^{l,K} \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}layer}}}}{\vphantom{fg}\odot}} X &amp; W^{l,K} &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}heads}}\times \ensuremath{\mathsf{\vphantom{fg}layer}}\times \ensuremath{\mathsf{\vphantom{fg}key}}} \\
  V &amp;= W^{l,V} \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}layer}}}}{\vphantom{fg}\odot}} X &amp; W^{l,V} &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}heads}}\times \ensuremath{\mathsf{\vphantom{fg}layer}}\times \ensuremath{\mathsf{\vphantom{fg}val}}} \\
  M &amp; \in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}seq}}\times \ensuremath{\mathsf{\vphantom{fg}seq}}&#39;} \\
  M_{\ensuremath{\mathsf{\vphantom{fg}seq}}(i), \ensuremath{\mathsf{\vphantom{fg}seq}}&#39;(j)} &amp;= \begin{cases}
    0 &amp; i \leq j\\
    -\infty &amp; \text{otherwise}
  \end{cases} \\
  Y &amp;= W^{l,O} \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}heads}}\\ \ensuremath{\mathsf{\vphantom{fg}val}}}}{\vphantom{fg}\odot}} \text{Attention}(Q, K, V, M)_{\ensuremath{\mathsf{\vphantom{fg}seq}}&#39;\rightarrow\ensuremath{\mathsf{\vphantom{fg}seq}}} &amp; W^{l,O} &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}heads}}\times \ensuremath{\mathsf{\vphantom{fg}val}}\times \ensuremath{\mathsf{\vphantom{fg}layer}}}\end{aligned}\]</span></p>
<p>Feedforward neural networks: <span class="math display">\[\begin{aligned}
  \text{FFN}^l \colon \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}layer}}} &amp;\rightarrow \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}layer}}} \\
  \text{FFN}^l(X) &amp;= X^2\end{aligned}\]</span> where <span class="math display">\[\begin{aligned}
  X^1 &amp;= \text{relu}(W^{l,1} \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}layer}}}}{\vphantom{fg}\odot}} X + b^{l,1}) &amp; W^{l,1} &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}hidden}}\times \ensuremath{\mathsf{\vphantom{fg}layer}}} &amp; b^{l,1} &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}hidden}}} \\
  X^2 &amp;= \text{relu}(W^{l,2} \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}hidden}}}}{\vphantom{fg}\odot}} X^1 + b^{l,2}) &amp; W^{l,2} &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}layer}}\times \ensuremath{\mathsf{\vphantom{fg}hidden}}} &amp; b^{l,2} &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}hidden}}}.\end{aligned}\]</span></p>
<h2 id="lenet"><span class="header-section-number">3.3</span> LeNet</h2>
<p><span class="math display">\[\begin{aligned}
X^0 &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}batch}}\times \ensuremath{\mathsf{\vphantom{fg}chans}}[c_0] \times \ensuremath{\mathsf{\vphantom{fg}height}}\times \ensuremath{\mathsf{\vphantom{fg}width}}} \\
T^1 &amp;= \text{relu}(\text{Conv}^1(X^0)) \\
X^1 &amp;= \text{MaxPool}^1(T^1) \\
T^2 &amp;= \text{relu}(\text{Conv}^2(X^1)) \\
X^2 &amp;= \text{MaxPool}^2(T^2)_{(\ensuremath{\mathsf{\vphantom{fg}height}},\ensuremath{\mathsf{\vphantom{fg}width}},\ensuremath{\mathsf{\vphantom{fg}chans}})\rightarrow\ensuremath{\mathsf{\vphantom{fg}layer}}} \\
X^3 &amp;= \text{relu}(W^3 \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}layer}}}}{\vphantom{fg}\odot}} X^2 + b^3) &amp; W^3 &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}hidden}}\times \ensuremath{\mathsf{\vphantom{fg}layer}}} &amp; b^3 &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}hidden}}} \\
O &amp;= \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}classes}}}}{\vphantom{fg}\mathrm{softmax}}} (W^4 \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}hidden}}}}{\vphantom{fg}\odot}} X^3 + b^4) &amp; W^4 &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}classes}}\times \ensuremath{\mathsf{\vphantom{fg}hidden}}} &amp; b^4 &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}classes}}}\end{aligned}\]</span> As an alternative to the flattening operation in the equation for <span class="math inline">\(X^2\)</span>, we could have written <span class="math display">\[\begin{aligned}
X^2 &amp;= \text{MaxPool}^2(T^2) \\
X^3 &amp;= \text{relu}(W^3 \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}height}}\\ \ensuremath{\mathsf{\vphantom{fg}width}}\\ \ensuremath{\mathsf{\vphantom{fg}chans}}}}{\vphantom{fg}\odot}} X^2 + b^3) &amp; W^3 &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}hidden}}\times \ensuremath{\mathsf{\vphantom{fg}height}}\times \ensuremath{\mathsf{\vphantom{fg}width}}\times \ensuremath{\mathsf{\vphantom{fg}chans}}}.\end{aligned}\]</span></p>
<p>The convolution and pooling operations are defined as follows: <span class="math display">\[\begin{aligned}
\text{Conv}^l(X) &amp;= \text{Conv2d}(X; W^l, b^l)_{\ensuremath{\mathsf{\vphantom{fg}chans}}&#39;\rightarrow\ensuremath{\mathsf{\vphantom{fg}chans}}}\end{aligned}\]</span> where <span class="math display">\[\begin{aligned}
W^l &amp; \in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}chans}}&#39;[c_l] \times \ensuremath{\mathsf{\vphantom{fg}chans}}[c_{l-1}] \times \ensuremath{\mathsf{\vphantom{fg}kh}}[kh_l] \times \ensuremath{\mathsf{\vphantom{fg}kw}}[kw_l]} \\
b^l &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}chans}}&#39;[c_l]}\end{aligned}\]</span> and <span class="math display">\[\begin{aligned}
\text{MaxPool}^l(X) &amp;= \text{MaxPool2d}_{ph^l,ph^l}(X).\end{aligned}\]</span></p>
<h2 id="other-examples"><span class="header-section-number">3.4</span> Other examples</h2>
<h3 id="discrete-random-variables"><span class="header-section-number">3.4.1</span> Discrete random variables</h3>
<p>Named axes are very helpful for working with discrete random variables, because each random variable can be represented by an axis with the same name. For instance, if <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}A}}\)</span> and <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}B}}\)</span> are random variables, we can treat <span class="math inline">\(p(\ensuremath{\mathsf{\vphantom{fg}B}} \mid \ensuremath{\mathsf{\vphantom{fg}A}})\)</span> and <span class="math inline">\(p(\ensuremath{\mathsf{\vphantom{fg}A}})\)</span> as tensors: <span class="math display">\[\begin{aligned}
p(\ensuremath{\mathsf{\vphantom{fg}B}} \mid \ensuremath{\mathsf{\vphantom{fg}A}}) &amp;\in [0, 1]^{\ensuremath{\mathsf{\vphantom{fg}A}} \times \ensuremath{\mathsf{\vphantom{fg}B}}} &amp; \sum\limits_{\substack{\ensuremath{\mathsf{\vphantom{fg}B}}}} p(\ensuremath{\mathsf{\vphantom{fg}B}} \mid \ensuremath{\mathsf{\vphantom{fg}A}}) &amp;= 1 \\
p(\ensuremath{\mathsf{\vphantom{fg}A}}) &amp;\in [0, 1]^{\ensuremath{\mathsf{\vphantom{fg}A}}} &amp; \sum\limits_{\substack{\ensuremath{\mathsf{\vphantom{fg}A}}}} p(\ensuremath{\mathsf{\vphantom{fg}A}}) &amp;= 1\end{aligned}\]</span> Then many common operations on probability distributions can be expressed in terms of tensor operations: <span class="math display">\[\begin{aligned}
p(\ensuremath{\mathsf{\vphantom{fg}A}}, \ensuremath{\mathsf{\vphantom{fg}B}}) &amp;= p(\ensuremath{\mathsf{\vphantom{fg}B}} \mid \ensuremath{\mathsf{\vphantom{fg}A}}) \odot p(\ensuremath{\mathsf{\vphantom{fg}A}}) &amp;&amp; \text{chain rule}\\
p(\ensuremath{\mathsf{\vphantom{fg}B}}) &amp;= \sum\limits_{\substack{\ensuremath{\mathsf{\vphantom{fg}A}}}} p(\ensuremath{\mathsf{\vphantom{fg}A}}, \ensuremath{\mathsf{\vphantom{fg}B}}) = p(\ensuremath{\mathsf{\vphantom{fg}B}} \mid \ensuremath{\mathsf{\vphantom{fg}A}}) \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}A}}}}{\vphantom{fg}\odot}} p(\ensuremath{\mathsf{\vphantom{fg}A}}) &amp;&amp; \text{marginalization} \\
p(\ensuremath{\mathsf{\vphantom{fg}A}} \mid \ensuremath{\mathsf{\vphantom{fg}B}}) &amp;= \frac{p(\ensuremath{\mathsf{\vphantom{fg}A}}, \ensuremath{\mathsf{\vphantom{fg}B}})}{p(\ensuremath{\mathsf{\vphantom{fg}B}})} = \frac{p(\ensuremath{\mathsf{\vphantom{fg}B}} \mid \ensuremath{\mathsf{\vphantom{fg}A}}) \odot p(\ensuremath{\mathsf{\vphantom{fg}A}})}{p(\ensuremath{\mathsf{\vphantom{fg}B}} \mid \ensuremath{\mathsf{\vphantom{fg}A}}) \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}A}}}}{\vphantom{fg}\odot}} p(\ensuremath{\mathsf{\vphantom{fg}A}})}. &amp;&amp; \text{Bayes&#39; rule}\end{aligned}\]</span></p>
<h3 id="advanced-indexing"><span class="header-section-number">3.4.2</span> Advanced indexing</h3>
<p>Contributors: Tongfei Chen and Chu-Cheng Lin</p>
<p>NumPy and its derivatives provide various ways to recombine elements of a tensor to form a new tensor: integer array indexing, and functions like <code>take</code>, <code>index_select</code>, <code>gather</code>, and <code>batch_gather</code>. Using named tensors, we can write nearly all of these operations with a single function: <span class="math display">\[\begin{aligned}
  \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}}{\vphantom{fg}\mathrm{index}}} \colon \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}ax}}[n]} \times [n] &amp;\rightarrow \mathbb{R}\\
  \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}}{\vphantom{fg}\mathrm{index}}}(A, i) &amp;= A_{\ensuremath{\mathsf{\vphantom{fg}ax}}(i)}.\end{aligned}\]</span> Suppose we have <span class="math display">\[\begin{aligned}
  E &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}vocab}}[n] \times \ensuremath{\mathsf{\vphantom{fg}emb}}} \\
  i &amp;\in [n] \\
  I &amp;\in [n]^{\ensuremath{\mathsf{\vphantom{fg}seq}}} \\
  P &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}seq}}\times \ensuremath{\mathsf{\vphantom{fg}vocab}}[n]}\end{aligned}\]</span> Tensor <span class="math inline">\(E\)</span> contains word embeddings for all the words in the vocabulary. Integer <span class="math inline">\(i\)</span> is the numeric identifier of a word, while tensor <span class="math inline">\(I\)</span> is a sequence of words. Tensor <span class="math inline">\(P\)</span> contains a sequence of probability distributions over the vocabulary. Then:</p>
<ul>
<li><p>The expression <span class="math inline">\(\mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}vocab}}}}{\vphantom{fg}\mathrm{index}}}(E,i)\)</span> broadcasts <span class="math inline">\(E\)</span>’s <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}emb}}\)</span> axis, giving the word embedding of word <span class="math inline">\(i\)</span>. This is the same as partial indexing (<span class="math inline">\(E_{\ensuremath{\mathsf{\vphantom{fg}vocab}}(i)}\)</span>).</p></li>
<li><p>The expression <span class="math inline">\(\mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}vocab}}}}{\vphantom{fg}\mathrm{index}}}(E,I)\)</span> also broadcasts <span class="math inline">\(I\)</span>’s <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}seq}}\)</span> axis, giving a sequence of word embeddings. This is the same as integer array indexing.</p></li>
<li><p>The expression <span class="math inline">\(\mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}vocab}}}}{\vphantom{fg}\mathrm{index}}}(P,I)\)</span> aligns <span class="math inline">\(P\)</span>’s and <span class="math inline">\(I\)</span>’s <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}seq}}\)</span> axes, giving a sequence of probabilities. This is the same as <code>gather</code>.</p></li>
</ul>
<p>In NumPy, indexing using two or more integer arrays requires a special definition with some surprising special cases. With named tensors, we simply apply the indexing function twice. For example, if we (for some reason) wanted to get probabilities of words at a subset of positions: <span class="math display">\[\begin{aligned}
  |\ensuremath{\mathsf{\vphantom{fg}seq}}| &amp;= m \\
  I_1 &amp;= [m]^\ensuremath{\mathsf{\vphantom{fg}subseq}}\\
  I_2 &amp;= [n]^\ensuremath{\mathsf{\vphantom{fg}subseq}}\\
  S &amp;= \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}vocab}}}}{\vphantom{fg}\mathrm{index}}}(\mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}seq}}}}{\vphantom{fg}\mathrm{index}}}(P, I_1), I_2) \in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}subseq}}} \\
  S_{\ensuremath{\mathsf{\vphantom{fg}subseq}}(i)} &amp;= P_{\ensuremath{\mathsf{\vphantom{fg}seq}}(I_{\ensuremath{\mathsf{\vphantom{fg}subseq}}(i)}), \ensuremath{\mathsf{\vphantom{fg}vocab}}(I_{\ensuremath{\mathsf{\vphantom{fg}subseq}}(i)})}.\end{aligned}\]</span></p>
<h3 id="continuous-bag-of-words"><span class="header-section-number">3.4.3</span> Continuous bag of words</h3>
<p>A continuous bag-of-words model classifies by summing up the embeddings of a sequence of words <span class="math inline">\(X\)</span> (as one-hot vectors) and projecting them to the space of classes.</p>
<p><span class="math display">\[\begin{aligned}
\text{CBOW} \colon \{0, 1\}^{\ensuremath{\mathsf{\vphantom{fg}seq}}\times \ensuremath{\mathsf{\vphantom{fg}vocab}}} &amp;\rightarrow \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}classes}}} \\
\text{CBOW}(X; E, W) &amp;= \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}classes}}}}{\vphantom{fg}\mathrm{softmax}}} \left(\sum\limits_{\substack{\ensuremath{\mathsf{\vphantom{fg}seq}}}} W \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}emb}}}}{\vphantom{fg}\odot}} E \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}vocab}}}}{\vphantom{fg}\odot}} X\right)\end{aligned}\]</span> where <span class="math display">\[\begin{aligned}
\sum\limits_{\substack{\ensuremath{\mathsf{\vphantom{fg}vocab}}}} X_{\ensuremath{\mathsf{\vphantom{fg}seq}}(i)} &amp;= 1 &amp; i &amp;= 1, \ldots, |\ensuremath{\mathsf{\vphantom{fg}seq}}| \\
E &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}vocab}}\times \ensuremath{\mathsf{\vphantom{fg}emb}}} \\
W &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}classes}}\times \ensuremath{\mathsf{\vphantom{fg}emb}}}.\end{aligned}\]</span></p>
<h3 id="sudoku-ilp"><span class="header-section-number">3.4.4</span> Sudoku ILP</h3>
<p>Sudoku puzzles can be represented as binary tiled tensors. Given a grid we can check that it is valid by converting it to a grid of grids. Constraints then ensure that there is one digit per row, per column and per sub-box.</p>
<p><span class="math display">\[\begin{aligned}
\text{check} \colon \{0, 1\}^{\ensuremath{\mathsf{\vphantom{fg}height}}[9] \times \ensuremath{\mathsf{\vphantom{fg}width}}[9] \times \ensuremath{\mathsf{\vphantom{fg}assign}}[9]} &amp;\rightarrow \{0, 1\} \\
\text{check}(X) &amp;=
\mathbb{I}\left[\begin{aligned}
\sum\limits_{\substack{\ensuremath{\mathsf{\vphantom{fg}assign}}}} X = 1 &amp;\land \sum\limits_{\substack{\ensuremath{\mathsf{\vphantom{fg}height}}\\ \ensuremath{\mathsf{\vphantom{fg}width}}}} Y = 1 \land {} \\
\sum\limits_{\substack{\ensuremath{\mathsf{\vphantom{fg}height}}}} X = 1 &amp;\land \sum\limits_{\substack{\ensuremath{\mathsf{\vphantom{fg}width}}}} X = 1
\end{aligned}\right]\end{aligned}\]</span> where <span class="math display">\[\begin{aligned}
Y &amp;\in \{0, 1\}^{\ensuremath{\mathsf{\vphantom{fg}height}}&#39;[3] \times \ensuremath{\mathsf{\vphantom{fg}width}}&#39;[3] \times \ensuremath{\mathsf{\vphantom{fg}height}}[3] \times \ensuremath{\mathsf{\vphantom{fg}width}}[3] \times \ensuremath{\mathsf{\vphantom{fg}assign}}[9]}  \\
Y_{\ensuremath{\mathsf{\vphantom{fg}height}}&#39;(h&#39;), \ensuremath{\mathsf{\vphantom{fg}height}}(h), \ensuremath{\mathsf{\vphantom{fg}width}}&#39;(w&#39;), \ensuremath{\mathsf{\vphantom{fg}width}}(w)} &amp;= X_{\ensuremath{\mathsf{\vphantom{fg}height}}(3h&#39; + h-1), \ensuremath{\mathsf{\vphantom{fg}width}}(3 w&#39; + w-1)}.\end{aligned}\]</span></p>
<h3 id="k-means-clustering"><span class="header-section-number">3.4.5</span> <span class="math inline">\(K\)</span>-means clustering</h3>
<p>The following equations define one step of <span class="math inline">\(k\)</span>-means clustering. Given a set of points <span class="math inline">\(X\)</span> and an initial set of cluster centers <span class="math inline">\(C\)</span>, <span class="math display">\[\begin{aligned}
  X &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}batch}}\times \ensuremath{\mathsf{\vphantom{fg}d}}} \\
C &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}clusters}}\times \ensuremath{\mathsf{\vphantom{fg}d}}}\end{aligned}\]</span> we repeat the following update: Compute cluster assignments <span class="math display">\[\begin{aligned}
Q &amp;= \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}clusters}}}}{\vphantom{fg}\mathrm{argmin}}} \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}d}}}}{\vphantom{fg}\mathrm{norm}}}(C-X)\end{aligned}\]</span> then recompute the cluster centers: <span class="math display">\[C \leftarrow \sum\limits_{\substack{\ensuremath{\mathsf{\vphantom{fg}batch}}}} \frac{Q \odot X}{Q}.\]</span></p>
<h3 id="beam-search"><span class="header-section-number">3.4.6</span> Beam search</h3>
<p>Beam search is a commonly used approach for approximate discrete search. Here <span class="math inline">\(H\)</span> is the score of each element in the beam, <span class="math inline">\(S\)</span> is the state of each element in the beam, and <span class="math inline">\(f\)</span> is an update function that returns the score of each state transition. <span class="math display">\[\begin{aligned}
H &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}beam}}} \\
S &amp;\in \{0, 1\}^{\ensuremath{\mathsf{\vphantom{fg}beam}}\times \ensuremath{\mathsf{\vphantom{fg}state}}} &amp; \sum\limits_{\substack{\ensuremath{\mathsf{\vphantom{fg}state}}}} S &amp;= 1 \\
f &amp;\colon \{0, 1\}^{\ensuremath{\mathsf{\vphantom{fg}state}}} \rightarrow \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}state}}} \\\end{aligned}\]</span> Then we repeat the following update: <span class="math display">\[\begin{aligned}
H&#39; &amp;= \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}beam}}}}{\vphantom{fg}\mathrm{max}}} (H \odot f(S)) \\
H &amp;\leftarrow \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}state}},\ensuremath{\mathsf{\vphantom{fg}beam}}}}{\vphantom{fg}\mathrm{maxk}}} H&#39; \\
S &amp;\leftarrow \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}state}},\ensuremath{\mathsf{\vphantom{fg}beam}}}}{\vphantom{fg}\mathrm{argmaxk}}} H&#39;\end{aligned}\]</span> where <span class="math display">\[\begin{aligned}
\mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}},\ensuremath{\mathsf{\vphantom{fg}k}}}}{\vphantom{fg}\mathrm{maxk}}} \colon \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}ax}}} &amp;\rightarrow \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}k}}} \\
\mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}},\ensuremath{\mathsf{\vphantom{fg}k}}}}{\vphantom{fg}\mathrm{argmaxk}}} \colon \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}ax}}} &amp;\rightarrow \{0,1\}^{\ensuremath{\mathsf{\vphantom{fg}ax}},\ensuremath{\mathsf{\vphantom{fg}k}}}\end{aligned}\]</span> are defined such that <span class="math inline">\([\mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}},\ensuremath{\mathsf{\vphantom{fg}k}}}}{\vphantom{fg}\mathrm{maxk}}} A]_{\ensuremath{\mathsf{\vphantom{fg}k}}(i)}\)</span> is the <span class="math inline">\(i\)</span>-th largest value along axis <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}ax}}\)</span> and <span class="math inline">\(A \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}}{\vphantom{fg}\odot}} (\mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}},\ensuremath{\mathsf{\vphantom{fg}k}}}}{\vphantom{fg}\mathrm{argmaxk}}}{A}) = \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}},\ensuremath{\mathsf{\vphantom{fg}k}}}}{\vphantom{fg}\mathrm{max}}} A\)</span>.</p>
<p>We can add a <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}batch}}\)</span> axis to <span class="math inline">\(H\)</span> and <span class="math inline">\(S\)</span> and the above equations will work unchanged.</p>
<h3 id="multivariate-normal-distribution"><span class="header-section-number">3.4.7</span> Multivariate normal distribution</h3>
<p>To define a multivariate normal distribution, we need some matrix operations. These have two axis names written under them, for rows and columns, respectively. Determinant and inverse have the following signatures: <span class="math display">\[\begin{aligned}
\mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}_1,\ensuremath{\mathsf{\vphantom{fg}ax}}_2}}{\vphantom{fg}\mathrm{det}}} \colon F^{\ensuremath{\mathsf{\vphantom{fg}ax}}_1[n] \times \ensuremath{\mathsf{\vphantom{fg}ax}}_2[n]} &amp;\rightarrow F \\
\mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}_1,\ensuremath{\mathsf{\vphantom{fg}ax}}_2}}{\vphantom{fg}\mathrm{inv}}} \colon F^{\ensuremath{\mathsf{\vphantom{fg}ax}}_1[n] \times \ensuremath{\mathsf{\vphantom{fg}ax}}_2[n]} &amp;\rightarrow F^{\ensuremath{\mathsf{\vphantom{fg}ax}}_1[n] \times \ensuremath{\mathsf{\vphantom{fg}ax}}_2[n]}.\end{aligned}\]</span> (We write <span class="math inline">\(\text{inv}\)</span> instead of <span class="math inline">\(\cdot^{-1}\)</span> because there’s no way to write axis names under the latter.)</p>
<p>In our notation, the application of a bilinear form is more verbose than the standard notation (<span class="math inline">\((X-\mu)^\top \Sigma^{-1} (X-\mu)\)</span>), but also makes it look more like a function of two arguments (and would generalize to three or more arguments).</p>
<p><span class="math display">\[\begin{aligned}
\mathcal{N} \colon \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}d}}} &amp;\rightarrow \mathbb{R}\\
\mathcal{N}(X; \mu, \Sigma) &amp;= \frac{\exp\left(-\frac{1}{2} \left(\mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}d}}_1, \ensuremath{\mathsf{\vphantom{fg}d}}_2}}{\vphantom{fg}\mathrm{inv}}} \Sigma\right) \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}d}}_1,\ensuremath{\mathsf{\vphantom{fg}d}}_2}}{\vphantom{fg}\odot}} \left([X - \mu]_{\ensuremath{\mathsf{\vphantom{fg}d}}\rightarrow\ensuremath{\mathsf{\vphantom{fg}d}}_1} \odot [X - \mu]_{\ensuremath{\mathsf{\vphantom{fg}d}}\rightarrow\ensuremath{\mathsf{\vphantom{fg}d}}_2} \right) \right)}{\sqrt{(2 \pi)^{|\ensuremath{\mathsf{\vphantom{fg}d}}|} \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}d}}_1, \ensuremath{\mathsf{\vphantom{fg}d}}_2}}{\vphantom{fg}\mathrm{det}}} \Sigma}}\end{aligned}\]</span> where <span class="math display">\[\begin{aligned}
|\ensuremath{\mathsf{\vphantom{fg}d}}| &amp;= |\ensuremath{\mathsf{\vphantom{fg}d}}_1| = |\ensuremath{\mathsf{\vphantom{fg}d}}_2| \\
\mu &amp;\in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}d}}} \\
\Sigma &amp; \in \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}d}}_1 \times \ensuremath{\mathsf{\vphantom{fg}d}}_2}.\end{aligned}\]</span></p>
<h1 id="latex-macros"><span class="header-section-number">4</span> LaTeX Macros</h1>
<p>Many of the LaTeX macros used in this document are available in the style file <a href="https://namedtensor.github.io/namedtensor.sty">https://namedtensor.github.io/namedtensor.sty</a>. To use it, put</p>
<blockquote>
<pre><code>\usepackage{namedtensor}</code></pre>
</blockquote>
<p>in the preamble of your LaTeX source file (after <code>\documentclass{article}</code> but before <code>\begin{document}</code>).</p>
<p>We write axis names in sans-serif font. To make this easier, <code>\ndef{\ax}{ax}</code> defines a macro <code>\ax</code> that looks like this: <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}ax}}\)</span>.</p>
<ul>
<li><p>Binary operators</p>
<ul>
<li><p>Use <code>A \ndot{\ax} B</code> for contraction: <span class="math inline">\(A \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}}{\vphantom{fg}\odot}} B\)</span>. You can use <code>\\</code> to stack up several names.</p></li>
<li><p>In general, you can use <code>\nbin</code> to make a new binary operator with a name under it: <code>A \nbin{\ax}{\star} B</code> gives you <span class="math inline">\(A \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}}{\vphantom{fg}\star}} B\)</span>.</p></li>
</ul></li>
<li><p>Functions</p>
<ul>
<li><p>Use <code>\nsum{\ax} A</code> for summation: <span class="math inline">\(\sum\limits_{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}} A\)</span>.</p></li>
<li><p>In general, you can use <code>\nfun</code> to make a function with a name under it: <code>\nfun{\ax}{qux} A</code> gives you <span class="math inline">\(\mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}}{\vphantom{fg}\mathrm{qux}}} A\)</span>.</p></li>
</ul></li>
</ul>
<h1 id="sec:definitions"><span class="header-section-number">5</span> Formal Definitions</h1>
<h2 id="records-and-shapes"><span class="header-section-number">5.1</span> Records and shapes</h2>
<p>A <em>named index</em> is a pair, written <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}ax}}(i)\)</span>, where <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}ax}}\)</span> is a <em>name</em> and <span class="math inline">\(i\)</span> is usually a natural number. We write both names and variables ranging over names using sans-serif font.</p>
<p>A <em>record</em> is a set of named indices <span class="math inline">\(\{\ensuremath{\mathsf{\vphantom{fg}ax}}_1(i_1), \ldots, \ensuremath{\mathsf{\vphantom{fg}ax}}_r(i_r)\}\)</span>, where <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}ax}}_1, \ldots \ensuremath{\mathsf{\vphantom{fg}ax}}_r\)</span> are pairwise distinct names.</p>
<p>An <em>axis</em> is a pair, written <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}ax}}[I]\)</span>, where <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}ax}}\)</span> is a name and <span class="math inline">\(I\)</span> is a set of <em>indices</em>. We deal with axes of the form <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}ax}}[[n]]\)</span> (that is, <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}ax}}[\{1, \ldots, n\}]\)</span>) so frequently that we abbreviate this as <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}ax}}[n]\)</span>.</p>
<p>In many contexts, there is only one axis with name <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}ax}}\)</span>, and so we refer to the axis simply as <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}ax}}\)</span>. The context always makes it clear whether <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}ax}}\)</span> is a name or an axis. If <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}ax}}\)</span> is an axis, we write <span class="math inline">\(\ind(\ensuremath{\mathsf{\vphantom{fg}ax}})\)</span> for its index set, and we write <span class="math inline">\(|\ensuremath{\mathsf{\vphantom{fg}ax}}|\)</span> as shorthand for <span class="math inline">\(|\ind(\ensuremath{\mathsf{\vphantom{fg}ax}})|\)</span>.</p>
<p>A <em>shape</em> is a set of axes, written <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}ax}}_1[I_1] \times \cdots \times \ensuremath{\mathsf{\vphantom{fg}ax}}_r[I_r]\)</span>, where <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}ax}}_1, \ldots \ensuremath{\mathsf{\vphantom{fg}ax}}_r\)</span> are pairwise distinct names. We write <span class="math inline">\(\emptyset\)</span> for the empty shape. A shape defines a set of records: <span class="math display">\[\rec (\ensuremath{\mathsf{\vphantom{fg}ax}}_1[I_1] \times \cdots \times \ensuremath{\mathsf{\vphantom{fg}ax}}_r[I_r]) = \left\{\{\ensuremath{\mathsf{\vphantom{fg}ax}}_1(i_1), \ldots, \ensuremath{\mathsf{\vphantom{fg}ax}}_r(i_r)\} \mid i_1 \in I_1, \ldots, i_r \in I_r\right\}.\]</span></p>
<p>We say two shapes <span class="math inline">\(\mathcal{S}\)</span> and <span class="math inline">\(\mathcal{T}\)</span> are <em>compatible</em> if whenever <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}ax}}[I] \in \mathcal{S}\)</span> and <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}ax}}[J] \in \mathcal{T}\)</span>, then <span class="math inline">\(I = J\)</span>. We say that <span class="math inline">\(\mathcal{S}\)</span> and <span class="math inline">\(\mathcal{T}\)</span> are <em>orthogonal</em> if there is no <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}ax}}\)</span> such that <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}ax}}[I] \in \mathcal{S}\)</span> and <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}ax}}[J] \in \mathcal{T}\)</span> for any <span class="math inline">\(I\)</span>, <span class="math inline">\(J\)</span>.</p>
<p>If <span class="math inline">\(t \in \rec \mathcal{T}\)</span> and <span class="math inline">\(\mathcal{S} \subseteq \mathcal{T}\)</span>, then we write <span class="math inline">\(\mathopen{}\left.t\right|_{\mathcal{S}}\)</span> for the unique record in <span class="math inline">\(\rec \mathcal{S}\)</span> such that <span class="math inline">\(\mathopen{}\left.t\right|_{\mathcal{S}} \subseteq t\)</span>.</p>
<h2 id="named-tensors-1"><span class="header-section-number">5.2</span> Named tensors</h2>
<p>Let <span class="math inline">\(F\)</span> be a field and let <span class="math inline">\(\mathcal{S}\)</span> be a shape. Then a <em>named tensor over <span class="math inline">\(F\)</span> with shape <span class="math inline">\(\mathcal{S}\)</span></em> is a mapping from <span class="math inline">\(\mathcal{S}\)</span> to <span class="math inline">\(F\)</span>. We write the set of all named tensors with shape <span class="math inline">\(\mathcal{S}\)</span> as <span class="math inline">\(F^{\mathcal{S}}\)</span>.</p>
<p>We don’t make any distinction between a scalar (an element of <span class="math inline">\(F\)</span>) and a named tensor with empty shape (an element of <span class="math inline">\(F^\emptyset\)</span>).</p>
<p>If <span class="math inline">\(A \in F^{\mathcal{S}}\)</span>, then we access an element of <span class="math inline">\(A\)</span> by applying it to a record <span class="math inline">\(s \in \rec \mathcal{S}\)</span>; but we write this using the usual subscript notation: <span class="math inline">\(A_s\)</span> rather than <span class="math inline">\(A(s)\)</span>. To avoid clutter, in place of <span class="math inline">\(A_{\{\ensuremath{\mathsf{\vphantom{fg}ax}}_1(i_1), \ldots, \ensuremath{\mathsf{\vphantom{fg}ax}}_r(i_r)\}}\)</span>, we usually write <span class="math inline">\(A_{\ensuremath{\mathsf{\vphantom{fg}ax}}_1(i_1), \ldots, \ensuremath{\mathsf{\vphantom{fg}ax}}_r(x_r)}\)</span>. When a named tensor is an expression like <span class="math inline">\((A+B)\)</span>, we surround it with square brackets like this: <span class="math inline">\([A+B]_{\ensuremath{\mathsf{\vphantom{fg}ax}}_1(i_1), \ldots, \ensuremath{\mathsf{\vphantom{fg}ax}}_r(x_r)}\)</span>.</p>
<p>We also allow partial indexing. If <span class="math inline">\(A\)</span> is a tensor with shape <span class="math inline">\(\mathcal{T}\)</span> and <span class="math inline">\(s \in \rec \mathcal{S}\)</span> where <span class="math inline">\(\mathcal{S} \subseteq \mathcal{T}\)</span>, then we define <span class="math inline">\(A_s\)</span> to be the named tensor with shape <span class="math inline">\(\mathcal{T} \setminus \mathcal{S}\)</span> such that, for any <span class="math inline">\(t \in \rec (\mathcal{T} \setminus \mathcal{S})\)</span>, <span class="math display">\[\begin{aligned}
\left[A_s\right]_t &amp;= A_{s \cup t}.\end{aligned}\]</span> (For the edge case <span class="math inline">\(\mathcal{T} = \emptyset\)</span>, our definitions for indexing and partial indexing coincide: one gives a scalar and the other gives a tensor with empty shape, but we don’t distinguish between the two.)</p>
<h2 id="sec:tensorfunctions"><span class="header-section-number">5.3</span> Named tensor operations</h2>
<p>In §<a href="#sec:overview" data-reference-type="ref" data-reference="sec:overview">2</a>, we described several classes of functions that can be extended to named tensors. Here, we define how to do this for general functions.</p>
<p>Let <span class="math inline">\(f \colon F^{\mathcal{S}} \rightarrow G^{\mathcal{T}}\)</span> be a function from tensors to tensors. For any shape <span class="math inline">\(\mathcal{S&#39;}\)</span> orthogonal to both <span class="math inline">\(\mathcal{S}\)</span> and <span class="math inline">\(\mathcal{T}\)</span>, we can extend <span class="math inline">\(f\)</span> to: <span class="math display">\[\begin{aligned}
f \colon F^{\mathcal{S} \cup \mathcal{S&#39;}} &amp;\rightarrow G^{\mathcal{T} \cup \mathcal{S&#39;}} \\
[f(A)]_s &amp;= f(A_s) \qquad \text{for all $s \in \rec\mathcal{S&#39;}$.}\end{aligned}\]</span></p>
<p>If <span class="math inline">\(f\)</span> is a multary function, we can extend its arguments to larger shapes, and we don’t have to extend all the arguments with the same names. We consider just the case of two arguments; three or more arguments are analogous. Let <span class="math inline">\(f \colon F^{\mathcal{S}} \times G^{\mathcal{T}} \rightarrow H^{\mathcal{U}}\)</span> be a binary function from tensors to tensors. For any shapes <span class="math inline">\(\mathcal{S&#39;}\)</span> and <span class="math inline">\(\mathcal{T&#39;}\)</span> that are compatible with each other and orthogonal to <span class="math inline">\(\mathcal{S}\)</span> and <span class="math inline">\(\mathcal{T}\)</span>, respectively, and <span class="math inline">\(\mathcal{S&#39;} \cup \mathcal{T&#39;}\)</span> is orthogonal to <span class="math inline">\(\mathcal{U}\)</span>, we can extend <span class="math inline">\(f\)</span> to: <span class="math display">\[\begin{aligned}
f \colon F^{\mathcal{S} \cup \mathcal{S&#39;}} \times G^{\mathcal{T} \cup \mathcal{T&#39;}} &amp;\rightarrow H^{\mathcal{U} \cup \mathcal{S&#39;} \cup \mathcal{T&#39;}} \\
  [f(A,B)]_s &amp;= f\left(A_{\mathopen{}\left.s\right|_{\mathcal{S&#39;}}},B_{\mathopen{}\left.s\right|_{\mathcal{T&#39;}}}\right) \qquad \text{for all $s \in \rec (\mathcal{S&#39;} \cup \mathcal{T&#39;})$.}\end{aligned}\]</span></p>
<h2 id="sec:commonops"><span class="header-section-number">5.4</span> Common operations</h2>
<p>All the tensor operations described in §<a href="#sec:operations" data-reference-type="ref" data-reference="sec:operations">2.2</a> can be defined in this way, and others listed below.</p>
<h4 id="elementwise-operations"><span class="header-section-number">5.4.0.1</span> Elementwise operations</h4>
<p>(<span class="math inline">\(\mathbb{R}\rightarrow \mathbb{R}\)</span>) <span class="math display">\[\begin{aligned}
  \sigma(x) &amp;= \frac{1}{1+\exp(-x)} \\
  \text{relu}(x) &amp;= \max(0, x)\end{aligned}\]</span></p>
<h4 id="reductions"><span class="header-section-number">5.4.0.2</span> Reductions</h4>
<p>(<span class="math inline">\(\mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}ax}}[n]} \rightarrow \mathbb{R}\)</span>) <span class="math display">\[\begin{aligned}
  \sum\limits_{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}} A &amp;= \sum_{i=1}^n A_{\ensuremath{\mathsf{\vphantom{fg}ax}}(i)} \\
  \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}}{\vphantom{fg}\mathrm{min}}} A &amp;= \min \{A_{\ensuremath{\mathsf{\vphantom{fg}ax}}(i)} \mid 1 \leq i \leq n\} \\
  \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}}{\vphantom{fg}\mathrm{max}}} A &amp;= \max \{A_{\ensuremath{\mathsf{\vphantom{fg}ax}}(i)} \mid 1 \leq i \leq n\} \\
  \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}}{\vphantom{fg}\mathrm{norm}}} A &amp;= \sqrt{\sum\limits_{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}} A^2} \\
  \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}}{\vphantom{fg}\mathrm{mean}}} A &amp;= \frac{1}{n} \sum\limits_{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}} A \\
  \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}}{\vphantom{fg}\mathrm{var}}} A &amp;= \frac{1}{n} \sum\limits_{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}} (A - \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}}{\vphantom{fg}\mathrm{mean}}} A)^2\end{aligned}\]</span></p>
<h4 id="contraction"><span class="header-section-number">5.4.0.3</span> Contraction</h4>
<p>(<span class="math inline">\(\mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}ax}}[n]} \times \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}ax}}[n]} \rightarrow F\)</span>) <span class="math display">\[\begin{aligned}
  A \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}}{\vphantom{fg}\odot}} B &amp;= \sum_{i=1}^n A_{\ensuremath{\mathsf{\vphantom{fg}ax}}(i)} B_{\ensuremath{\mathsf{\vphantom{fg}ax}}(i)}\end{aligned}\]</span></p>
<h4 id="vectors-to-vectors"><span class="header-section-number">5.4.0.4</span> Vectors to vectors</h4>
<p>(<span class="math inline">\(\mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}ax}}[n]} \rightarrow \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}ax}}[n]}\)</span>) <span class="math display">\[\begin{aligned}
  \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}}{\vphantom{fg}\mathrm{softmax}}} A &amp;= \frac{\exp A}{\sum\limits_{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}} \exp A} \\
  \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}}{\vphantom{fg}\mathrm{argmax}}} A &amp;= \lim_{\alpha \rightarrow \infty} \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}}{\vphantom{fg}\mathrm{softmax}}} \alpha A \\
  \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}}{\vphantom{fg}\mathrm{argmin}}} A &amp;= \lim_{\alpha \rightarrow -\infty} \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}}{\vphantom{fg}\mathrm{softmax}}} \alpha A\end{aligned}\]</span></p>
<h4 id="renaming"><span class="header-section-number">5.4.0.5</span> Renaming</h4>
<p>(<span class="math inline">\(\mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}ax}}[n]} \rightarrow \mathbb{R}^{\ensuremath{\mathsf{\vphantom{fg}ax}}&#39;[n]}\)</span>) <span class="math display">\[\begin{aligned}
  [A_{\ensuremath{\mathsf{\vphantom{fg}ax}}\rightarrow\ensuremath{\mathsf{\vphantom{fg}ax}}&#39;}]_{\ensuremath{\mathsf{\vphantom{fg}ax}}&#39;(i)} &amp;= A_{\ensuremath{\mathsf{\vphantom{fg}ax}}(i)}\end{aligned}\]</span></p>
<h1 id="differentiation"><span class="header-section-number">6</span> Differentiation</h1>
<p>Let <span class="math inline">\(f\)</span> be a function from order-<span class="math inline">\(m\)</span> tensors to order-<span class="math inline">\(n\)</span> tensors and let <span class="math inline">\(Y = f(X)\)</span>. The partial derivatives of <span class="math inline">\(Y\)</span> with respect to <span class="math inline">\(X\)</span> form an order-<span class="math inline">\((m+n)\)</span> tensor: <span class="math inline">\(m\)</span> “input” axes for the directions in which <span class="math inline">\(X\)</span> could change and <span class="math inline">\(n\)</span> “output” axes for the change in <span class="math inline">\(Y\)</span>.</p>
<p>For example, if <span class="math inline">\(f\)</span> maps from vectors to vectors, then <span class="math inline">\(\frac{\partial Y}{\partial X}\)</span> is a matrix (the Jacobian). But using matrix notation, there are conflicting conventions about whether the first axis is the input axis (“denominator layout”) or the output axis (“numerator layout”). The derivative of a function from vectors to matrices or matrices to vectors cannot be represented as a matrix at all, so one must resort to flattening the matrices into vectors.</p>
<p>With tensors, taking derivatives of higher-order tensors with respect to higher-order tensors is not difficult <span class="citation" data-cites="laue+:2018">(Laue, Mitterreiter, and Giesen 2018)</span>. With named tensors, we get the additional advantage of using names to distinguish input and output axes.</p>
<h2 id="definition"><span class="header-section-number">6.1</span> Definition</h2>
<p>Let <span class="math inline">\(f \colon \mathbb{R}^\mathcal{S} \rightarrow \mathbb{R}^\mathcal{T}\)</span>, where <span class="math inline">\(\mathcal{S}\)</span> and <span class="math inline">\(\mathcal{T}\)</span> are orthogonal, and let <span class="math inline">\(Y = f(X)\)</span>. Then the derivative of <span class="math inline">\(Y\)</span> at <span class="math inline">\(X\)</span> is the tensor with shape <span class="math inline">\(\mathcal{S} \times \mathcal{T}\)</span> such that for all <span class="math inline">\(s \in \rec\mathcal{S}\)</span> and <span class="math inline">\(t \in \rec\mathcal{T}\)</span>, <span class="math display">\[\left[\frac{\partial Y}{\partial X} \right]_{s,t} = \frac{\partial Y_t}{\partial X_s}.\]</span></p>
<p>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>’s shapes are not orthogonal, we take the derivative of <span class="math inline">\(Y_{\mathcal{T}\rightarrow\mathcal{T&#39;}}\)</span> instead. (It’s also possible to rename <span class="math inline">\(X\)</span>, but we think it’s easier to think about renaming <span class="math inline">\(Y\)</span>, so that’s what we’ll do.) Assume <span class="math inline">\(\mathcal{T} = \ensuremath{\mathsf{\vphantom{fg}ax}}_1 \times \cdots \times \ensuremath{\mathsf{\vphantom{fg}ax}}_r\)</span>. Then for each <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}ax}}_i\)</span>, choose a new name <span class="math inline">\(\ensuremath{\mathsf{\vphantom{fg}ax}}_i&#39;\)</span> not in either <span class="math inline">\(\mathcal{S}\)</span> or <span class="math inline">\(\mathcal{T}\)</span>, and let <span class="math inline">\(\mathcal{T&#39;} = \ensuremath{\mathsf{\vphantom{fg}ax}}_1&#39; \times \cdots \times \ensuremath{\mathsf{\vphantom{fg}ax}}_r&#39;\)</span>. Then we seek the tensor of partial derivatives <span class="math display">\[\left[\frac{\partial Y_{\mathcal{T}\rightarrow\mathcal{T&#39;}}}{\partial X} \right]_{s,t&#39;} = \frac{\partial Y_t}{\partial X_s}.\]</span></p>
<h2 id="rules"><span class="header-section-number">6.2</span> Rules</h2>
<p>To compute derivatives, we use the method of differentials <span class="citation" data-cites="magnus+neudecker:1985">(Magnus and Neudecker 1985)</span>. The differential of an expression <span class="math inline">\(U\)</span>, written <span class="math inline">\(\partial U\)</span>, is a tensor with the same shape as <span class="math inline">\(U\)</span>, computed using rules like the following: <span class="math display">\[\begin{aligned}
  \partial f(U) &amp;= f&#39;(U) \mathbin{\underset{\substack{\mathcal{U}}}{\vphantom{fg}\odot}} \partial U &amp;&amp; f \colon \mathbb{R}^\mathcal{U} \rightarrow \mathbb{R}^{\mathcal{V}} \\
  \partial (U + V) &amp;= \partial U + \partial V \\
  \partial \sum\limits_{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}} U &amp;= \sum\limits_{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}} \partial U \\
  \partial (U \odot V) &amp;= \partial U \odot V + U \odot \partial V \\
  \partial (U \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}}{\vphantom{fg}\odot}} V) &amp;= \partial U \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}}{\vphantom{fg}\odot}} V + U \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}}{\vphantom{fg}\odot}} \partial V \\
  \partial \left(\frac{U}{V}\right) &amp;= \frac{\partial U \odot V - U \odot \partial V}{V^2} \\
  \partial U_s &amp;= \left[\partial U\right]_s \\
  \partial U_{\ensuremath{\mathsf{\vphantom{fg}ax}}\rightarrow\ensuremath{\mathsf{\vphantom{fg}ax}}&#39;} &amp;= \left[\partial U\right]_{\ensuremath{\mathsf{\vphantom{fg}ax}}\rightarrow\ensuremath{\mathsf{\vphantom{fg}ax}}&#39;}\end{aligned}\]</span> If we obtain an equation in the so-called <em>canonical</em> form <span class="math display">\[\partial Y = A \mathbin{\underset{\substack{\mathcal{S}}}{\vphantom{fg}\odot}} \partial X + \text{const.}\]</span> where <span class="math inline">\(\mathcal{S}\)</span> is orthogonal to <span class="math inline">\(\mathcal{T}\)</span> and “const” stands for terms not depending on <span class="math inline">\(\partial X\)</span>, then we have <span class="math display">\[\frac{\partial Y}{\partial X} = A.\]</span></p>
<p>In order to get equations into canonical form, some tricks are useful. First, contractions can be easier to reason about if rewritten as sums of elementwise products: <span class="math display">\[A \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}}{\vphantom{fg}\odot}} B = \sum\limits_{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}} A \odot B.\]</span> Second, renaming can be thought of as contraction with an identity matrix: <span class="math display">\[\begin{aligned}
_{\ensuremath{\mathsf{\vphantom{fg}ax}}(i),\ensuremath{\mathsf{\vphantom{fg}ax}}&#39;(j)} &amp;= \begin{cases} 1 &amp; i = j \\ 0 &amp; i \neq j \end{cases} \\
A_{\ensuremath{\mathsf{\vphantom{fg}ax}}\rightarrow\ensuremath{\mathsf{\vphantom{fg}ax}}&#39;} &amp;= \sum\limits_{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}} I_{\ensuremath{\mathsf{\vphantom{fg}ax}},\ensuremath{\mathsf{\vphantom{fg}ax}}&#39;} \odot A.\end{aligned}\]</span></p>
<h2 id="example"><span class="header-section-number">6.3</span> Example</h2>
<p>Let’s find the differential of the softmax operator. <span class="math display">\[\begin{aligned}
  Y &amp;= \mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}}{\vphantom{fg}\mathrm{softmax}}} X \\
  \partial Y &amp;= \partial \biggl(\frac{\exp X}{\sum\limits_{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}} \exp X}\biggr) \\
    &amp;= \frac{\exp X \odot \partial X \odot \sum\limits_{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}} \exp X - \exp X \odot \sum\limits_{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}} (\exp X \odot \partial X)}{\bigl(\sum\limits_{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}} \exp X\bigr)^2} \\
  &amp;= Y \odot (\partial X - Y \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}}{\vphantom{fg}\odot}} \partial X).\end{aligned}\]</span></p>
<p>Next, use this to find the Jacobian, <span class="math inline">\(\frac{\partial Y}{\partial X}\)</span>. Since <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> have the same shape, we rename <span class="math inline">\(Y\)</span>: <span class="math display">\[\begin{aligned}
  \partial Y_{\ensuremath{\mathsf{\vphantom{fg}ax}}\rightarrow\ensuremath{\mathsf{\vphantom{fg}ax}}&#39;} &amp;= [Y \odot (\partial X - \sum\limits_{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}} Y \odot \partial X)]_{\ensuremath{\mathsf{\vphantom{fg}ax}}\rightarrow\ensuremath{\mathsf{\vphantom{fg}ax}}&#39;} \\
  &amp;= Y_{\ensuremath{\mathsf{\vphantom{fg}ax}}\rightarrow\ensuremath{\mathsf{\vphantom{fg}ax}}&#39;} \odot (\partial X_{\ensuremath{\mathsf{\vphantom{fg}ax}}\rightarrow\ensuremath{\mathsf{\vphantom{fg}ax}}&#39;} - \sum\limits_{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}} Y \odot \partial X) \\
  &amp;= Y_{\ensuremath{\mathsf{\vphantom{fg}ax}}\rightarrow\ensuremath{\mathsf{\vphantom{fg}ax}}&#39;} \odot \left(\sum\limits_{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}} I_{\ensuremath{\mathsf{\vphantom{fg}ax}}&#39;,\ensuremath{\mathsf{\vphantom{fg}ax}}} \odot \partial X - \sum\limits_{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}} Y \odot \partial X\right) \\
  &amp;= \sum\limits_{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}} Y_{\ensuremath{\mathsf{\vphantom{fg}ax}}\rightarrow\ensuremath{\mathsf{\vphantom{fg}ax}}&#39;} \odot (I_{\ensuremath{\mathsf{\vphantom{fg}ax}}&#39;,\ensuremath{\mathsf{\vphantom{fg}ax}}} - Y) \odot \partial X \\
  \frac{\partial Y_{\ensuremath{\mathsf{\vphantom{fg}ax}}\rightarrow\ensuremath{\mathsf{\vphantom{fg}ax}}&#39;}}{\partial X} &amp;= Y_{\ensuremath{\mathsf{\vphantom{fg}ax}}\rightarrow\ensuremath{\mathsf{\vphantom{fg}ax}}&#39;} \odot (I_{\ensuremath{\mathsf{\vphantom{fg}ax}}&#39;,\ensuremath{\mathsf{\vphantom{fg}ax}}} - Y).\end{aligned}\]</span></p>
<p>To derive the rule for backpropagation, we assume a function <span class="math inline">\(f \colon \mathbb{R}^\ensuremath{\mathsf{\vphantom{fg}ax}}\rightarrow \mathbb{R}\)</span> and differentiate <span class="math inline">\(f(Y)\)</span>. Since <span class="math inline">\(f\)</span> is scalar-valued, there is no name overlap, so no renaming is needed. <span class="math display">\[\begin{aligned}
  \partial f(Y) &amp;= \sum\limits_{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}} f&#39;(Y) \odot \partial Y \\
  &amp;= \sum\limits_{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}} f&#39;(Y) \odot Y \odot (\partial X - \sum\limits_{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}} Y \odot \partial X) \\
  %&amp;= f&#39;(Y) \ndot{\ax} (Y \odot (\partial X - Y \ndot{\ax} \partial X)) \\
  &amp;= \sum\limits_{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}} f&#39;(Y) \odot Y \odot \partial X - \sum\limits_{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}} f&#39;(Y) \odot Y \odot \sum\limits_{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}} Y \odot \partial X \\
  &amp;= \sum\limits_{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}} f&#39;(Y) \odot Y \odot \partial X - \sum\limits_{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}} \left(\sum\limits_{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}} f&#39;(Y) \odot Y\right) \odot Y \odot \partial X \\
  &amp;= \sum\limits_{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}} Y \odot (f&#39;(Y) - \sum\limits_{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}} f&#39;(Y) \odot Y) \odot \partial X \\
  \frac{\partial f(Y)}{\partial X} &amp;= Y \odot (f&#39;(Y) - f&#39;(Y) \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}}{\vphantom{fg}\odot}} Y).\end{aligned}\]</span></p>
<h2 id="broadcasting"><span class="header-section-number">6.4</span> Broadcasting</h2>
<p>Let <span class="math inline">\(f \colon \mathbb{R}^\mathcal{S} \rightarrow \mathbb{R}^\mathcal{T}\)</span>, and let <span class="math inline">\(f&#39;\)</span> be its derivative. If <span class="math inline">\(X \in \mathbb{R}^{\mathcal{S} \cup \mathcal{U}}\)</span>, where <span class="math inline">\(\mathcal{U}\)</span> is orthogonal to both <span class="math inline">\(\mathcal{S}\)</span> and <span class="math inline">\(\mathcal{T}\)</span>, recall that <span class="math inline">\(Y = f(X)\)</span> is defined by: <span class="math display">\[\begin{aligned}
  Y_r &amp;= f(X_r)\end{aligned}\]</span> Finding the differential of <span class="math inline">\(Y\)</span> is easy: <span class="math display">\[\begin{aligned}
  \partial Y_r &amp;= f&#39;(X_r) \mathbin{\underset{\substack{\mathcal{S}}}{\vphantom{fg}\odot}} \partial X_r \\
  \partial Y &amp;= f&#39;(X) \mathbin{\underset{\substack{\mathcal{S}}}{\vphantom{fg}\odot}} \partial X.\end{aligned}\]</span> But although <span class="math inline">\(f&#39;\)</span> extends to <span class="math inline">\(X\)</span> using the usual broadcasting rules, it’s not the case that <span class="math inline">\(\frac{\partial Y}{\partial X} = f&#39;(X)\)</span>, which would have the wrong shape. The reason is that the contraction is only over <span class="math inline">\(\mathcal{S}\)</span>, not <span class="math inline">\(\mathcal{S}\cup\mathcal{U}\)</span>. To get this into the form (<a href="#eq:canonical" data-reference-type="ref" data-reference="eq:canonical">[eq:canonical]</a>): <span class="math display">\[\begin{aligned}
  \partial Y_{\mathcal{U}\rightarrow\mathcal{U&#39;}} &amp;= \sum\limits_{\substack{\mathcal{S}}} [f&#39;(X) \odot \partial X]_{\mathcal{U}\rightarrow\mathcal{U&#39;}} \\
  &amp;= \sum\limits_{\substack{\mathcal{S}}} \sum\limits_{\substack{\mathcal{U}}} I_{\mathcal{U},\mathcal{U&#39;}} \odot f&#39;(X) \odot \partial X \\
  \frac{\partial Y_{\mathcal{U}\rightarrow\mathcal{U&#39;}}}{\partial X} &amp;= I_{\mathcal{U},\mathcal{U&#39;}} \odot f&#39;(X).\end{aligned}\]</span> In general, then, when we extend a function to new axes, we extend its derivative by multiplying by the identity matrix for those axes.</p>
<h1 id="alternatives"><span class="header-section-number">7</span> Alternatives</h1>
<p>A very frequently asked question is why we haven’t used index notation as used in physics, and the Einstein summation convention in particular. In this notation, axes are ordered, and every equation is written in terms of tensor components. If an index appears on both sides of an equation, then the equation must hold for each value of the index, and if an index appears twice on one side and not on the other, there is an implicit summation over that index. <span class="math display">\[\begin{aligned}
  \text{Attention} \colon \mathbb{R}^{n&#39; \times d_k} \times \mathbb{R}^{n \times d_k} \times \mathbb{R}^{n \times d_v} &amp;\rightarrow \mathbb{R}^{n&#39; \times d_v} \\
  \left[\text{Attention}(Q, K, V)\right]_{i&#39;k} &amp;= \softmax_i \left( \frac{Q_{i&#39;j} K_{ij}}{\sqrt{d_k}} \right) V_{ik}.\end{aligned}\]</span> Because <span class="math inline">\(i&#39;\)</span> and <span class="math inline">\(k\)</span> appear on both sides, the equation must hold over all values of these indices. But because <span class="math inline">\(j\)</span> and <span class="math inline">\(k\)</span> occur twice on only the right-hand side, they are both summed over. We’d have to define exactly what the <span class="math inline">\(i\)</span> under softmax means (<span class="math inline">\(i\)</span> is bound inside the softmax and free outside it), and since softmax doesn’t distribute over addition, we’d need to clarify that the summation over <span class="math inline">\(j\)</span> occurs inside the softmax.</p>
<p>Other than that, this is concise and unambiguous. But it doesn’t really solve the main problem we set out to solve, which is that ordered axes force the author and reader to remember the purpose of each axis. The indices do act as symbolic names for axes (indeed, in <em>abstract</em> index notation, they really are symbols, not variables), but they are temporary names; they could be totally different in the next equation. It would be up to the author to choose to use consistent names, and to do so correctly.</p>
<p>A second issue is that because it depends on repetition of indices to work, index notation can be a little bit more verbose than our notation, particularly for reductions and contractions: <span class="math display">\[\begin{aligned}
  C &amp;= \max_i A_i &amp; C &amp;=\mathop{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}}{\vphantom{fg}\mathrm{max}}} A \\
  C &amp;= A_i B_i &amp; C &amp;= A \mathbin{\underset{\substack{\ensuremath{\mathsf{\vphantom{fg}ax}}}}{\vphantom{fg}\odot}} B.\end{aligned}\]</span></p>
<p>Finally, index notation requires us to write out all indices explicitly. So if we wanted to extend attention to multiple heads and minibatches, we would write: <span class="math display">\[\begin{gathered}
  \text{Attention} \colon \mathbb{R}^{B \times H \times n&#39; \times d_k} \times \mathbb{R}^{B \times H \times n \times d_k} \times \mathbb{R}^{B \times H \times n \times d_v} \rightarrow \mathbb{R}^{B \times H \times n&#39; \times d_v} \\
  \left[\text{Attention}(Q, K, V)\right]_{bhi&#39;k} = \softmax_i \left( \frac{Q_{bhi&#39;j} K_{bhij}}{\sqrt{d_k}} \right) V_{bhik}.\end{gathered}\]</span> We could adopt a convention that extends a function on tensors to tensors that have extra axes to the <em>left</em>, but such conventions tend to lead to messy reordering and squeezing/unsqueezing of axes. Named axes make this unnecessary.</p>
<h1 id="acknowledgements" class="unnumbered">Acknowledgements</h1>
<p>Thanks to Ekin Akyürek, Justin Bayer, Colin McDonald, Adam Poliak, Matt Post, Chung-chieh Shan, Nishant Sinha, and Yee Whye Teh for their input to this document (or the ideas in it).</p>
<h1 id="references" class="unnumbered">References</h1>
<div id="refs" class="references">
<div id="ref-chen2017typesafe">
<p>Chen, Tongfei. 2017. “Typesafe Abstractions for Tensor Operations.” In <em>Proceedings of the 8th ACM SIGPLAN International Symposium on Scala</em>, 45–50. SCALA 2017. <a href="https://doi.org/10.1145/3136000.3136001">https://doi.org/10.1145/3136000.3136001</a>.</p>
</div>
<div id="ref-numpy">
<p>Harris, Charles R., K. Jarrod Millman, Stéfan J. van der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, et al. 2020. “Array Programming with NumPy.” <em>Nature</em> 585 (7825): 357–62. <a href="https://doi.org/10.1038/s41586-020-2649-2">https://doi.org/10.1038/s41586-020-2649-2</a>.</p>
</div>
<div id="ref-xarray">
<p>Hoyer, Stephan, and Joe Hamman. 2017. “xarray: N-D Labeled Arrays and Datasets in Python.” <em>Journal of Open Research Software</em> 5 (1): 10. <a href="https://doi.org/http://doi.org/10.5334/jors.148">https://doi.org/http://doi.org/10.5334/jors.148</a>.</p>
</div>
<div id="ref-laue+:2018">
<p>Laue, Soeren, Matthias Mitterreiter, and Joachim Giesen. 2018. “Computing Higher Order Derivatives of Matrix and Tensor Expressions.” In <em>Advances in Neural Information Processing Systems</em>, edited by S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, 31:2750–9. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper/2018/file/0a1bf96b7165e962e90cb14648c9462d-Paper.pdf">https://proceedings.neurips.cc/paper/2018/file/0a1bf96b7165e962e90cb14648c9462d-Paper.pdf</a>.</p>
</div>
<div id="ref-maclaurin+:2019">
<p>Maclaurin, Dougal, Alexey Radul, Matthew J. Johnson, and Dimitrios Vytiniotis. 2019. “Dex: Array Programming with Typed Indices.” In <em>NeurIPS Workshop on Program Transformations for ML</em>. <a href="https://openreview.net/forum?id=rJxd7vsWPS">https://openreview.net/forum?id=rJxd7vsWPS</a>.</p>
</div>
<div id="ref-magnus+neudecker:1985">
<p>Magnus, Jan R., and H. Neudecker. 1985. “Matrix Differential Calculus with Applications to Simple, Hadamard, and Kronecker Products.” <em>Journal of Mathematical Psychology</em> 29 (4): 474–92. <a href="https://doi.org/https://doi.org/10.1016/0022-2496(85)90006-9">https://doi.org/https://doi.org/10.1016/0022-2496(85)90006-9</a>.</p>
</div>
<div id="ref-pytorch">
<p>Paszke, Adam, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, et al. 2019. “PyTorch: An Imperative Style, High-Performance Deep Learning Library.” In <em>Advances in Neural Information Processing Systems 32</em>, edited by H. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alché-Buc, E. Fox, and R. Garnett, 8024–35. Curran Associates, Inc. <a href="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf">http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf</a>.</p>
</div>
<div id="ref-namedtensor">
<p>Rush, Alexander. 2019. “Named Tensors.” <a href="https://github.com/harvardnlp/NamedTensor">https://github.com/harvardnlp/NamedTensor</a>.</p>
</div>
<div id="ref-tsalib">
<p>Sinha, Nishant. 2018. “Tensor Shape (Annotation) Library.” <a href="https://github.com/ofnote/tsalib">https://github.com/ofnote/tsalib</a>.</p>
</div>
<div id="ref-named-tensors">
<p>Torch Contributors. 2019. “Named Tensors.” <a href="https://pytorch.org/docs/stable/named_tensor.html">https://pytorch.org/docs/stable/named_tensor.html</a>.</p>
</div>
<div id="ref-vaswani+:2017">
<p>Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” In <em>Advances in Neural Information Processing Systems</em>, edited by I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, 30:5998–6008. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</a>.</p>
</div>
</div>
            </div>
    </div>
  </div>
  <script src="https://vjs.zencdn.net/5.4.4/video.js"></script>

</body>
</html>
